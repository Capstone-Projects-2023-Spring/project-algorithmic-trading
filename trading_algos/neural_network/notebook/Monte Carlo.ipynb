{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688093b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas_market_calendars as mcal\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "os.chdir(\"../scripts\")\n",
    "import preprocess, train, inference, interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036137b",
   "metadata": {},
   "source": [
    "### Note that only 5% of the data is used for validation as we are concerned with short term predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05782eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"alpha_vantage\": {\n",
    "        \"key\": \"2JMCN347HZ3BU9RC\", \n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"key_adjusted_close\": \"5. adjusted close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 20,\n",
    "        \"train_split_size\": 0.95,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"xticks_interval\": 90, # show a date every 90 days\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since for now we are only using close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\",\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epoch\": 100,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6915b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Low</th>\n",
       "      <th>High</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>390.800</td>\n",
       "      <td>393.74</td>\n",
       "      <td>390.07</td>\n",
       "      <td>394.17</td>\n",
       "      <td>93055783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>397.240</td>\n",
       "      <td>398.91</td>\n",
       "      <td>395.58</td>\n",
       "      <td>399.41</td>\n",
       "      <td>91524248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>2023-03-22</td>\n",
       "      <td>398.730</td>\n",
       "      <td>392.11</td>\n",
       "      <td>392.07</td>\n",
       "      <td>402.49</td>\n",
       "      <td>111746583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5885</th>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>395.090</td>\n",
       "      <td>393.17</td>\n",
       "      <td>390.35</td>\n",
       "      <td>399.29</td>\n",
       "      <td>119351319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5886</th>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>391.841</td>\n",
       "      <td>395.75</td>\n",
       "      <td>389.40</td>\n",
       "      <td>395.84</td>\n",
       "      <td>107770124.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date     Open   Close     Low    High       Volume\n",
       "5882  2023-03-20  390.800  393.74  390.07  394.17   93055783.0\n",
       "5883  2023-03-21  397.240  398.91  395.58  399.41   91524248.0\n",
       "5884  2023-03-22  398.730  392.11  392.07  402.49  111746583.0\n",
       "5885  2023-03-23  395.090  393.17  390.35  399.29  119351319.0\n",
       "5886  2023-03-24  391.841  395.75  389.40  395.84  107770124.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../model/params.yaml\", \"r\") as params_file:\n",
    "    params = yaml.safe_load(params_file)\n",
    "\n",
    "data_dir = params['data_dir']\n",
    "file_name = \"SPY.csv\"\n",
    "data = preprocess.load_data(file_name)\n",
    "data.columns = data.columns.str.capitalize()\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23711909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of simulations and early stop amount\n",
    "\n",
    "n = 100\n",
    "stop = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54106e",
   "metadata": {},
   "source": [
    "### Univariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb560d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88.21718508,  87.58674188,  88.17651326, ..., 392.11      ,\n",
       "       393.17      , 395.75      ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data for univariate time series model \n",
    "\n",
    "num_data_points = len(data.index)\n",
    "date_data = data['Date'].to_numpy()\n",
    "display_date_range = \"from \" + date_data[0] + \" to \" + date_data[num_data_points-1]\n",
    "close_price_data = data['Close'].to_numpy()\n",
    "close_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b1318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary classes for univariate model\n",
    "\n",
    "class Normalization():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "    \n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    # use the next day as label\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # right now we have only 1 feature, so we need to convert `x` into [batch, sequence, features]\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        \n",
    "        # layer 2\n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "    \n",
    "def run_epoch(dataloader, is_training=False):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        y = y.to(config[\"training\"][\"device\"])\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += (loss.detach().item() / batchsize)\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    return epoch_loss, lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9deb5444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape (5573, 20, 1) (5573,)\n",
      "Validation data shape (294, 20, 1) (294,)\n"
     ]
    }
   ],
   "source": [
    "# prerequisite data manipulation\n",
    "\n",
    "scaler = Normalization()\n",
    "normalized_close_price_data = scaler.fit_transform(close_price_data)\n",
    "normalized_close_price_data\n",
    "\n",
    "data_x, data_x_unseen = prepare_data_x(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "data_y = prepare_data_y(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "\n",
    "split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n",
    "data_x_train = data_x[:split_index]\n",
    "data_x_val = data_x[split_index:]\n",
    "data_y_train = data_y[:split_index]\n",
    "data_y_val = data_y[split_index:]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "val_dataset = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "\n",
    "print(\"Train data shape\", train_dataset.x.shape, train_dataset.y.shape)\n",
    "print(\"Validation data shape\", val_dataset.x.shape, val_dataset.y.shape)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60e8562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.074050, test:0.000388 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012439, test:0.000905 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009268, test:0.001914 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010221, test:0.001101 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011573, test:0.002381 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011444, test:0.004928 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008642, test:0.002057 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009438, test:0.002327 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009024, test:0.003335 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009722, test:0.001022 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008062, test:0.001718 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007308, test:0.006196 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007450, test:0.002129 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008675, test:0.000453 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007825, test:0.000696 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009013, test:0.002782 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008120, test:0.000375 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007193, test:0.002237 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007886, test:0.001466 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007777, test:0.000768 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007231, test:0.003288 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007662, test:0.000675 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007585, test:0.000461 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007237, test:0.002807 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007640, test:0.000374 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007539, test:0.000317 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006957, test:0.000718 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009358, test:0.000592 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007263, test:0.001499 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007708, test:0.003128 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009284, test:0.001082 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007742, test:0.000718 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006680, test:0.000918 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007128, test:0.000712 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007150, test:0.000494 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006698, test:0.000489 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006751, test:0.000803 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007477, test:0.002488 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009178, test:0.000330 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008012, test:0.000413 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006471, test:0.000520 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005008, test:0.000336 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005690, test:0.000319 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006610, test:0.000601 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005082, test:0.000401 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005252, test:0.000435 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006658, test:0.000539 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005735, test:0.000708 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006035, test:0.000333 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006225, test:0.000336 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004968, test:0.000381 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004983, test:0.000325 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005007, test:0.000319 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006130, test:0.000305 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005239, test:0.000342 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005211, test:0.000318 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005179, test:0.000289 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005513, test:0.000309 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007076, test:0.000434 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005144, test:0.000320 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005445, test:0.000510 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005717, test:0.000297 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007045, test:0.000313 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006208, test:0.000308 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005548, test:0.000335 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005442, test:0.000415 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005142, test:0.000377 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005303, test:0.000310 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005128, test:0.000328 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004915, test:0.000340 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005027, test:0.000383 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005165, test:0.000300 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004820, test:0.000288 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005141, test:0.000311 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004994, test:0.000392 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005111, test:0.000379 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005361, test:0.000332 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005060, test:0.000429 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004964, test:0.000359 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004988, test:0.000289 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006590, test:0.000292 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.007278, test:0.000305 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004890, test:0.000298 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004874, test:0.000304 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005462, test:0.000326 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005169, test:0.000320 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005163, test:0.000358 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005278, test:0.000304 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004993, test:0.000304 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004934, test:0.000318 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005596, test:0.000356 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004992, test:0.000304 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005054, test:0.000320 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005374, test:0.000284 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005100, test:0.000299 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005574, test:0.000294 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004835, test:0.000327 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004815, test:0.000320 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004876, test:0.000315 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005922, test:0.000321 | lr:0.000100\n",
      "Mean absolute error:  1.6587953499023695\n",
      "Root mean squared error:  5.774125742118309\n",
      "Epoch[1/100] | loss train:0.051083, test:0.001146 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013756, test:0.004685 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012446, test:0.000798 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010153, test:0.000892 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008446, test:0.003864 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009554, test:0.000297 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007511, test:0.000500 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008980, test:0.000329 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008243, test:0.000735 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008138, test:0.001290 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009274, test:0.000356 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008199, test:0.003229 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007639, test:0.001293 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008085, test:0.001681 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007351, test:0.000351 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008332, test:0.001093 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008888, test:0.001000 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007519, test:0.000903 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006957, test:0.001584 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011562, test:0.003363 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009236, test:0.000380 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009424, test:0.001843 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013637, test:0.003983 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009439, test:0.000393 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007418, test:0.001362 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[26/100] | loss train:0.006627, test:0.000686 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008194, test:0.000337 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009290, test:0.002476 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007905, test:0.000497 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009753, test:0.002448 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008082, test:0.000501 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007323, test:0.000474 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008447, test:0.000962 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007935, test:0.001272 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007723, test:0.000519 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007099, test:0.000495 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006374, test:0.003674 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008135, test:0.000609 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007586, test:0.000634 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006862, test:0.000354 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006171, test:0.000566 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005871, test:0.000489 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005339, test:0.000399 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005743, test:0.000500 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006071, test:0.000377 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004868, test:0.000380 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005443, test:0.000353 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005485, test:0.000470 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004990, test:0.000372 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005052, test:0.000382 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005492, test:0.000365 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005917, test:0.000395 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005019, test:0.000433 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005625, test:0.000351 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005977, test:0.000481 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005527, test:0.000352 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008204, test:0.000602 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007295, test:0.000346 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005097, test:0.000327 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005502, test:0.000339 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005879, test:0.000527 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005301, test:0.000334 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004991, test:0.000380 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005333, test:0.000357 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005181, test:0.000329 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005076, test:0.000376 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005057, test:0.000311 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005162, test:0.000415 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005896, test:0.000423 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005307, test:0.000360 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004649, test:0.000303 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005281, test:0.000621 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004636, test:0.000320 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005373, test:0.000319 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005827, test:0.000584 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005012, test:0.000350 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004981, test:0.000357 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005232, test:0.000317 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005170, test:0.000335 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005325, test:0.000391 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004754, test:0.000363 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004894, test:0.000343 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.006409, test:0.000323 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005707, test:0.000308 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005581, test:0.000321 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.006446, test:0.000318 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006574, test:0.000371 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005505, test:0.000327 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.006521, test:0.000346 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005203, test:0.000333 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005421, test:0.000335 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005183, test:0.000306 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004583, test:0.000310 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004785, test:0.000344 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004629, test:0.000321 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005065, test:0.000347 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005131, test:0.000289 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005129, test:0.000322 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005038, test:0.000320 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004836, test:0.000307 | lr:0.000100\n",
      "Mean absolute error:  1.6700440490947182\n",
      "Root mean squared error:  5.787099444701097\n",
      "Epoch[1/100] | loss train:0.045532, test:0.000383 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011463, test:0.000544 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011429, test:0.000449 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010048, test:0.000293 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008248, test:0.001112 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007933, test:0.001939 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008060, test:0.002227 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008221, test:0.000629 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008042, test:0.004384 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009895, test:0.001384 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013614, test:0.003123 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007886, test:0.002305 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007797, test:0.001249 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007282, test:0.004033 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008742, test:0.001249 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009855, test:0.001899 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009814, test:0.000618 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008417, test:0.001647 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008202, test:0.001854 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007415, test:0.001061 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007430, test:0.000379 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011822, test:0.000441 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008975, test:0.000596 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007270, test:0.004000 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008898, test:0.005223 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007871, test:0.001850 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008646, test:0.000529 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008519, test:0.000963 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007015, test:0.000853 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006797, test:0.000418 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009778, test:0.000664 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007270, test:0.000322 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010874, test:0.000880 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007835, test:0.001037 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008639, test:0.001027 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006667, test:0.000754 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006955, test:0.000959 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007970, test:0.000618 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007956, test:0.001033 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006950, test:0.000591 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006061, test:0.000470 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005241, test:0.000484 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006031, test:0.000401 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005469, test:0.000665 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005488, test:0.000723 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006066, test:0.000372 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006439, test:0.000539 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005157, test:0.000436 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008562, test:0.000438 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005872, test:0.000462 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[51/100] | loss train:0.004744, test:0.000517 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005423, test:0.000386 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005589, test:0.000428 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005341, test:0.000479 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005568, test:0.000388 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006415, test:0.000400 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005506, test:0.000385 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005361, test:0.000557 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005517, test:0.000371 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006106, test:0.000385 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005273, test:0.000573 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005096, test:0.000331 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.010829, test:0.000663 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006491, test:0.000525 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005844, test:0.000349 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005479, test:0.000359 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005030, test:0.000427 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005190, test:0.000370 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006241, test:0.000338 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006630, test:0.000526 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005113, test:0.000337 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007247, test:0.000505 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007215, test:0.000486 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005623, test:0.000354 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005457, test:0.000353 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005690, test:0.000367 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006393, test:0.000361 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005832, test:0.000369 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005466, test:0.000522 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005273, test:0.000393 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004972, test:0.000381 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005251, test:0.000362 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005074, test:0.000369 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005384, test:0.000381 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006003, test:0.000357 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.006427, test:0.000339 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004723, test:0.000377 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005399, test:0.000352 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005473, test:0.000389 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005155, test:0.000347 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005082, test:0.000331 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005782, test:0.000363 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005689, test:0.000325 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005366, test:0.000355 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.006533, test:0.000368 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005935, test:0.000385 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005239, test:0.000337 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004579, test:0.000377 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006377, test:0.000317 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004957, test:0.000357 | lr:0.000100\n",
      "Mean absolute error:  1.7349974873024203\n",
      "Root mean squared error:  5.840440148206552\n",
      "Epoch[1/100] | loss train:0.063577, test:0.000714 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015034, test:0.003335 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009614, test:0.000399 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012384, test:0.000488 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009350, test:0.000309 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011218, test:0.000591 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009754, test:0.001147 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009943, test:0.000317 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009466, test:0.005107 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011080, test:0.000702 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007094, test:0.000411 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007606, test:0.000760 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007297, test:0.003919 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007623, test:0.005718 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008810, test:0.000388 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008134, test:0.000396 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006606, test:0.001502 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008755, test:0.000514 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010810, test:0.000406 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008371, test:0.000970 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010139, test:0.000435 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009512, test:0.001216 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009208, test:0.000835 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007125, test:0.001146 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007927, test:0.000439 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006906, test:0.000356 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008349, test:0.000511 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007400, test:0.001411 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009611, test:0.000394 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008280, test:0.000375 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007162, test:0.002913 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007995, test:0.000655 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007412, test:0.000886 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006959, test:0.000330 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006582, test:0.000342 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008301, test:0.000336 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007482, test:0.000300 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006123, test:0.000871 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006095, test:0.002201 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008083, test:0.000856 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005414, test:0.000392 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005647, test:0.000342 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005666, test:0.000363 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005178, test:0.000368 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005180, test:0.000315 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006167, test:0.000400 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005204, test:0.000300 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005117, test:0.000566 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005343, test:0.000342 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005301, test:0.000631 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007620, test:0.000399 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005296, test:0.000293 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006136, test:0.000409 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007341, test:0.000298 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004944, test:0.000277 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006748, test:0.000348 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005324, test:0.000355 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005409, test:0.000396 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005485, test:0.000328 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005594, test:0.000305 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004959, test:0.000293 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005106, test:0.000295 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006103, test:0.000301 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005233, test:0.000342 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005071, test:0.000447 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005394, test:0.000289 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005032, test:0.000390 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004961, test:0.000287 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005798, test:0.000313 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004808, test:0.000276 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005017, test:0.000355 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005193, test:0.000303 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005358, test:0.000300 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005427, test:0.000345 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005078, test:0.000269 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[76/100] | loss train:0.004924, test:0.000333 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005049, test:0.000289 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004937, test:0.000515 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005369, test:0.000295 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004868, test:0.000281 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004987, test:0.000290 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004815, test:0.000316 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004703, test:0.000330 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004778, test:0.000281 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006368, test:0.000323 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004798, test:0.000298 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004997, test:0.000281 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004764, test:0.000302 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004827, test:0.000297 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004703, test:0.000282 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.008451, test:0.000292 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004602, test:0.000277 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005531, test:0.000279 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004957, test:0.000303 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005409, test:0.000308 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005062, test:0.000294 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004684, test:0.000308 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005217, test:0.000304 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004705, test:0.000271 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005237, test:0.000302 | lr:0.000100\n",
      "Mean absolute error:  1.7123682519072712\n",
      "Root mean squared error:  5.798376816445767\n",
      "Epoch[1/100] | loss train:0.051574, test:0.000442 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009389, test:0.001495 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010307, test:0.000962 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008534, test:0.002837 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008588, test:0.000578 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008768, test:0.000570 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008636, test:0.000454 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008246, test:0.000474 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007543, test:0.000333 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006418, test:0.005820 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008493, test:0.001536 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007869, test:0.000320 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006229, test:0.000918 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008620, test:0.002192 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008406, test:0.000483 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007737, test:0.000518 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006986, test:0.000961 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007920, test:0.000377 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007577, test:0.000406 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006670, test:0.001581 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007631, test:0.004349 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008581, test:0.000416 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007190, test:0.000422 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006571, test:0.001547 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006595, test:0.001165 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008401, test:0.000795 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007221, test:0.001302 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011623, test:0.000398 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008258, test:0.000394 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007389, test:0.000407 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006687, test:0.000305 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010777, test:0.000421 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008876, test:0.000516 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006666, test:0.000701 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007181, test:0.000297 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006596, test:0.001503 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009086, test:0.001172 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007101, test:0.001111 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006380, test:0.001403 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006839, test:0.000363 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005972, test:0.000388 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005182, test:0.000318 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005230, test:0.000439 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005078, test:0.000371 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004841, test:0.000295 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005380, test:0.000280 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005091, test:0.000323 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004956, test:0.000280 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007085, test:0.000315 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004810, test:0.000375 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004984, test:0.000267 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005358, test:0.000314 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005062, test:0.000315 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004802, test:0.000289 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006025, test:0.000327 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004882, test:0.000404 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005463, test:0.000286 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005613, test:0.000464 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005412, test:0.000299 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005472, test:0.000464 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005045, test:0.000580 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007324, test:0.000313 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004812, test:0.000295 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004906, test:0.000325 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008538, test:0.000294 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004896, test:0.000286 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005087, test:0.000325 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004916, test:0.000290 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004736, test:0.000290 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007509, test:0.000316 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005580, test:0.000572 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004818, test:0.000474 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007019, test:0.000764 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.009550, test:0.000358 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006168, test:0.000282 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005219, test:0.000288 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005061, test:0.000532 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006653, test:0.000287 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004765, test:0.000298 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005140, test:0.000279 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004723, test:0.000286 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004977, test:0.000267 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004749, test:0.000319 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005130, test:0.000309 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004976, test:0.000285 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005719, test:0.000294 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004989, test:0.000302 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004611, test:0.000275 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005230, test:0.000287 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005677, test:0.000284 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005161, test:0.000285 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005428, test:0.000304 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004995, test:0.000256 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004740, test:0.000291 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005164, test:0.000302 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004773, test:0.000288 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005277, test:0.000274 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004837, test:0.000282 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005652, test:0.000333 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005763, test:0.000297 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  1.7097442331299986\n",
      "Root mean squared error:  5.783543628168172\n",
      "Epoch[1/100] | loss train:0.047911, test:0.000338 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012039, test:0.001147 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011237, test:0.000506 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008201, test:0.001243 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008715, test:0.000776 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008371, test:0.000298 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009346, test:0.002775 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009104, test:0.000318 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007711, test:0.000898 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008817, test:0.001755 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008528, test:0.000869 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009117, test:0.003744 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008195, test:0.000829 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008312, test:0.000686 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007546, test:0.001819 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007002, test:0.000573 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008736, test:0.001920 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007380, test:0.002041 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008696, test:0.003084 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008604, test:0.006496 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008491, test:0.001983 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009449, test:0.001026 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010019, test:0.001369 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007993, test:0.000451 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006974, test:0.005372 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007448, test:0.000409 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008101, test:0.004039 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007850, test:0.001499 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007548, test:0.000645 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007745, test:0.000623 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007945, test:0.003487 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006966, test:0.000525 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007777, test:0.000623 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006909, test:0.000365 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.013588, test:0.000412 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008032, test:0.000504 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010414, test:0.000640 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007082, test:0.002053 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007860, test:0.001696 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007767, test:0.000430 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005535, test:0.000406 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005602, test:0.000434 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006626, test:0.000419 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005309, test:0.000359 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006656, test:0.000696 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005183, test:0.000421 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005122, test:0.000511 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005249, test:0.000583 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005089, test:0.000388 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005510, test:0.000393 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005249, test:0.000366 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005137, test:0.000374 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005288, test:0.000411 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005956, test:0.000393 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005601, test:0.000382 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005520, test:0.000363 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004964, test:0.000396 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006801, test:0.000356 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005533, test:0.000471 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005253, test:0.000543 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005265, test:0.000378 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005068, test:0.000397 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008371, test:0.000371 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005049, test:0.000395 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005231, test:0.000356 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005143, test:0.000379 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004963, test:0.000439 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005225, test:0.000475 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005159, test:0.000385 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004795, test:0.000360 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005632, test:0.000344 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.008946, test:0.000334 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005331, test:0.000404 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005398, test:0.000506 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005404, test:0.000325 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005245, test:0.000330 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005708, test:0.000361 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005891, test:0.000317 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004822, test:0.000360 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005972, test:0.000340 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006477, test:0.000345 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004861, test:0.000367 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004986, test:0.000370 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005015, test:0.000346 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005327, test:0.000364 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004873, test:0.000333 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005715, test:0.000393 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005474, test:0.000342 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005479, test:0.000326 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005910, test:0.000355 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005011, test:0.000336 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005815, test:0.000371 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005315, test:0.000340 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004774, test:0.000317 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004880, test:0.000364 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007591, test:0.000372 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004813, test:0.000334 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004630, test:0.000370 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005031, test:0.000367 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004963, test:0.000324 | lr:0.000100\n",
      "Mean absolute error:  1.7144089830637057\n",
      "Root mean squared error:  5.811514534834868\n",
      "Epoch[1/100] | loss train:0.053364, test:0.004878 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013407, test:0.004703 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011034, test:0.001734 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012031, test:0.000430 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010274, test:0.009973 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013890, test:0.007713 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009270, test:0.000355 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008986, test:0.002372 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007804, test:0.000478 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008588, test:0.003259 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009000, test:0.000430 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007711, test:0.000827 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013112, test:0.000658 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008997, test:0.000632 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007475, test:0.000518 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011186, test:0.004070 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007659, test:0.000593 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007351, test:0.001302 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006946, test:0.000556 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006889, test:0.000497 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006278, test:0.005547 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006872, test:0.000588 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007827, test:0.001049 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007315, test:0.000501 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[25/100] | loss train:0.007091, test:0.000784 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006854, test:0.000641 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006865, test:0.000712 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007321, test:0.000536 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007602, test:0.005370 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007812, test:0.000704 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008954, test:0.001864 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007076, test:0.001764 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008247, test:0.003342 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007041, test:0.000839 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006389, test:0.001877 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.031107, test:0.000446 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007643, test:0.000328 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007048, test:0.001005 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007491, test:0.000675 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006695, test:0.000865 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006179, test:0.000675 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005543, test:0.000378 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005198, test:0.000379 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005289, test:0.000408 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005107, test:0.000406 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005227, test:0.000388 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008300, test:0.000491 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004970, test:0.000365 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005879, test:0.000369 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005063, test:0.000448 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005087, test:0.000407 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004790, test:0.000405 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005666, test:0.000460 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005281, test:0.000357 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004592, test:0.000381 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005092, test:0.000543 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006642, test:0.000364 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005042, test:0.000469 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004956, test:0.000360 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005211, test:0.000424 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006138, test:0.000335 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005292, test:0.000407 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006088, test:0.000323 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004961, test:0.000347 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004974, test:0.000491 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004790, test:0.000290 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005935, test:0.000340 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005116, test:0.000320 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005273, test:0.000484 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005082, test:0.000333 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005025, test:0.000335 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005325, test:0.000535 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006969, test:0.000300 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004852, test:0.000323 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004845, test:0.000669 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005442, test:0.000562 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004811, test:0.000373 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005101, test:0.000308 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005237, test:0.000356 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005388, test:0.000335 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004805, test:0.000315 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004861, test:0.000312 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005939, test:0.000325 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005035, test:0.000283 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004994, test:0.000297 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004737, test:0.000332 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005130, test:0.000313 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004669, test:0.000279 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004876, test:0.000320 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004868, test:0.000307 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004861, test:0.000306 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004757, test:0.000299 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.006537, test:0.000292 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005764, test:0.000297 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005063, test:0.000306 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005189, test:0.000317 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004652, test:0.000302 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005881, test:0.000336 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005389, test:0.000329 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004837, test:0.000344 | lr:0.000100\n",
      "Mean absolute error:  1.8210380406781452\n",
      "Root mean squared error:  5.859590393136528\n",
      "Epoch[1/100] | loss train:0.061190, test:0.001133 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014352, test:0.001702 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015668, test:0.000424 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011806, test:0.000312 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008937, test:0.001926 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010859, test:0.013502 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010722, test:0.000715 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009485, test:0.003004 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008127, test:0.002103 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013572, test:0.000621 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008526, test:0.000880 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007910, test:0.000554 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007612, test:0.001168 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009139, test:0.002174 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008185, test:0.000593 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007804, test:0.003925 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008792, test:0.000490 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006725, test:0.000531 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007986, test:0.000755 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007226, test:0.001257 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007990, test:0.000394 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012129, test:0.003235 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007532, test:0.002081 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007944, test:0.000987 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007525, test:0.002544 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007590, test:0.000467 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010031, test:0.001864 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007134, test:0.000611 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006653, test:0.001128 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006594, test:0.002665 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007170, test:0.002661 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007306, test:0.001538 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006941, test:0.000418 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007349, test:0.000652 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007068, test:0.000521 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007100, test:0.001922 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007166, test:0.001766 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007317, test:0.000713 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007113, test:0.001173 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008031, test:0.000350 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005990, test:0.000351 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005673, test:0.000480 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005322, test:0.000420 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004972, test:0.000410 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005869, test:0.000586 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005828, test:0.000363 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006582, test:0.000347 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005495, test:0.000380 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005891, test:0.000490 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[50/100] | loss train:0.005452, test:0.000367 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004961, test:0.000365 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005333, test:0.000361 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005477, test:0.000413 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006032, test:0.000348 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004915, test:0.000361 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006303, test:0.000550 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005605, test:0.000505 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005028, test:0.000585 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005490, test:0.000380 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005055, test:0.000387 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005367, test:0.000362 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005833, test:0.000317 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005324, test:0.000343 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005287, test:0.000397 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004659, test:0.000315 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005294, test:0.000318 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005220, test:0.000343 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005380, test:0.000327 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005390, test:0.000492 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005567, test:0.000393 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005440, test:0.000405 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005553, test:0.000361 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004971, test:0.000349 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005005, test:0.000354 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005239, test:0.000365 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.008040, test:0.000320 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005381, test:0.000373 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005884, test:0.000302 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006658, test:0.000310 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006132, test:0.000379 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005089, test:0.000307 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005127, test:0.000329 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004991, test:0.000306 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005145, test:0.000317 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005489, test:0.000345 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005172, test:0.000367 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004874, test:0.000355 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005497, test:0.000366 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004978, test:0.000344 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006209, test:0.000325 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005309, test:0.000302 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005720, test:0.000320 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004913, test:0.000390 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005287, test:0.000378 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005004, test:0.000310 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004776, test:0.000350 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005020, test:0.000340 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005197, test:0.000322 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005554, test:0.000379 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004941, test:0.000333 | lr:0.000100\n",
      "Mean absolute error:  1.7057983831035852\n",
      "Root mean squared error:  5.804248744102116\n",
      "Epoch[1/100] | loss train:0.054050, test:0.001106 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013915, test:0.002441 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013162, test:0.003656 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010650, test:0.000918 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010700, test:0.004768 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011011, test:0.003074 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008201, test:0.000374 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008707, test:0.000684 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008567, test:0.000353 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008391, test:0.001115 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008382, test:0.000414 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007364, test:0.001167 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007354, test:0.001616 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007766, test:0.002267 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007610, test:0.000352 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008659, test:0.003115 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006893, test:0.001699 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008570, test:0.000490 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008654, test:0.000382 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006964, test:0.000428 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007384, test:0.000509 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006751, test:0.001391 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007650, test:0.002148 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008132, test:0.000412 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006819, test:0.000668 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007149, test:0.000718 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007850, test:0.002001 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007719, test:0.000430 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007285, test:0.002817 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007766, test:0.001816 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007206, test:0.000757 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008690, test:0.001868 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007327, test:0.000645 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007542, test:0.002999 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006608, test:0.000484 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007814, test:0.001725 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006847, test:0.002130 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007849, test:0.000566 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006304, test:0.001192 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007917, test:0.000932 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006378, test:0.000460 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005969, test:0.000455 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005022, test:0.000371 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005386, test:0.000395 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005337, test:0.000442 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005553, test:0.000393 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005136, test:0.000367 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005562, test:0.000625 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005001, test:0.000330 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007577, test:0.000338 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005251, test:0.000316 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005004, test:0.000345 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005385, test:0.000415 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006372, test:0.000381 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005277, test:0.000720 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008748, test:0.000522 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006960, test:0.000354 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005406, test:0.000336 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004954, test:0.000337 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005004, test:0.000362 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005309, test:0.000360 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005618, test:0.000357 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005573, test:0.000337 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.011756, test:0.000391 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005578, test:0.000348 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005860, test:0.000317 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005916, test:0.000341 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006068, test:0.000464 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005897, test:0.000385 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005676, test:0.000345 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005082, test:0.000422 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005992, test:0.000316 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005682, test:0.000394 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005472, test:0.000295 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[75/100] | loss train:0.005490, test:0.000317 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005581, test:0.000420 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004772, test:0.000339 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005647, test:0.000322 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005828, test:0.000350 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004847, test:0.000342 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005277, test:0.000351 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005171, test:0.000320 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005579, test:0.000311 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.007710, test:0.000387 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004782, test:0.000324 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004891, test:0.000313 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005544, test:0.000357 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004938, test:0.000360 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005024, test:0.000339 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006323, test:0.000345 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005224, test:0.000330 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.009600, test:0.000357 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005035, test:0.000368 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006103, test:0.000327 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005069, test:0.000321 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004399, test:0.000332 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005674, test:0.000319 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005053, test:0.000352 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.007415, test:0.000347 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004955, test:0.000324 | lr:0.000100\n",
      "Mean absolute error:  1.7281670477194262\n",
      "Root mean squared error:  5.792986452527751\n",
      "Epoch[1/100] | loss train:0.045077, test:0.000956 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012219, test:0.003712 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010548, test:0.000627 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011266, test:0.001926 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011369, test:0.001368 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010473, test:0.001470 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009874, test:0.001212 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009231, test:0.002718 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008297, test:0.000349 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008555, test:0.001742 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009616, test:0.007505 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007467, test:0.000400 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007255, test:0.002238 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007903, test:0.000891 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007436, test:0.001073 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010828, test:0.000502 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011458, test:0.005165 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007432, test:0.000520 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007229, test:0.001677 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008565, test:0.000651 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007677, test:0.000774 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007905, test:0.000447 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013101, test:0.000504 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007654, test:0.000695 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007229, test:0.001547 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007535, test:0.001143 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007520, test:0.000751 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007738, test:0.000473 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006800, test:0.001590 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008032, test:0.000970 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007033, test:0.000453 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008148, test:0.000515 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007178, test:0.000635 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007139, test:0.000760 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009351, test:0.001500 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006593, test:0.000922 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007208, test:0.001258 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006711, test:0.000412 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006891, test:0.001466 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006092, test:0.001335 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005544, test:0.000398 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005615, test:0.000596 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005616, test:0.000404 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004897, test:0.000433 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007954, test:0.000350 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005485, test:0.000352 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005269, test:0.000383 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004504, test:0.000337 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006106, test:0.000319 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005683, test:0.000328 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005162, test:0.000432 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005302, test:0.000535 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006257, test:0.000336 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005047, test:0.000355 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005785, test:0.000327 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005395, test:0.000464 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006160, test:0.000357 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005410, test:0.000363 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005000, test:0.000361 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005446, test:0.000323 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005197, test:0.000395 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005464, test:0.000812 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005240, test:0.000312 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005519, test:0.000522 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005082, test:0.000277 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004975, test:0.000384 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005319, test:0.000404 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005146, test:0.000290 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005014, test:0.000303 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005298, test:0.000356 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005216, test:0.000526 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005136, test:0.000304 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005086, test:0.000335 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004678, test:0.000341 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004770, test:0.000299 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005378, test:0.000302 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005159, test:0.000328 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005038, test:0.000350 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005105, test:0.000339 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005090, test:0.000311 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006789, test:0.000292 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004687, test:0.000336 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005329, test:0.000298 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005308, test:0.000323 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006132, test:0.000294 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005150, test:0.000319 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004896, test:0.000286 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004804, test:0.000282 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.007848, test:0.000298 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005319, test:0.000304 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005507, test:0.000298 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004877, test:0.000318 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004698, test:0.000306 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005549, test:0.000313 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005741, test:0.000305 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005134, test:0.000299 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005155, test:0.000333 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005058, test:0.000283 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004783, test:0.000327 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[100/100] | loss train:0.004888, test:0.000302 | lr:0.000100\n",
      "Mean absolute error:  1.7194946501342108\n",
      "Root mean squared error:  5.818934551156203\n",
      "Epoch[1/100] | loss train:0.066644, test:0.001071 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010978, test:0.001446 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009692, test:0.000328 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009477, test:0.014900 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010719, test:0.000296 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007868, test:0.000701 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007298, test:0.002456 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008816, test:0.004328 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010043, test:0.001691 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009884, test:0.000445 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008905, test:0.001143 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008783, test:0.001750 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008365, test:0.000781 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009075, test:0.000861 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007038, test:0.001825 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006919, test:0.000371 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007733, test:0.003744 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007161, test:0.002480 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006991, test:0.000362 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007523, test:0.002472 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007757, test:0.000974 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007941, test:0.000957 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006821, test:0.000905 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006773, test:0.001249 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006850, test:0.000454 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007111, test:0.003531 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007127, test:0.000463 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007299, test:0.000919 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006831, test:0.000591 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007804, test:0.001875 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008684, test:0.003811 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007410, test:0.000411 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007583, test:0.000461 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007085, test:0.001042 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008680, test:0.001505 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007457, test:0.001595 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009272, test:0.000581 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012321, test:0.000519 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008493, test:0.000353 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006520, test:0.000676 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005905, test:0.000535 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005475, test:0.000380 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005814, test:0.000368 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006251, test:0.000367 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005334, test:0.000333 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005289, test:0.000406 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005104, test:0.000389 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005056, test:0.000399 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005065, test:0.000771 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006981, test:0.000294 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005259, test:0.000326 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005030, test:0.000362 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007236, test:0.000478 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005379, test:0.000367 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004827, test:0.000352 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005369, test:0.000364 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006555, test:0.000356 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005297, test:0.000361 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009874, test:0.000355 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005572, test:0.000382 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004708, test:0.000429 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005862, test:0.000307 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004799, test:0.000396 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005025, test:0.000450 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004810, test:0.000358 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005052, test:0.000346 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008673, test:0.000313 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005569, test:0.000342 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005701, test:0.000295 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006184, test:0.000378 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005106, test:0.000410 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005227, test:0.000349 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005036, test:0.000331 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005567, test:0.000430 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006075, test:0.000374 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004737, test:0.000512 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005199, test:0.000312 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005115, test:0.000466 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.013492, test:0.000293 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005159, test:0.000445 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004858, test:0.000321 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006043, test:0.000325 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005001, test:0.000323 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005363, test:0.000325 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004928, test:0.000344 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004623, test:0.000328 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004970, test:0.000325 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005810, test:0.000327 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004799, test:0.000316 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005082, test:0.000334 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005345, test:0.000329 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005456, test:0.000359 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005507, test:0.000336 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004980, test:0.000334 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005254, test:0.000312 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005465, test:0.000322 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004866, test:0.000367 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005089, test:0.000343 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006250, test:0.000295 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005134, test:0.000309 | lr:0.000100\n",
      "Mean absolute error:  1.7178930720041037\n",
      "Root mean squared error:  5.802159618645223\n",
      "Epoch[1/100] | loss train:0.064316, test:0.001701 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014139, test:0.000862 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009808, test:0.000556 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012486, test:0.003531 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008790, test:0.000366 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009035, test:0.000795 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008817, test:0.001138 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008709, test:0.001574 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009977, test:0.002314 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010711, test:0.000327 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009185, test:0.004323 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008042, test:0.000823 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.021315, test:0.007885 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.016069, test:0.000620 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008593, test:0.000301 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008282, test:0.000611 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009675, test:0.000455 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008094, test:0.000453 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008108, test:0.000695 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008505, test:0.001771 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007808, test:0.000635 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.016084, test:0.000518 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009044, test:0.001998 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[24/100] | loss train:0.009523, test:0.000611 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009335, test:0.001607 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008379, test:0.002616 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008531, test:0.000899 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006950, test:0.001328 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008227, test:0.000338 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009026, test:0.000940 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008370, test:0.002149 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007070, test:0.000477 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007906, test:0.000305 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007593, test:0.000389 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006175, test:0.000290 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009519, test:0.000652 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007512, test:0.000354 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007467, test:0.000437 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007744, test:0.000407 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006427, test:0.000667 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005903, test:0.000355 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005371, test:0.000374 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005497, test:0.000331 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005663, test:0.000460 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005129, test:0.000318 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005528, test:0.000394 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005973, test:0.000328 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005323, test:0.000427 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006749, test:0.000422 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005589, test:0.000314 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005931, test:0.000293 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005379, test:0.000339 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005664, test:0.000309 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005247, test:0.000282 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005741, test:0.001164 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007068, test:0.000328 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005682, test:0.000309 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005326, test:0.000781 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004962, test:0.000310 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005896, test:0.000374 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004830, test:0.000325 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005546, test:0.000386 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005727, test:0.000325 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005526, test:0.000368 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004907, test:0.000312 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005678, test:0.000549 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007609, test:0.000297 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005625, test:0.000316 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006126, test:0.000356 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005280, test:0.000468 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005939, test:0.000466 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005410, test:0.000298 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004669, test:0.000296 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005032, test:0.000420 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004949, test:0.000298 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005380, test:0.000988 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005525, test:0.000390 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005324, test:0.000524 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005317, test:0.000341 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005400, test:0.000643 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005263, test:0.000305 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005254, test:0.000303 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.006040, test:0.000289 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004777, test:0.000299 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006007, test:0.000296 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005356, test:0.000288 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005099, test:0.000291 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005217, test:0.000283 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005061, test:0.000291 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005546, test:0.000337 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004890, test:0.000372 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005214, test:0.000274 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004976, test:0.000290 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005287, test:0.000319 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005119, test:0.000328 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005350, test:0.000333 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005149, test:0.000301 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004783, test:0.000322 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004859, test:0.000356 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004968, test:0.000283 | lr:0.000100\n",
      "Mean absolute error:  1.6582705388014547\n",
      "Root mean squared error:  5.774196584523832\n",
      "Epoch[1/100] | loss train:0.048717, test:0.001722 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011571, test:0.003326 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011653, test:0.000535 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011571, test:0.003212 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010721, test:0.000842 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008670, test:0.000484 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008417, test:0.004641 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009784, test:0.000564 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008430, test:0.001803 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009789, test:0.004388 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007770, test:0.001559 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009466, test:0.001648 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008521, test:0.001114 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008710, test:0.000368 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008198, test:0.000474 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007449, test:0.000605 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010375, test:0.000565 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011498, test:0.003217 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008127, test:0.002545 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008814, test:0.004926 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008973, test:0.001464 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007701, test:0.000477 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006679, test:0.000537 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010347, test:0.002206 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008415, test:0.000587 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008212, test:0.000402 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008377, test:0.001536 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006880, test:0.000438 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008678, test:0.000930 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008704, test:0.001361 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007525, test:0.000929 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008458, test:0.000467 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006652, test:0.001079 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006868, test:0.000457 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007702, test:0.002093 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007127, test:0.000549 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007057, test:0.002666 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007683, test:0.001333 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006513, test:0.000500 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007550, test:0.000722 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005604, test:0.000376 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005217, test:0.000414 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006371, test:0.000337 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005352, test:0.000336 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009400, test:0.000339 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005741, test:0.000407 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005279, test:0.000541 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006565, test:0.000321 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[49/100] | loss train:0.004747, test:0.000319 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005331, test:0.000401 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005012, test:0.000323 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005817, test:0.000324 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005115, test:0.000372 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004899, test:0.000328 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006535, test:0.000380 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005704, test:0.000323 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004611, test:0.000464 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005525, test:0.000367 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005110, test:0.000306 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005303, test:0.000472 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005601, test:0.000324 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005121, test:0.000299 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005089, test:0.000414 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004775, test:0.000319 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005199, test:0.000507 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005016, test:0.000400 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005593, test:0.000301 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005689, test:0.000303 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005671, test:0.000504 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005113, test:0.000296 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008820, test:0.000325 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005167, test:0.000345 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005310, test:0.000672 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004791, test:0.000384 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004777, test:0.000333 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005394, test:0.000319 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.016017, test:0.000308 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005118, test:0.000377 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005269, test:0.000338 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005815, test:0.000319 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005282, test:0.000324 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004645, test:0.000304 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005593, test:0.000384 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005343, test:0.000302 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005047, test:0.000297 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005331, test:0.000282 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005000, test:0.000301 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005042, test:0.000296 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004490, test:0.000315 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005040, test:0.000342 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005605, test:0.000300 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005229, test:0.000281 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004981, test:0.000304 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005652, test:0.000319 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004972, test:0.000332 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004909, test:0.000304 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005190, test:0.000289 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004627, test:0.000282 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005185, test:0.000297 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004866, test:0.000320 | lr:0.000100\n",
      "Mean absolute error:  1.6537020905728872\n",
      "Root mean squared error:  5.771053538871315\n",
      "Epoch[1/100] | loss train:0.051540, test:0.003434 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009955, test:0.001108 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009834, test:0.000535 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009467, test:0.005579 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009350, test:0.005123 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008078, test:0.001229 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008520, test:0.000493 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007417, test:0.000377 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008294, test:0.000759 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009212, test:0.003464 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008838, test:0.000598 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008430, test:0.000765 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007558, test:0.000318 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007419, test:0.000318 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008365, test:0.000352 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007262, test:0.000348 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007664, test:0.005974 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008712, test:0.000403 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007478, test:0.002052 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008616, test:0.002055 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007424, test:0.000531 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008246, test:0.003896 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006688, test:0.002200 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008085, test:0.004816 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009216, test:0.002810 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007218, test:0.000673 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008141, test:0.000407 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007108, test:0.002004 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006925, test:0.000801 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006921, test:0.000809 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007246, test:0.000614 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006778, test:0.001926 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009858, test:0.000729 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007672, test:0.000494 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007485, test:0.001037 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007097, test:0.000492 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007460, test:0.001596 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006016, test:0.000863 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009719, test:0.000592 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007599, test:0.000453 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005259, test:0.000392 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005558, test:0.000385 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005284, test:0.000561 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005075, test:0.000387 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005537, test:0.000412 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005130, test:0.000389 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005639, test:0.000391 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005382, test:0.000315 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005774, test:0.000318 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005864, test:0.000334 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006424, test:0.000352 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004456, test:0.000559 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005207, test:0.000478 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005089, test:0.000390 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005207, test:0.000519 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005650, test:0.000353 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005021, test:0.000403 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005862, test:0.000343 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005342, test:0.000392 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005392, test:0.000485 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005016, test:0.000324 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005054, test:0.000326 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005474, test:0.000329 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005374, test:0.000394 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005343, test:0.000325 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004926, test:0.000310 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004985, test:0.000298 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006485, test:0.000315 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005014, test:0.000298 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005259, test:0.000307 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005307, test:0.000559 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005219, test:0.000472 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007236, test:0.000496 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[74/100] | loss train:0.006143, test:0.000321 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006970, test:0.000412 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005631, test:0.000323 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005164, test:0.000376 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005033, test:0.000891 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005323, test:0.000365 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005992, test:0.000402 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005531, test:0.000359 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005356, test:0.000347 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005968, test:0.000332 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004589, test:0.000322 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004889, test:0.000342 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005209, test:0.000342 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005761, test:0.000335 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005442, test:0.000357 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005312, test:0.000355 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005372, test:0.000355 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005432, test:0.000329 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005213, test:0.000346 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005006, test:0.000350 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004968, test:0.000307 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.006975, test:0.000308 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007622, test:0.000313 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004734, test:0.000354 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004619, test:0.000302 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005072, test:0.000360 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005104, test:0.000323 | lr:0.000100\n",
      "Mean absolute error:  1.68017326820285\n",
      "Root mean squared error:  5.791699973824901\n",
      "Epoch[1/100] | loss train:0.060780, test:0.008042 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015253, test:0.003525 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009812, test:0.002215 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009930, test:0.003149 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011169, test:0.001317 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013308, test:0.000652 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008194, test:0.000430 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008189, test:0.001874 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012294, test:0.000363 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009907, test:0.000407 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008307, test:0.002401 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009715, test:0.001623 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008831, test:0.000587 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012702, test:0.000407 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008717, test:0.000430 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007042, test:0.001733 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009360, test:0.001227 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009110, test:0.000436 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007655, test:0.000473 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012099, test:0.000835 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009064, test:0.000465 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013799, test:0.000696 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008115, test:0.000437 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007201, test:0.002707 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007536, test:0.000656 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008199, test:0.001898 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006891, test:0.001371 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006677, test:0.000938 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007526, test:0.004017 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007191, test:0.000575 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007567, test:0.000995 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007268, test:0.001025 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006736, test:0.000451 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006645, test:0.000789 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007185, test:0.001223 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007702, test:0.000569 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010903, test:0.001562 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007551, test:0.000437 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007186, test:0.000567 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006618, test:0.001251 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006347, test:0.000363 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005693, test:0.000366 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005583, test:0.000350 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006153, test:0.000379 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005976, test:0.000348 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006013, test:0.000506 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005821, test:0.000337 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005162, test:0.000346 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004877, test:0.000320 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005324, test:0.000396 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005317, test:0.000448 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008528, test:0.000359 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006084, test:0.000597 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005265, test:0.000358 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005456, test:0.000364 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006303, test:0.000331 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005146, test:0.000524 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006564, test:0.000337 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006859, test:0.000378 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005347, test:0.000407 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005589, test:0.001049 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005997, test:0.000652 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005528, test:0.000310 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005087, test:0.000317 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005187, test:0.000385 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007792, test:0.000342 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005385, test:0.000363 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007332, test:0.000347 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.011240, test:0.000322 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.008952, test:0.000372 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005148, test:0.000419 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005719, test:0.000371 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005328, test:0.000454 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005799, test:0.000463 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005394, test:0.000541 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005770, test:0.000771 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005388, test:0.000340 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005771, test:0.000330 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006787, test:0.000377 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005828, test:0.000420 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005507, test:0.000301 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005409, test:0.000296 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005300, test:0.000326 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005622, test:0.000339 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005369, test:0.000327 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005013, test:0.000319 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005400, test:0.000288 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004965, test:0.000320 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.006004, test:0.000316 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005609, test:0.000313 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005255, test:0.000299 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005421, test:0.000333 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005978, test:0.000325 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006954, test:0.000318 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005339, test:0.000328 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005187, test:0.000314 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.008748, test:0.000333 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006731, test:0.000342 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[99/100] | loss train:0.005244, test:0.000309 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004879, test:0.000318 | lr:0.000100\n",
      "Mean absolute error:  1.709867989037527\n",
      "Root mean squared error:  5.816058556690193\n",
      "Epoch[1/100] | loss train:0.054235, test:0.003972 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012825, test:0.005106 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010329, test:0.000332 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009883, test:0.000292 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010539, test:0.000319 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010006, test:0.000401 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008832, test:0.002241 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009545, test:0.000482 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007430, test:0.001673 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009638, test:0.001036 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008192, test:0.000525 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009521, test:0.002693 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007497, test:0.000391 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012411, test:0.000403 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009168, test:0.002754 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009491, test:0.000472 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008087, test:0.002908 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007134, test:0.000417 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008395, test:0.001205 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007313, test:0.001323 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007890, test:0.004372 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009441, test:0.000453 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007196, test:0.000393 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007851, test:0.001290 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008064, test:0.001704 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007984, test:0.000607 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010815, test:0.000398 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007007, test:0.000619 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007264, test:0.000433 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007882, test:0.000812 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007096, test:0.000522 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006980, test:0.000886 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007002, test:0.000853 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007534, test:0.001803 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006554, test:0.001879 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007273, test:0.000485 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009114, test:0.000473 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007029, test:0.000695 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006568, test:0.002321 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006923, test:0.001065 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005845, test:0.000556 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006943, test:0.000433 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005960, test:0.000396 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006541, test:0.000469 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005536, test:0.000389 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006039, test:0.000366 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005509, test:0.000363 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005440, test:0.000405 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005382, test:0.000338 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005646, test:0.000400 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005095, test:0.000352 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005039, test:0.000384 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005459, test:0.000483 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004731, test:0.000337 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006366, test:0.000343 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004946, test:0.000333 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004935, test:0.000424 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004835, test:0.000419 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007425, test:0.000751 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007595, test:0.000383 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005368, test:0.000344 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005704, test:0.000411 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004961, test:0.000309 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005329, test:0.000337 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006850, test:0.000337 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005573, test:0.000342 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004903, test:0.000443 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005400, test:0.000354 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005058, test:0.000330 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005097, test:0.000332 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005131, test:0.000345 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005101, test:0.000310 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005180, test:0.000437 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004863, test:0.000376 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005071, test:0.000307 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005322, test:0.000330 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005229, test:0.000299 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005294, test:0.000342 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006896, test:0.000315 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004909, test:0.000347 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006608, test:0.000299 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005070, test:0.000303 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005382, test:0.000289 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004915, test:0.000307 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004896, test:0.000291 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005535, test:0.000283 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004965, test:0.000303 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004850, test:0.000324 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004815, test:0.000341 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005309, test:0.000298 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004560, test:0.000292 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004592, test:0.000312 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004668, test:0.000326 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005210, test:0.000339 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.006035, test:0.000314 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005099, test:0.000323 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004863, test:0.000334 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004961, test:0.000281 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005074, test:0.000321 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005699, test:0.000297 | lr:0.000100\n",
      "Mean absolute error:  1.6887124750806424\n",
      "Root mean squared error:  5.792715665295249\n",
      "Epoch[1/100] | loss train:0.072940, test:0.000737 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011469, test:0.000888 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008952, test:0.000355 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009261, test:0.000594 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009543, test:0.000295 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009528, test:0.000661 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009737, test:0.003735 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009157, test:0.002774 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007754, test:0.001732 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010057, test:0.001929 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012332, test:0.000466 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008314, test:0.000725 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009308, test:0.001559 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007662, test:0.000585 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007457, test:0.002413 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007788, test:0.000459 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008278, test:0.001812 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007324, test:0.005476 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007464, test:0.001257 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008662, test:0.000602 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007854, test:0.002010 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007425, test:0.000718 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/100] | loss train:0.006160, test:0.001368 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006808, test:0.001233 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007888, test:0.000702 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010594, test:0.003351 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008266, test:0.000586 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006531, test:0.002833 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006466, test:0.001109 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007375, test:0.001658 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006234, test:0.000626 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007315, test:0.000386 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006977, test:0.000403 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007529, test:0.000446 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007015, test:0.001459 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006137, test:0.001042 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.020289, test:0.001980 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007988, test:0.000654 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006559, test:0.000669 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007315, test:0.002526 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006368, test:0.000401 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008473, test:0.000441 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005559, test:0.000422 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005500, test:0.000374 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005885, test:0.000393 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005103, test:0.000375 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005548, test:0.000354 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005777, test:0.000348 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005119, test:0.000425 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005565, test:0.000382 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005179, test:0.000364 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006561, test:0.000427 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005538, test:0.000376 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005768, test:0.000340 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005644, test:0.000342 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006844, test:0.000387 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004849, test:0.000297 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005332, test:0.000518 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006466, test:0.000452 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005511, test:0.000339 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005262, test:0.000393 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005231, test:0.000333 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005546, test:0.000771 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005479, test:0.000334 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005041, test:0.000384 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004818, test:0.000399 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005384, test:0.000398 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005065, test:0.000342 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005711, test:0.000446 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005158, test:0.000809 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004909, test:0.000350 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005668, test:0.000312 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005119, test:0.000278 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005286, test:0.000319 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004922, test:0.000467 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005706, test:0.000358 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005116, test:0.000320 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005130, test:0.000323 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005368, test:0.000312 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005251, test:0.000543 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005024, test:0.000362 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004727, test:0.000408 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005340, test:0.000332 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004817, test:0.000313 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005141, test:0.000301 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004476, test:0.000316 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004906, test:0.000322 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005004, test:0.000330 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005130, test:0.000359 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005432, test:0.000331 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004464, test:0.000322 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005490, test:0.000317 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004956, test:0.000341 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004976, test:0.000365 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.008420, test:0.000308 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005275, test:0.000337 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005234, test:0.000324 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005514, test:0.000332 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004773, test:0.000329 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005480, test:0.000319 | lr:0.000100\n",
      "Mean absolute error:  1.7201386430536532\n",
      "Root mean squared error:  5.813192253877584\n",
      "Epoch[1/100] | loss train:0.057806, test:0.003581 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011148, test:0.000727 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009157, test:0.000490 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009931, test:0.002054 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011959, test:0.007399 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009481, test:0.000914 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009013, test:0.000288 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008956, test:0.001192 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007753, test:0.000939 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008763, test:0.002508 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007176, test:0.000557 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008106, test:0.003480 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007459, test:0.000461 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008071, test:0.000383 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008919, test:0.000525 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007116, test:0.001857 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008386, test:0.000392 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.021050, test:0.000392 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008968, test:0.000359 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008426, test:0.000336 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007792, test:0.003145 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007132, test:0.000276 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007283, test:0.001862 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010158, test:0.000444 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007953, test:0.000303 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008178, test:0.000661 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006912, test:0.000544 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007349, test:0.000288 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006917, test:0.000364 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006564, test:0.001181 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008028, test:0.000780 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006542, test:0.000417 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007027, test:0.004074 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007021, test:0.003850 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007192, test:0.000302 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007309, test:0.000481 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007066, test:0.000333 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006597, test:0.000589 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011509, test:0.002064 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008367, test:0.001210 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006013, test:0.000304 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005282, test:0.000323 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004994, test:0.000316 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004983, test:0.000417 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006357, test:0.000315 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005548, test:0.000323 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008378, test:0.000377 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[48/100] | loss train:0.008246, test:0.000286 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005292, test:0.000279 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004965, test:0.000282 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005614, test:0.000319 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005905, test:0.000372 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005898, test:0.000292 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004927, test:0.000388 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005228, test:0.000281 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005859, test:0.000281 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005400, test:0.000313 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005026, test:0.000289 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004944, test:0.000297 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005214, test:0.000278 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005006, test:0.000551 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004956, test:0.000303 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005251, test:0.000492 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005045, test:0.000281 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005090, test:0.000292 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004654, test:0.000309 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005547, test:0.000320 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005698, test:0.000337 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005737, test:0.000287 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004719, test:0.000311 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005374, test:0.000295 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004940, test:0.000329 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005034, test:0.000342 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005235, test:0.000291 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007287, test:0.000360 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004916, test:0.000288 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005237, test:0.000282 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006253, test:0.000281 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005749, test:0.000293 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004803, test:0.000477 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004786, test:0.000305 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004460, test:0.000280 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004697, test:0.000294 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005318, test:0.000271 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004762, test:0.000283 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005222, test:0.000290 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.007956, test:0.000320 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005064, test:0.000260 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005070, test:0.000291 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004988, test:0.000287 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005053, test:0.000293 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004887, test:0.000316 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005048, test:0.000274 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005103, test:0.000290 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004817, test:0.000274 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004849, test:0.000283 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005124, test:0.000312 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005937, test:0.000289 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005815, test:0.000293 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005073, test:0.000285 | lr:0.000100\n",
      "Mean absolute error:  1.627595548864707\n",
      "Root mean squared error:  5.750418163171882\n",
      "Epoch[1/100] | loss train:0.047579, test:0.000502 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010339, test:0.012347 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011267, test:0.000470 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009139, test:0.000653 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009465, test:0.012977 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010810, test:0.002025 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008556, test:0.003072 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013646, test:0.000398 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008758, test:0.001793 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008417, test:0.000484 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008971, test:0.001226 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008274, test:0.000589 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007898, test:0.005229 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008877, test:0.000370 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008428, test:0.002057 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008577, test:0.004889 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008739, test:0.001101 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007568, test:0.001420 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008094, test:0.001804 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008122, test:0.000972 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007863, test:0.000896 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007124, test:0.000707 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007858, test:0.000538 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009633, test:0.000506 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007666, test:0.001109 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007593, test:0.001362 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006475, test:0.003241 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011211, test:0.000364 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007398, test:0.002903 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009964, test:0.002085 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008793, test:0.001144 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007437, test:0.000315 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006903, test:0.000453 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007243, test:0.000386 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007303, test:0.003494 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007937, test:0.001242 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007066, test:0.001496 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007362, test:0.000452 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008734, test:0.000497 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006705, test:0.000625 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006121, test:0.000394 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006001, test:0.000351 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005808, test:0.000319 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004977, test:0.000435 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005101, test:0.000377 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006162, test:0.000316 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004939, test:0.000328 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005401, test:0.000316 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005198, test:0.000319 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005034, test:0.000468 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005017, test:0.000303 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004964, test:0.000282 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005097, test:0.000269 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006038, test:0.000391 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006244, test:0.000350 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005481, test:0.000264 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006180, test:0.000347 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005112, test:0.000290 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005230, test:0.000459 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005000, test:0.000275 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005218, test:0.000272 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007074, test:0.000286 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005385, test:0.000300 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005168, test:0.000290 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005570, test:0.000959 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005443, test:0.000285 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004919, test:0.000366 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004800, test:0.000356 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005453, test:0.000438 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005712, test:0.000278 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005590, test:0.000274 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005498, test:0.000451 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[73/100] | loss train:0.005319, test:0.000487 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006304, test:0.000290 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005583, test:0.000429 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007062, test:0.000288 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005421, test:0.000381 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005311, test:0.000308 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005497, test:0.000306 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006748, test:0.000256 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006025, test:0.000300 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004681, test:0.000294 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004661, test:0.000278 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004834, test:0.000290 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005049, test:0.000273 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005711, test:0.000266 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005538, test:0.000261 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004986, test:0.000272 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005022, test:0.000277 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005819, test:0.000284 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005510, test:0.000291 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004867, test:0.000296 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005450, test:0.000274 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006372, test:0.000302 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004808, test:0.000314 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005861, test:0.000272 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005160, test:0.000297 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004838, test:0.000284 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004817, test:0.000263 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005968, test:0.000317 | lr:0.000100\n",
      "Mean absolute error:  1.765133959576226\n",
      "Root mean squared error:  5.830024830342465\n",
      "Epoch[1/100] | loss train:0.058233, test:0.000956 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013534, test:0.007718 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010992, test:0.001551 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011615, test:0.000371 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011038, test:0.007039 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009894, test:0.003586 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008424, test:0.002370 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009568, test:0.001051 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008021, test:0.000831 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008960, test:0.000630 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008404, test:0.001485 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009079, test:0.000347 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008306, test:0.001327 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008130, test:0.000659 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008231, test:0.001066 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008560, test:0.000418 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007434, test:0.000393 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007756, test:0.000551 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008352, test:0.001974 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008064, test:0.000691 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007902, test:0.000816 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006935, test:0.000408 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009550, test:0.001177 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007465, test:0.000942 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008279, test:0.000335 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008460, test:0.002321 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007106, test:0.000337 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007984, test:0.000389 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008703, test:0.004622 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007845, test:0.000351 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008148, test:0.000303 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006553, test:0.000522 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007863, test:0.000361 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008352, test:0.003425 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007901, test:0.000608 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007834, test:0.000425 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007183, test:0.000337 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006672, test:0.001680 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006833, test:0.000560 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006513, test:0.000553 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005234, test:0.000475 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005250, test:0.000430 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005328, test:0.000360 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005298, test:0.000293 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005482, test:0.000306 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005121, test:0.000330 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005603, test:0.000346 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009488, test:0.000307 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005136, test:0.000334 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005239, test:0.000309 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008061, test:0.000419 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005375, test:0.000282 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005370, test:0.000271 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005126, test:0.000409 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006063, test:0.000283 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005194, test:0.000290 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009068, test:0.000285 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005031, test:0.000297 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005095, test:0.000350 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005032, test:0.000314 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005973, test:0.000384 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005640, test:0.000308 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004976, test:0.000354 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005477, test:0.000274 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005446, test:0.000285 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005566, test:0.000296 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006015, test:0.000278 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005502, test:0.000315 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005598, test:0.000291 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005359, test:0.000275 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005315, test:0.000357 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005136, test:0.000287 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005306, test:0.000301 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005071, test:0.000287 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.009815, test:0.000267 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.010609, test:0.000297 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005740, test:0.000298 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005048, test:0.000321 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005850, test:0.000287 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006265, test:0.000306 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006679, test:0.000276 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005188, test:0.000288 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005658, test:0.000312 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004584, test:0.000301 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004949, test:0.000279 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.009684, test:0.000293 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006787, test:0.000276 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005705, test:0.000291 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004855, test:0.000273 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004994, test:0.000261 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004842, test:0.000281 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006580, test:0.000265 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005040, test:0.000290 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004590, test:0.000285 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004840, test:0.000260 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005474, test:0.000270 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004579, test:0.000272 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[98/100] | loss train:0.005322, test:0.000286 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005305, test:0.000302 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005038, test:0.000316 | lr:0.000100\n",
      "Mean absolute error:  1.6350400342534306\n",
      "Root mean squared error:  5.7583378282531505\n",
      "Epoch[1/100] | loss train:0.052323, test:0.003273 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010567, test:0.006536 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010233, test:0.002776 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010143, test:0.002013 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008764, test:0.000343 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009289, test:0.000320 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008101, test:0.000978 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009057, test:0.000608 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006650, test:0.000466 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009691, test:0.000409 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009639, test:0.004261 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010792, test:0.000442 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009461, test:0.000486 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008956, test:0.000587 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007513, test:0.001547 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006995, test:0.001028 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007174, test:0.000674 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013087, test:0.002010 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007783, test:0.001084 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007056, test:0.000455 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006916, test:0.000374 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011649, test:0.001246 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007499, test:0.000738 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007381, test:0.001631 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007230, test:0.002782 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007679, test:0.000572 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007612, test:0.000694 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008554, test:0.000637 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010084, test:0.001748 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008142, test:0.000410 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009965, test:0.000642 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007514, test:0.000980 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006982, test:0.002773 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007480, test:0.001085 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006675, test:0.000591 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005875, test:0.000643 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006137, test:0.000373 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006788, test:0.000754 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006094, test:0.000957 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.017356, test:0.002130 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006240, test:0.000511 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005424, test:0.000419 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006721, test:0.000476 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005343, test:0.000383 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005132, test:0.000338 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005210, test:0.000503 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006164, test:0.000403 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004593, test:0.000341 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004496, test:0.000442 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005541, test:0.000488 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005285, test:0.000396 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005362, test:0.000415 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007277, test:0.000335 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004726, test:0.000384 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005984, test:0.000325 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004973, test:0.000310 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005080, test:0.000337 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005488, test:0.000329 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004918, test:0.000480 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006118, test:0.000439 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005624, test:0.000321 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005073, test:0.000299 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005752, test:0.000275 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006148, test:0.000503 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005484, test:0.000294 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005764, test:0.000369 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005118, test:0.000299 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005109, test:0.000341 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004973, test:0.000329 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004889, test:0.000312 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004672, test:0.000303 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004987, test:0.000377 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005554, test:0.000343 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004796, test:0.000340 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005363, test:0.000351 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005065, test:0.000326 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005345, test:0.000324 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.012171, test:0.000398 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004846, test:0.000309 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005092, test:0.000609 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005375, test:0.000315 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005052, test:0.000326 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004210, test:0.000313 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006694, test:0.000308 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004846, test:0.000331 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005217, test:0.000352 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005432, test:0.000330 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005077, test:0.000304 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005350, test:0.000310 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004910, test:0.000295 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004563, test:0.000288 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005094, test:0.000290 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.006627, test:0.000333 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005231, test:0.000314 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004615, test:0.000309 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004871, test:0.000314 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004792, test:0.000314 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005901, test:0.000306 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005173, test:0.000323 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005006, test:0.000369 | lr:0.000100\n",
      "Mean absolute error:  1.772151721967013\n",
      "Root mean squared error:  5.856589924342988\n",
      "Epoch[1/100] | loss train:0.054619, test:0.000425 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012471, test:0.001009 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012585, test:0.000537 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013078, test:0.002848 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010946, test:0.000773 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009528, test:0.001351 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007742, test:0.001703 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008196, test:0.000338 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008903, test:0.001233 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008779, test:0.000343 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008673, test:0.000508 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007994, test:0.006350 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008793, test:0.000584 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008403, test:0.001439 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008514, test:0.001285 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007173, test:0.000686 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008486, test:0.001970 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008217, test:0.000927 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007733, test:0.001047 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007526, test:0.000462 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007480, test:0.002383 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[22/100] | loss train:0.006910, test:0.001833 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007287, test:0.003627 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008135, test:0.002256 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007967, test:0.000411 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007962, test:0.000601 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009720, test:0.000469 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008468, test:0.002293 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007443, test:0.000427 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007548, test:0.001060 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006920, test:0.000947 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008260, test:0.001097 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006906, test:0.002214 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008334, test:0.000573 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007141, test:0.000660 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007499, test:0.002183 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008054, test:0.001243 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006954, test:0.000895 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008084, test:0.002780 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007948, test:0.000558 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005924, test:0.000422 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005327, test:0.000477 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008308, test:0.000531 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005921, test:0.000362 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005686, test:0.000388 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006007, test:0.000373 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005735, test:0.000367 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005395, test:0.000387 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005780, test:0.000355 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005160, test:0.000440 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005638, test:0.000489 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005934, test:0.000344 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005198, test:0.000520 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005629, test:0.000494 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005269, test:0.000391 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006214, test:0.000372 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006975, test:0.000373 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005920, test:0.000542 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005377, test:0.000382 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005685, test:0.000331 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005960, test:0.000509 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005242, test:0.000638 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005342, test:0.000392 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005462, test:0.000455 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005887, test:0.000445 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005695, test:0.000596 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004927, test:0.000347 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005480, test:0.000465 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005171, test:0.000379 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007390, test:0.000418 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005326, test:0.000346 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005338, test:0.000520 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004821, test:0.000464 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005423, test:0.000319 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005885, test:0.000446 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005173, test:0.000384 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005380, test:0.000692 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005178, test:0.000329 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005051, test:0.000330 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005588, test:0.000326 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004924, test:0.000317 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005029, test:0.000355 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005666, test:0.000384 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004862, test:0.000343 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005101, test:0.000345 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004712, test:0.000333 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005343, test:0.000346 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005443, test:0.000320 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004706, test:0.000341 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006229, test:0.000345 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005057, test:0.000327 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005692, test:0.000307 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005070, test:0.000337 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004982, test:0.000314 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005174, test:0.000315 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005089, test:0.000337 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004831, test:0.000344 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004571, test:0.000336 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005463, test:0.000353 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.007820, test:0.000345 | lr:0.000100\n",
      "Mean absolute error:  1.6509188032775408\n",
      "Root mean squared error:  5.7879033334304\n",
      "Epoch[1/100] | loss train:0.054378, test:0.010302 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015943, test:0.000769 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010049, test:0.012319 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010936, test:0.000312 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012368, test:0.002266 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010993, test:0.000306 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011809, test:0.005871 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010220, test:0.000340 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008208, test:0.001633 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015974, test:0.002863 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010254, test:0.000333 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009472, test:0.002580 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007850, test:0.004994 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008116, test:0.000405 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008880, test:0.000845 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009777, test:0.002822 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011376, test:0.000534 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007743, test:0.000434 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007980, test:0.000376 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006937, test:0.000530 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008351, test:0.001302 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007882, test:0.003007 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007940, test:0.002205 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008490, test:0.000780 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006588, test:0.000571 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007202, test:0.002818 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007598, test:0.001044 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006990, test:0.001136 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007249, test:0.001454 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007789, test:0.000725 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009456, test:0.000424 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007923, test:0.000375 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006263, test:0.000406 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006547, test:0.001222 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006532, test:0.008397 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007005, test:0.002442 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007054, test:0.001427 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007210, test:0.000390 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006836, test:0.000435 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006991, test:0.008755 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006350, test:0.000350 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.010887, test:0.000661 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006402, test:0.000322 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005321, test:0.000451 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005436, test:0.000323 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005165, test:0.000313 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/100] | loss train:0.005396, test:0.000368 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005298, test:0.000292 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005199, test:0.000342 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005419, test:0.000338 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005793, test:0.000356 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006022, test:0.000349 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007608, test:0.000306 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005502, test:0.000342 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004927, test:0.000398 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005647, test:0.000306 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005302, test:0.000294 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004759, test:0.000265 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005627, test:0.000334 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005197, test:0.000320 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005522, test:0.000332 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005066, test:0.000288 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005930, test:0.000485 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005276, test:0.000302 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005305, test:0.000340 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005147, test:0.000277 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004589, test:0.000483 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005080, test:0.000395 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005623, test:0.000324 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005962, test:0.000321 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005673, test:0.000423 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005820, test:0.000314 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005742, test:0.000345 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006575, test:0.000270 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005838, test:0.000352 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007413, test:0.000471 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006061, test:0.000263 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005676, test:0.000464 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005793, test:0.000293 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005643, test:0.000349 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005613, test:0.000281 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.009983, test:0.000287 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004981, test:0.000277 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004950, test:0.000284 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004605, test:0.000267 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004832, test:0.000287 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004630, test:0.000298 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005525, test:0.000300 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005337, test:0.000273 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005059, test:0.000302 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.007684, test:0.000276 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004683, test:0.000300 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004570, test:0.000327 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004874, test:0.000319 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005211, test:0.000302 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004857, test:0.000297 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.007011, test:0.000296 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006177, test:0.000288 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005258, test:0.000280 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004616, test:0.000311 | lr:0.000100\n",
      "Mean absolute error:  1.742711881926277\n",
      "Root mean squared error:  5.812850408709531\n",
      "Epoch[1/100] | loss train:0.058616, test:0.001533 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012519, test:0.001360 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012520, test:0.000829 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009108, test:0.001401 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008603, test:0.009070 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009636, test:0.003705 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009885, test:0.000415 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008772, test:0.004454 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010429, test:0.000869 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008077, test:0.000762 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009042, test:0.003099 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009289, test:0.003679 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009738, test:0.000749 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008761, test:0.010727 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010088, test:0.000470 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011677, test:0.001188 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008741, test:0.000377 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008856, test:0.000792 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009029, test:0.002094 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007471, test:0.001693 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008215, test:0.000504 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009697, test:0.000508 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008354, test:0.001992 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008840, test:0.000801 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007252, test:0.000933 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009027, test:0.000583 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007743, test:0.001952 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008927, test:0.001442 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007993, test:0.000857 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007810, test:0.001370 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007219, test:0.000396 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007285, test:0.000422 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006637, test:0.000576 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008415, test:0.001781 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006825, test:0.001626 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008925, test:0.003467 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008158, test:0.000640 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010425, test:0.002316 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007900, test:0.003909 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007710, test:0.002900 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006085, test:0.000463 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005636, test:0.000358 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005350, test:0.000336 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005490, test:0.000377 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005840, test:0.000347 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005385, test:0.000320 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.013998, test:0.000390 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005438, test:0.000346 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005868, test:0.000319 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005107, test:0.000347 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006273, test:0.000357 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005004, test:0.000376 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005644, test:0.000657 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005407, test:0.000330 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006326, test:0.000320 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005108, test:0.000322 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005092, test:0.000458 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006623, test:0.000326 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005202, test:0.000301 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005592, test:0.000333 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004990, test:0.000376 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007411, test:0.000344 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006783, test:0.000345 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006894, test:0.000358 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005771, test:0.000442 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005796, test:0.000296 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005095, test:0.000441 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005442, test:0.000395 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005195, test:0.000399 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005251, test:0.000327 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005099, test:0.000327 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[72/100] | loss train:0.005100, test:0.000322 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005805, test:0.000296 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005302, test:0.000404 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005417, test:0.000652 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.010309, test:0.000303 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005158, test:0.000317 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006237, test:0.000349 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.009774, test:0.000348 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005386, test:0.000412 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.009420, test:0.000337 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005349, test:0.000366 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005112, test:0.000312 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004617, test:0.000319 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005210, test:0.000309 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004819, test:0.000310 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005129, test:0.000345 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005583, test:0.000330 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004891, test:0.000317 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005601, test:0.000300 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005054, test:0.000320 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004783, test:0.000290 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005063, test:0.000315 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006004, test:0.000309 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005002, test:0.000287 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005643, test:0.000320 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005122, test:0.000320 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005259, test:0.000336 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005310, test:0.000305 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005608, test:0.000310 | lr:0.000100\n",
      "Mean absolute error:  1.6595124843714215\n",
      "Root mean squared error:  5.780403221209512\n",
      "Epoch[1/100] | loss train:0.047406, test:0.002834 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011990, test:0.000325 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015214, test:0.002772 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012352, test:0.004032 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009452, test:0.008777 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010519, test:0.000352 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008124, test:0.000413 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010193, test:0.000525 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008077, test:0.002140 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008773, test:0.001758 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007965, test:0.001274 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009035, test:0.000699 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008349, test:0.000436 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008581, test:0.000409 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007380, test:0.001981 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007773, test:0.000412 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006731, test:0.001105 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007361, test:0.000487 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007342, test:0.002424 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007999, test:0.001330 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009985, test:0.000447 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008244, test:0.000448 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.016647, test:0.000504 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008028, test:0.000450 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008479, test:0.000366 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007181, test:0.000626 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008019, test:0.000403 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006761, test:0.000621 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007380, test:0.003012 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008103, test:0.000458 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007944, test:0.000479 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010673, test:0.000430 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008341, test:0.000805 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006294, test:0.002391 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007857, test:0.000522 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011846, test:0.004501 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009714, test:0.001523 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006487, test:0.000979 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007810, test:0.000698 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008731, test:0.000885 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005451, test:0.000419 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005463, test:0.000382 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005778, test:0.000425 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005233, test:0.000407 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005602, test:0.000955 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005750, test:0.000357 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005429, test:0.000417 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005528, test:0.000384 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007960, test:0.000532 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008829, test:0.000341 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005960, test:0.000479 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005126, test:0.000494 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.018365, test:0.000643 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005662, test:0.000386 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005318, test:0.000343 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007545, test:0.000401 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004943, test:0.000427 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005674, test:0.000357 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005421, test:0.000385 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005509, test:0.000396 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007067, test:0.000370 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004965, test:0.000390 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005386, test:0.000386 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005299, test:0.000351 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005702, test:0.000443 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005285, test:0.000349 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005582, test:0.000329 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005195, test:0.000350 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006519, test:0.000346 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005857, test:0.000485 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005198, test:0.000316 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005118, test:0.000369 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005320, test:0.000361 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005089, test:0.000409 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005085, test:0.000510 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005515, test:0.000353 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004843, test:0.000523 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005061, test:0.000344 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004870, test:0.000390 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006346, test:0.000336 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004615, test:0.000375 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005679, test:0.000359 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005078, test:0.000321 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004971, test:0.000365 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005523, test:0.000360 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005056, test:0.000345 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005685, test:0.000368 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005348, test:0.000356 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004859, test:0.000333 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004891, test:0.000336 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004918, test:0.000350 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005428, test:0.000358 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005047, test:0.000360 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004957, test:0.000346 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005173, test:0.000330 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004776, test:0.000325 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[97/100] | loss train:0.004810, test:0.000400 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005286, test:0.000364 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005085, test:0.000359 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004832, test:0.000349 | lr:0.000100\n",
      "Mean absolute error:  1.6829948205540046\n",
      "Root mean squared error:  5.794536818964298\n",
      "Epoch[1/100] | loss train:0.064383, test:0.006987 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015294, test:0.000876 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009498, test:0.001787 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009982, test:0.001200 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010034, test:0.001146 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009274, test:0.006695 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010659, test:0.001462 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008376, test:0.005801 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009004, test:0.001260 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010650, test:0.002318 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008392, test:0.000352 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009721, test:0.008225 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008296, test:0.008619 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015274, test:0.005288 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009997, test:0.002234 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007670, test:0.003682 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008964, test:0.000592 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007711, test:0.002144 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006792, test:0.003216 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008705, test:0.000417 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009128, test:0.000741 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007612, test:0.000454 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006593, test:0.002561 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006881, test:0.000544 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007195, test:0.000343 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006956, test:0.000412 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007734, test:0.000708 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007217, test:0.000613 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007676, test:0.000416 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007052, test:0.001159 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008039, test:0.000917 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008282, test:0.003297 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006671, test:0.000969 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006585, test:0.000665 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007268, test:0.001697 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007512, test:0.000870 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008125, test:0.000479 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008130, test:0.000562 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007809, test:0.000513 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007847, test:0.000836 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006462, test:0.000401 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005530, test:0.000367 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007874, test:0.000412 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005480, test:0.000360 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005200, test:0.000468 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005417, test:0.000531 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007438, test:0.000538 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005659, test:0.000525 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005549, test:0.000347 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005362, test:0.000339 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006349, test:0.000388 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005296, test:0.000469 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005648, test:0.000406 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005528, test:0.000567 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005439, test:0.000492 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005455, test:0.000394 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007104, test:0.000336 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005805, test:0.000334 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005741, test:0.000434 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006239, test:0.000337 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005022, test:0.000338 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006306, test:0.000344 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005078, test:0.000404 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005911, test:0.000348 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005340, test:0.000371 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005021, test:0.000433 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005789, test:0.000359 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005373, test:0.000323 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005254, test:0.000375 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006619, test:0.000442 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005245, test:0.000309 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006288, test:0.000353 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006142, test:0.000443 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005612, test:0.000526 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005675, test:0.000405 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007008, test:0.000322 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.009032, test:0.000386 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006099, test:0.000314 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005649, test:0.000300 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004989, test:0.000331 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005687, test:0.000343 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004937, test:0.000329 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005120, test:0.000317 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005003, test:0.000365 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005639, test:0.000329 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005423, test:0.000302 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005474, test:0.000309 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.010243, test:0.000336 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005581, test:0.000325 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004729, test:0.000329 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005992, test:0.000346 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004966, test:0.000339 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004976, test:0.000300 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005064, test:0.000321 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005070, test:0.000380 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005088, test:0.000318 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005297, test:0.000356 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005544, test:0.000322 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005364, test:0.000310 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004994, test:0.000351 | lr:0.000100\n",
      "Mean absolute error:  1.688459439820561\n",
      "Root mean squared error:  5.804642886656821\n",
      "Epoch[1/100] | loss train:0.060452, test:0.000489 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011193, test:0.000393 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009068, test:0.003028 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011451, test:0.005414 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010286, test:0.002046 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007975, test:0.002422 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008518, test:0.000916 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009909, test:0.000380 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008214, test:0.002387 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007705, test:0.003236 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009033, test:0.000355 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009288, test:0.001424 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010152, test:0.002026 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013465, test:0.002272 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008918, test:0.000505 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007830, test:0.000421 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007261, test:0.000543 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007708, test:0.002099 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008478, test:0.000622 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006553, test:0.000945 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/100] | loss train:0.007984, test:0.001149 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006715, test:0.001127 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009925, test:0.002780 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007007, test:0.001237 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008390, test:0.000720 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006822, test:0.002998 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007190, test:0.000475 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008217, test:0.001081 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007585, test:0.000475 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007354, test:0.001364 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011749, test:0.001144 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009242, test:0.003052 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007747, test:0.000536 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007886, test:0.000891 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006873, test:0.001677 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007711, test:0.001489 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007683, test:0.000774 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007046, test:0.001419 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007966, test:0.001081 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005870, test:0.000490 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005759, test:0.000562 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005452, test:0.000420 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005638, test:0.000366 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005400, test:0.000486 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005195, test:0.000392 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005009, test:0.000403 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004664, test:0.000680 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007127, test:0.000414 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005215, test:0.000448 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004969, test:0.000323 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005646, test:0.000539 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005467, test:0.000744 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005842, test:0.000388 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005017, test:0.000351 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005642, test:0.000405 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004812, test:0.000363 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005540, test:0.000358 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005467, test:0.000399 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004629, test:0.000494 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005966, test:0.000773 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005508, test:0.000521 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005120, test:0.000570 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005479, test:0.000524 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004746, test:0.000378 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004715, test:0.000419 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005436, test:0.000614 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005137, test:0.000339 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006064, test:0.000342 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004949, test:0.000303 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005184, test:0.000317 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005404, test:0.000800 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004680, test:0.000341 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.010801, test:0.000479 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006845, test:0.000416 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006143, test:0.000393 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004993, test:0.000348 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005184, test:0.000357 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005181, test:0.000390 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007724, test:0.000408 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.008109, test:0.000468 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004652, test:0.000340 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004474, test:0.000406 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004541, test:0.000373 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005667, test:0.000338 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004718, test:0.000310 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004656, test:0.000352 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005480, test:0.000369 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004687, test:0.000324 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005225, test:0.000334 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004540, test:0.000388 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005296, test:0.000360 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004966, test:0.000372 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005640, test:0.000335 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004904, test:0.000348 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004950, test:0.000332 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004983, test:0.000345 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005628, test:0.000346 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005096, test:0.000338 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004922, test:0.000318 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005035, test:0.000357 | lr:0.000100\n",
      "Mean absolute error:  1.7270994766260153\n",
      "Root mean squared error:  5.811685263037138\n",
      "Epoch[1/100] | loss train:0.056593, test:0.000349 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014882, test:0.003153 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013462, test:0.006786 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010895, test:0.000475 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009431, test:0.000816 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009920, test:0.000383 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008681, test:0.001580 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009142, test:0.004844 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008869, test:0.004348 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016570, test:0.006605 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011014, test:0.001271 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007841, test:0.000474 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007852, test:0.000382 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008381, test:0.002493 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008561, test:0.000359 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008547, test:0.002762 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007876, test:0.002738 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008318, test:0.001876 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007532, test:0.000388 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010979, test:0.003798 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007304, test:0.000449 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009112, test:0.001597 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008208, test:0.000471 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007493, test:0.000590 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008100, test:0.003875 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007630, test:0.000863 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007308, test:0.002444 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007380, test:0.000469 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007283, test:0.001722 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007759, test:0.000927 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006710, test:0.000528 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006895, test:0.000502 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006830, test:0.000466 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008142, test:0.002381 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007714, test:0.000520 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006439, test:0.000464 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008214, test:0.000450 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009621, test:0.000677 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007130, test:0.000451 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007708, test:0.001017 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005637, test:0.000395 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008778, test:0.000426 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005992, test:0.000458 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005602, test:0.000365 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005820, test:0.000344 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[46/100] | loss train:0.005170, test:0.000371 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005483, test:0.000411 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005004, test:0.000350 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005439, test:0.000785 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006114, test:0.000401 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005280, test:0.000328 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005319, test:0.000330 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005426, test:0.000345 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005430, test:0.000334 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005238, test:0.000340 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005628, test:0.000418 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005215, test:0.000331 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005438, test:0.000322 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005281, test:0.000380 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005419, test:0.000639 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005349, test:0.000347 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004983, test:0.000372 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005562, test:0.000323 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004992, test:0.000451 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005066, test:0.000416 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005166, test:0.000643 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005615, test:0.000282 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005246, test:0.000458 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005835, test:0.000381 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006008, test:0.000311 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006087, test:0.000413 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005960, test:0.000315 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005513, test:0.000300 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005592, test:0.000469 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006161, test:0.000372 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004912, test:0.000415 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007482, test:0.000346 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005093, test:0.000461 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005156, test:0.000376 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005619, test:0.000557 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005257, test:0.000320 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005277, test:0.000314 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004675, test:0.000327 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005406, test:0.000322 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006554, test:0.000318 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005812, test:0.000318 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006805, test:0.000329 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005323, test:0.000336 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005005, test:0.000368 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005230, test:0.000343 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005057, test:0.000342 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005083, test:0.000318 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004817, test:0.000295 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005047, test:0.000351 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005239, test:0.000337 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005013, test:0.000332 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004643, test:0.000345 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005625, test:0.000318 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005009, test:0.000316 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005421, test:0.000334 | lr:0.000100\n",
      "Mean absolute error:  1.7010085307307328\n",
      "Root mean squared error:  5.804385756489306\n",
      "Epoch[1/100] | loss train:0.053273, test:0.004143 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012976, test:0.004761 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011585, test:0.000344 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011762, test:0.001335 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009323, test:0.000969 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009543, test:0.007880 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010197, test:0.004075 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009091, test:0.003369 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007397, test:0.000436 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008450, test:0.000395 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008497, test:0.005135 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008998, test:0.000547 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007649, test:0.001845 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010085, test:0.005950 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008276, test:0.000643 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008518, test:0.000430 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008319, test:0.004992 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010054, test:0.000421 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008081, test:0.000631 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008183, test:0.000460 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007982, test:0.003452 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007502, test:0.000596 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008896, test:0.001189 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009445, test:0.001674 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009469, test:0.006236 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007837, test:0.001530 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008658, test:0.001762 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008116, test:0.006278 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007771, test:0.000398 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009208, test:0.000398 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008868, test:0.000429 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009464, test:0.007382 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007169, test:0.000875 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008030, test:0.000601 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008491, test:0.000402 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006947, test:0.001454 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008104, test:0.000642 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007750, test:0.001900 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008664, test:0.000559 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008986, test:0.001127 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006295, test:0.000493 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005976, test:0.000465 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006013, test:0.000477 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005812, test:0.000432 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005528, test:0.000428 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006854, test:0.000494 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007450, test:0.000678 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005843, test:0.000443 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006299, test:0.000487 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005117, test:0.000513 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009284, test:0.000489 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006237, test:0.000485 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008939, test:0.000470 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005783, test:0.000517 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005574, test:0.000453 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006463, test:0.000512 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005391, test:0.000481 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005711, test:0.000580 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006200, test:0.000378 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005401, test:0.000387 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005584, test:0.000420 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005895, test:0.000447 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005364, test:0.000669 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005890, test:0.000373 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006124, test:0.000358 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005314, test:0.000372 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005670, test:0.000362 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005329, test:0.000415 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006745, test:0.000516 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006135, test:0.000804 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[71/100] | loss train:0.005546, test:0.000373 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005338, test:0.000515 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006192, test:0.000365 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005441, test:0.000482 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005256, test:0.000529 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005605, test:0.000537 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005389, test:0.000366 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.013560, test:0.000386 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005947, test:0.000352 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006592, test:0.000326 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005311, test:0.000360 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005112, test:0.000317 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005155, test:0.000336 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006198, test:0.000369 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005548, test:0.000338 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.006388, test:0.000351 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005347, test:0.000339 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.006176, test:0.000336 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.007524, test:0.000345 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004881, test:0.000350 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.007517, test:0.000362 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005364, test:0.000335 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005856, test:0.000334 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005212, test:0.000324 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005623, test:0.000335 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005288, test:0.000360 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006633, test:0.000358 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004939, test:0.000357 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005661, test:0.000348 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005221, test:0.000316 | lr:0.000100\n",
      "Mean absolute error:  1.711871975268534\n",
      "Root mean squared error:  5.827578692149532\n",
      "Epoch[1/100] | loss train:0.060366, test:0.000463 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018065, test:0.019090 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015768, test:0.002486 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009493, test:0.002890 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010478, test:0.001380 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008783, test:0.000421 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008194, test:0.001041 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008663, test:0.000459 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007795, test:0.001771 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007825, test:0.001533 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007539, test:0.010183 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009521, test:0.000855 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007255, test:0.002227 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006986, test:0.001082 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013671, test:0.000706 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010343, test:0.001628 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007187, test:0.000662 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008812, test:0.000533 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009533, test:0.001516 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006994, test:0.000651 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007732, test:0.000385 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007178, test:0.000390 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007745, test:0.000770 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006762, test:0.000589 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007827, test:0.000481 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008760, test:0.002006 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007224, test:0.000696 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008657, test:0.000429 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008879, test:0.000902 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008458, test:0.001620 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007985, test:0.000428 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008718, test:0.000904 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.014178, test:0.000513 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011976, test:0.000782 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008064, test:0.002808 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006834, test:0.000477 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007512, test:0.000503 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006927, test:0.000561 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007251, test:0.000627 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.013138, test:0.001059 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005678, test:0.000415 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005979, test:0.000463 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005649, test:0.000780 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005328, test:0.000491 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005305, test:0.000621 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005335, test:0.000496 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005258, test:0.000374 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006050, test:0.000424 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005418, test:0.000398 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007520, test:0.000382 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005467, test:0.000352 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005836, test:0.000543 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005980, test:0.000501 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005384, test:0.000330 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005543, test:0.000333 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005029, test:0.000449 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005061, test:0.000472 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004798, test:0.000310 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005235, test:0.000296 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005739, test:0.000452 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006780, test:0.000342 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006000, test:0.000335 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007915, test:0.000347 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005002, test:0.000432 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004915, test:0.000317 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005177, test:0.000515 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005185, test:0.000358 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005046, test:0.000360 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005611, test:0.000726 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007084, test:0.000404 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005132, test:0.000567 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005801, test:0.000348 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005106, test:0.000372 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004861, test:0.000311 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005266, test:0.000404 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004879, test:0.000901 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004950, test:0.000441 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004903, test:0.000419 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004771, test:0.000381 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005172, test:0.000509 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005335, test:0.000328 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005243, test:0.000333 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.006619, test:0.000386 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004918, test:0.000350 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005872, test:0.000335 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005041, test:0.000343 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004966, test:0.000355 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004901, test:0.000330 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005066, test:0.000332 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005104, test:0.000347 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004938, test:0.000364 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005348, test:0.000339 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005233, test:0.000339 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005283, test:0.000317 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004915, test:0.000348 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[96/100] | loss train:0.004903, test:0.000345 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005282, test:0.000347 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.009093, test:0.000436 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005084, test:0.000341 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005259, test:0.000350 | lr:0.000100\n",
      "Mean absolute error:  1.7023884063248866\n",
      "Root mean squared error:  5.812328368154331\n",
      "Epoch[1/100] | loss train:0.039197, test:0.000480 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010352, test:0.000992 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009185, test:0.005605 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010122, test:0.000673 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009831, test:0.005088 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009756, test:0.001218 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009555, test:0.000841 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008331, test:0.001933 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009391, test:0.000362 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008244, test:0.001078 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009269, test:0.000530 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009349, test:0.000355 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007919, test:0.000477 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008722, test:0.002781 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007714, test:0.000610 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007260, test:0.000403 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007027, test:0.000891 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007271, test:0.000366 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007383, test:0.000429 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008045, test:0.001731 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007416, test:0.002122 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008072, test:0.000800 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008736, test:0.001706 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008845, test:0.001185 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006545, test:0.000400 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007574, test:0.000444 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008363, test:0.000400 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006976, test:0.000519 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006545, test:0.000529 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007810, test:0.000510 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006883, test:0.000434 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009449, test:0.000666 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006717, test:0.000624 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007244, test:0.000584 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007735, test:0.000492 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006544, test:0.001595 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006263, test:0.002288 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007978, test:0.000484 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007024, test:0.000869 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006183, test:0.002024 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005808, test:0.000429 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005094, test:0.000350 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005335, test:0.000320 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005167, test:0.000379 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005369, test:0.000389 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005195, test:0.000319 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005819, test:0.000310 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005143, test:0.000299 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005091, test:0.000311 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005086, test:0.000320 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008445, test:0.000328 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005024, test:0.000282 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005108, test:0.000554 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004878, test:0.000383 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004650, test:0.000351 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009290, test:0.000280 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004846, test:0.000275 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005192, test:0.000299 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005885, test:0.000323 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004950, test:0.000303 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004927, test:0.000380 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004747, test:0.000341 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005170, test:0.000491 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004948, test:0.000408 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005432, test:0.000309 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005253, test:0.000320 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005006, test:0.000368 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005019, test:0.000307 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004903, test:0.000300 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005541, test:0.000398 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004716, test:0.000272 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005221, test:0.000433 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005571, test:0.000356 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005521, test:0.000294 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007197, test:0.000420 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004943, test:0.000299 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005775, test:0.000306 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006311, test:0.000325 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006105, test:0.000357 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.008068, test:0.000478 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004721, test:0.000289 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004938, test:0.000317 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005790, test:0.000288 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005203, test:0.000315 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005193, test:0.000268 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005319, test:0.000305 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005228, test:0.000277 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004977, test:0.000291 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005301, test:0.000281 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005817, test:0.000305 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004875, test:0.000289 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.009983, test:0.000283 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005246, test:0.000299 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005173, test:0.000284 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005013, test:0.000284 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.006693, test:0.000332 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004855, test:0.000302 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004987, test:0.000268 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004857, test:0.000281 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004704, test:0.000282 | lr:0.000100\n",
      "Mean absolute error:  1.6331162101755892\n",
      "Root mean squared error:  5.755052565711069\n",
      "Epoch[1/100] | loss train:0.046173, test:0.001126 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013604, test:0.001405 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010169, test:0.001704 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011876, test:0.001212 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008682, test:0.001664 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008898, test:0.001298 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010064, test:0.001602 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009971, test:0.001657 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008691, test:0.000545 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007550, test:0.000650 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008015, test:0.001227 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008595, test:0.001966 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008772, test:0.000350 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011334, test:0.000479 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007759, test:0.000382 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006880, test:0.000319 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007230, test:0.000342 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009224, test:0.002768 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007854, test:0.000413 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[20/100] | loss train:0.008850, test:0.000370 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008137, test:0.000613 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006843, test:0.000874 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007412, test:0.000528 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007361, test:0.001303 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008018, test:0.003085 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008610, test:0.000554 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007828, test:0.000343 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007573, test:0.001964 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007630, test:0.000710 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007602, test:0.000396 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007307, test:0.001152 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008668, test:0.000566 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006606, test:0.001330 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007363, test:0.001507 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008267, test:0.001234 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008231, test:0.000482 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007544, test:0.000356 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007538, test:0.000502 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007126, test:0.000618 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007241, test:0.001765 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005986, test:0.000687 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006580, test:0.000370 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005317, test:0.000410 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005908, test:0.000461 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005885, test:0.000639 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005893, test:0.000371 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005240, test:0.000376 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005465, test:0.000359 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006132, test:0.000457 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005785, test:0.000399 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005380, test:0.000401 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005533, test:0.000338 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005907, test:0.000342 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005217, test:0.000337 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005642, test:0.000363 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005693, test:0.000303 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005443, test:0.000307 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005209, test:0.000304 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004780, test:0.000335 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005774, test:0.000350 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004936, test:0.000311 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005264, test:0.000605 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005152, test:0.000324 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005382, test:0.000304 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004936, test:0.000350 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005924, test:0.000311 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005559, test:0.000327 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005532, test:0.000449 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005061, test:0.000335 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004798, test:0.000461 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007279, test:0.000286 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005813, test:0.000528 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005401, test:0.000300 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004920, test:0.000497 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005173, test:0.000326 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005482, test:0.000313 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005447, test:0.000313 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006583, test:0.000339 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005170, test:0.000329 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004954, test:0.000405 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.009099, test:0.000331 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004972, test:0.000320 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004534, test:0.000297 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004652, test:0.000365 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005635, test:0.000341 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004777, test:0.000280 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005889, test:0.000307 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005197, test:0.000311 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005107, test:0.000309 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005068, test:0.000307 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004961, test:0.000327 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004981, test:0.000316 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004807, test:0.000323 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005710, test:0.000299 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005266, test:0.000294 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005277, test:0.000298 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004979, test:0.000301 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005372, test:0.000310 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004764, test:0.000331 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004956, test:0.000312 | lr:0.000100\n",
      "Mean absolute error:  1.7400853001248027\n",
      "Root mean squared error:  5.828874880384028\n",
      "Epoch[1/100] | loss train:0.057345, test:0.003062 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012044, test:0.001555 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016499, test:0.001428 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.018498, test:0.005430 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010337, test:0.000920 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011976, test:0.003399 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008886, test:0.003703 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009297, test:0.002904 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009200, test:0.000459 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007395, test:0.000931 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010040, test:0.004671 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008580, test:0.000337 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010131, test:0.003197 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007473, test:0.000844 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007306, test:0.001657 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009664, test:0.001623 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008315, test:0.000405 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.017562, test:0.001308 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009534, test:0.000478 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008539, test:0.000933 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007513, test:0.000723 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007853, test:0.003096 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007832, test:0.000497 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006778, test:0.001993 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008231, test:0.000437 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007527, test:0.000488 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006816, test:0.003031 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006653, test:0.000478 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009737, test:0.001039 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011480, test:0.003075 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007977, test:0.000500 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007213, test:0.000528 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007318, test:0.000787 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006261, test:0.000400 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007671, test:0.000597 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006916, test:0.000388 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006272, test:0.000404 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007344, test:0.001046 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006866, test:0.000984 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007922, test:0.000890 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005658, test:0.000431 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.011759, test:0.000559 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005910, test:0.000375 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005029, test:0.000357 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/100] | loss train:0.005876, test:0.000377 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005020, test:0.000564 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005528, test:0.000399 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005195, test:0.000367 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005285, test:0.000366 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005397, test:0.000362 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005551, test:0.000361 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005642, test:0.000345 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005811, test:0.000442 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005702, test:0.000510 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005630, test:0.000351 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005154, test:0.000494 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005273, test:0.000345 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005112, test:0.000652 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005010, test:0.000426 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005337, test:0.000387 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004803, test:0.000618 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005445, test:0.000348 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005209, test:0.000380 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.015602, test:0.000353 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005415, test:0.000405 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005716, test:0.000414 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005175, test:0.000326 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004899, test:0.000333 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005285, test:0.000371 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005157, test:0.000354 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005110, test:0.000363 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005062, test:0.000402 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005247, test:0.000660 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.010230, test:0.000381 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005430, test:0.000342 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006083, test:0.000347 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005614, test:0.000378 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005407, test:0.000352 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006038, test:0.000374 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005026, test:0.000381 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005947, test:0.000370 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004885, test:0.000332 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005344, test:0.000314 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006834, test:0.000334 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004853, test:0.000373 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004778, test:0.000301 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004827, test:0.000312 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.006001, test:0.000321 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004888, test:0.000343 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005259, test:0.000330 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005034, test:0.000321 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005575, test:0.000320 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005366, test:0.000331 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005365, test:0.000321 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005142, test:0.000330 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005305, test:0.000340 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006093, test:0.000339 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004804, test:0.000338 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005182, test:0.000308 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005344, test:0.000339 | lr:0.000100\n",
      "Mean absolute error:  1.7369896098084672\n",
      "Root mean squared error:  5.821990711470737\n",
      "Epoch[1/100] | loss train:0.076795, test:0.000553 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013515, test:0.000645 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012802, test:0.002412 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015243, test:0.009361 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011588, test:0.000831 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011190, test:0.000615 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009842, test:0.002652 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010564, test:0.000487 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009879, test:0.001057 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008195, test:0.000416 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011461, test:0.000473 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008823, test:0.001452 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006975, test:0.000391 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007794, test:0.001144 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009487, test:0.005686 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009223, test:0.000889 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007777, test:0.000600 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007562, test:0.000376 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006912, test:0.000387 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006402, test:0.001416 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007599, test:0.001186 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006445, test:0.001028 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008787, test:0.000997 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007048, test:0.002308 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007595, test:0.000651 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007276, test:0.001261 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008559, test:0.002552 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008276, test:0.001685 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010010, test:0.002623 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008311, test:0.000959 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006469, test:0.000858 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010185, test:0.004108 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011817, test:0.000556 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006815, test:0.000478 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006196, test:0.000626 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007083, test:0.000556 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010005, test:0.003373 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006882, test:0.001822 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007148, test:0.000394 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006792, test:0.002521 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006902, test:0.000388 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005922, test:0.000511 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006701, test:0.000570 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004854, test:0.000392 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005642, test:0.000352 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005312, test:0.000426 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005824, test:0.000361 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005550, test:0.000377 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005498, test:0.000514 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005475, test:0.000498 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005165, test:0.000406 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005503, test:0.000364 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006335, test:0.000351 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005087, test:0.000388 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005011, test:0.000392 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005172, test:0.000338 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005200, test:0.000328 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005821, test:0.000350 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007061, test:0.000356 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007012, test:0.000654 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005451, test:0.000344 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005293, test:0.000414 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005241, test:0.000396 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005291, test:0.000475 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004990, test:0.000333 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004968, test:0.000404 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007405, test:0.000709 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005652, test:0.000364 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005307, test:0.000353 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[70/100] | loss train:0.006645, test:0.000346 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005232, test:0.000343 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004897, test:0.000324 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005270, test:0.000345 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005186, test:0.000335 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005111, test:0.000378 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005862, test:0.000330 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005292, test:0.000340 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005803, test:0.000352 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004991, test:0.000324 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005549, test:0.000470 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005189, test:0.000335 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005736, test:0.000329 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005262, test:0.000314 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.007475, test:0.000341 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005269, test:0.000328 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004989, test:0.000337 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005188, test:0.000347 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005230, test:0.000321 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004901, test:0.000352 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005972, test:0.000334 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005621, test:0.000321 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005642, test:0.000323 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004711, test:0.000329 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004994, test:0.000333 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005284, test:0.000406 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005069, test:0.000331 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005287, test:0.000322 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005007, test:0.000347 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006258, test:0.000356 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.006209, test:0.000353 | lr:0.000100\n",
      "Mean absolute error:  1.791009332662597\n",
      "Root mean squared error:  5.870309795777031\n",
      "Epoch[1/100] | loss train:0.044754, test:0.003378 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012002, test:0.001175 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009655, test:0.000309 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010234, test:0.000346 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008073, test:0.000396 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009118, test:0.001100 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010037, test:0.003394 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010705, test:0.001917 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007594, test:0.000858 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008590, test:0.004581 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008460, test:0.002929 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007657, test:0.000351 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007054, test:0.002531 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008475, test:0.000938 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007709, test:0.000364 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008875, test:0.001404 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007586, test:0.000778 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008587, test:0.001365 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009519, test:0.002314 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009348, test:0.004694 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008307, test:0.002642 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009100, test:0.003006 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007488, test:0.000772 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007361, test:0.002137 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012810, test:0.002835 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009625, test:0.001400 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006995, test:0.000550 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007575, test:0.000394 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007252, test:0.000408 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006757, test:0.000461 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007801, test:0.000409 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007654, test:0.000551 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007813, test:0.000565 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006819, test:0.001762 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006938, test:0.000375 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007856, test:0.000362 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006297, test:0.002641 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009152, test:0.003536 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006819, test:0.002515 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006959, test:0.001209 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005791, test:0.000478 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005749, test:0.000405 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006295, test:0.000515 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005175, test:0.000566 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005309, test:0.000628 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005331, test:0.000386 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005278, test:0.000354 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010416, test:0.000521 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.012953, test:0.000333 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006006, test:0.000287 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006009, test:0.000506 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006897, test:0.000318 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.010241, test:0.000363 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005306, test:0.000359 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006424, test:0.000425 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006938, test:0.000699 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005080, test:0.000329 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005248, test:0.000318 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005225, test:0.000355 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005288, test:0.000405 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005528, test:0.000456 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005565, test:0.000479 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005092, test:0.000421 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005036, test:0.000370 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006313, test:0.000417 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005472, test:0.000302 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005767, test:0.000327 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006557, test:0.000539 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005548, test:0.000665 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004916, test:0.000360 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005829, test:0.000493 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005625, test:0.000390 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005156, test:0.000347 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005143, test:0.000422 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005792, test:0.000841 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005088, test:0.000323 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005832, test:0.000423 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005217, test:0.000369 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005205, test:0.000335 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004854, test:0.000532 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006628, test:0.000328 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005443, test:0.000343 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.012386, test:0.000364 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004885, test:0.000345 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005225, test:0.000347 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004891, test:0.000351 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006204, test:0.000310 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005201, test:0.000344 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005142, test:0.000322 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005564, test:0.000322 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005327, test:0.000348 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006279, test:0.000306 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005371, test:0.000358 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004851, test:0.000317 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[95/100] | loss train:0.005909, test:0.000301 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.006349, test:0.000335 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005832, test:0.000330 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.008351, test:0.000319 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005724, test:0.000378 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005472, test:0.000314 | lr:0.000100\n",
      "Mean absolute error:  1.6836506250688457\n",
      "Root mean squared error:  5.795724694719607\n",
      "Epoch[1/100] | loss train:0.060743, test:0.002496 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013037, test:0.000459 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013021, test:0.001318 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009430, test:0.000388 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011504, test:0.000372 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014089, test:0.000759 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010782, test:0.000437 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010613, test:0.000341 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009899, test:0.002105 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008585, test:0.000678 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009820, test:0.000374 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008793, test:0.000420 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014822, test:0.003349 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010366, test:0.002425 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008209, test:0.001164 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008388, test:0.001331 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010600, test:0.001487 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008146, test:0.001276 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008852, test:0.000509 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008836, test:0.000458 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008371, test:0.001045 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008330, test:0.000776 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009328, test:0.000807 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009501, test:0.001861 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009776, test:0.001170 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008261, test:0.000639 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008042, test:0.000444 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007539, test:0.001167 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009461, test:0.000531 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008596, test:0.000714 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007762, test:0.001350 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010425, test:0.000499 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010924, test:0.000871 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009072, test:0.002089 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008814, test:0.001096 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008071, test:0.000463 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008847, test:0.000670 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008044, test:0.000855 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007481, test:0.000566 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008630, test:0.000449 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006573, test:0.000395 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005925, test:0.000462 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008823, test:0.000490 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006504, test:0.000449 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006417, test:0.000422 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007890, test:0.000362 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005662, test:0.000381 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006026, test:0.000338 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005537, test:0.000368 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005853, test:0.000339 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005993, test:0.000399 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005966, test:0.000507 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005647, test:0.000818 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006511, test:0.000431 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006059, test:0.000492 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005570, test:0.000402 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006397, test:0.000383 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005764, test:0.000438 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006342, test:0.000761 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008444, test:0.000356 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006745, test:0.000441 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009729, test:0.000352 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007552, test:0.000352 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.009238, test:0.000321 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006743, test:0.000317 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.009540, test:0.000355 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006097, test:0.000493 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006036, test:0.000398 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006814, test:0.000458 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005438, test:0.000346 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006024, test:0.000380 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006836, test:0.000346 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005951, test:0.000358 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005745, test:0.000678 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.008020, test:0.000428 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.013492, test:0.000448 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006852, test:0.000355 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005689, test:0.000350 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007125, test:0.000335 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005596, test:0.000473 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.007111, test:0.000317 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005742, test:0.000410 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005833, test:0.000393 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005215, test:0.000355 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006187, test:0.000345 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.006582, test:0.000418 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005967, test:0.000330 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005800, test:0.000373 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005868, test:0.000345 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005756, test:0.000329 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005920, test:0.000352 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006333, test:0.000353 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005638, test:0.000356 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006112, test:0.000331 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.010121, test:0.000355 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.011461, test:0.000347 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005631, test:0.000339 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006082, test:0.000336 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005228, test:0.000326 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005386, test:0.000387 | lr:0.000100\n",
      "Mean absolute error:  1.853294060934632\n",
      "Root mean squared error:  5.887358174121602\n",
      "Epoch[1/100] | loss train:0.051987, test:0.000761 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015523, test:0.001059 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012743, test:0.000885 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011954, test:0.000324 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015127, test:0.004318 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009786, test:0.001003 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008357, test:0.000422 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008143, test:0.002105 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010869, test:0.003898 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010192, test:0.001818 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008255, test:0.000805 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007908, test:0.003539 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011048, test:0.015489 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009542, test:0.000394 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009939, test:0.000517 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009474, test:0.001934 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007287, test:0.000434 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007948, test:0.000386 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/100] | loss train:0.007720, test:0.000421 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007741, test:0.003795 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007907, test:0.000674 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007848, test:0.000992 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007945, test:0.000764 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006958, test:0.001235 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008956, test:0.001486 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.016101, test:0.002477 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008926, test:0.001254 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008833, test:0.000844 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007182, test:0.001987 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009735, test:0.002237 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007135, test:0.000700 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008115, test:0.001670 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008222, test:0.000464 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008237, test:0.002210 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007257, test:0.000632 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007559, test:0.000616 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007018, test:0.005892 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007398, test:0.000573 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007343, test:0.000527 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008299, test:0.001071 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006317, test:0.000400 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005531, test:0.000393 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006920, test:0.000445 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006697, test:0.000425 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005661, test:0.000397 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005563, test:0.000452 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005612, test:0.000410 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005488, test:0.000367 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005313, test:0.000388 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005800, test:0.000345 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005106, test:0.000445 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005455, test:0.000372 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005796, test:0.000320 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006160, test:0.000609 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005859, test:0.000365 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006159, test:0.000647 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008855, test:0.000362 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005318, test:0.000363 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005725, test:0.000317 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005707, test:0.000337 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005977, test:0.000338 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005849, test:0.000334 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.009202, test:0.000335 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007279, test:0.000328 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005211, test:0.000643 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005559, test:0.000377 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005515, test:0.000334 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005199, test:0.000612 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005343, test:0.000324 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006290, test:0.000324 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005310, test:0.000600 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005144, test:0.000537 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004891, test:0.000371 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005915, test:0.000360 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005810, test:0.000479 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005118, test:0.000329 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005850, test:0.000482 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005387, test:0.000322 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005159, test:0.000373 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.017560, test:0.000354 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.008099, test:0.000330 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005456, test:0.000358 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005690, test:0.000331 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005856, test:0.000318 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005253, test:0.000322 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005715, test:0.000337 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005252, test:0.000340 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.006555, test:0.000321 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005066, test:0.000330 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005385, test:0.000332 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.007306, test:0.000316 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006144, test:0.000313 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005068, test:0.000317 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005316, test:0.000362 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005399, test:0.000300 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005017, test:0.000375 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005618, test:0.000319 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005822, test:0.000336 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005021, test:0.000301 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.009798, test:0.000374 | lr:0.000100\n",
      "Mean absolute error:  1.7375513141368866\n",
      "Root mean squared error:  5.835695140944243\n",
      "Epoch[1/100] | loss train:0.036832, test:0.002949 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011827, test:0.000347 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011531, test:0.000351 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010789, test:0.000454 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008601, test:0.003847 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009734, test:0.002203 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010121, test:0.008331 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010914, test:0.003082 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009351, test:0.011468 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009599, test:0.002746 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007964, test:0.002661 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009162, test:0.000749 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009860, test:0.002534 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010071, test:0.000886 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007828, test:0.001300 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008765, test:0.000821 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009050, test:0.001155 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010064, test:0.000668 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006987, test:0.000357 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007599, test:0.000547 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008885, test:0.000572 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007766, test:0.001107 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008582, test:0.002949 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008302, test:0.002366 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007354, test:0.001187 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007797, test:0.000699 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007480, test:0.001267 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007473, test:0.000477 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007063, test:0.000800 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007692, test:0.000944 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006766, test:0.000993 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007817, test:0.000861 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008176, test:0.000871 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007021, test:0.003282 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006740, test:0.001686 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007739, test:0.002382 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.014707, test:0.000367 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012651, test:0.003208 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007631, test:0.000652 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008507, test:0.002248 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006114, test:0.000464 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006625, test:0.000418 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005143, test:0.000433 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[44/100] | loss train:0.005647, test:0.000720 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005436, test:0.000547 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005729, test:0.000415 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004915, test:0.000624 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005046, test:0.000402 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005085, test:0.000515 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005309, test:0.000785 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005445, test:0.000515 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005561, test:0.000393 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005142, test:0.000468 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006164, test:0.000361 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005544, test:0.000424 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005103, test:0.000361 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004879, test:0.000418 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005278, test:0.000625 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004946, test:0.000385 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005385, test:0.000375 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005361, test:0.000427 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005925, test:0.000359 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005160, test:0.000405 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004701, test:0.000410 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005100, test:0.000421 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005364, test:0.000443 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005157, test:0.000448 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005789, test:0.000573 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005207, test:0.000505 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005402, test:0.000346 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005587, test:0.000392 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005170, test:0.000493 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005072, test:0.000521 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005157, test:0.000408 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004796, test:0.000362 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005785, test:0.000369 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004973, test:0.000369 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005116, test:0.000426 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005432, test:0.000412 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004861, test:0.000665 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005331, test:0.000348 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004711, test:0.000372 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004843, test:0.000438 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004712, test:0.000401 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004575, test:0.000381 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004583, test:0.000348 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004502, test:0.000419 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005079, test:0.000385 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005160, test:0.000371 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005357, test:0.000347 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005084, test:0.000357 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005388, test:0.000355 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.006572, test:0.000370 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004942, test:0.000388 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005472, test:0.000380 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004633, test:0.000358 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004931, test:0.000371 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004959, test:0.000336 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005400, test:0.000363 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005016, test:0.000403 | lr:0.000100\n",
      "Mean absolute error:  1.7100240999929923\n",
      "Root mean squared error:  5.820749380646352\n",
      "Epoch[1/100] | loss train:0.057439, test:0.000510 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008946, test:0.000534 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011642, test:0.004021 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011786, test:0.001570 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009116, test:0.001806 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008489, test:0.002094 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009246, test:0.001343 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008479, test:0.000475 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008981, test:0.004228 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008482, test:0.001047 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007495, test:0.003173 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007901, test:0.000670 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008015, test:0.000514 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008781, test:0.004598 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008635, test:0.000453 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007729, test:0.000594 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007591, test:0.000547 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007501, test:0.000347 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006361, test:0.000498 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011605, test:0.004509 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009509, test:0.000393 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006564, test:0.001598 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007248, test:0.001123 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007725, test:0.002958 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007174, test:0.000438 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013197, test:0.002639 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008193, test:0.003234 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009561, test:0.000396 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007138, test:0.000906 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006986, test:0.002328 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007750, test:0.000444 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006849, test:0.000714 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007538, test:0.001373 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007714, test:0.000884 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007128, test:0.000548 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007050, test:0.002007 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007586, test:0.009205 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009743, test:0.000619 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007198, test:0.003033 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006540, test:0.000622 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006022, test:0.000417 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005102, test:0.000650 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005762, test:0.000376 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005606, test:0.000339 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005302, test:0.000416 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005224, test:0.000417 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004763, test:0.000330 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004980, test:0.000329 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005132, test:0.000763 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005477, test:0.000318 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005803, test:0.000335 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005161, test:0.000436 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004825, test:0.000345 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004787, test:0.000365 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005037, test:0.000595 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005951, test:0.000406 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005162, test:0.000326 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004814, test:0.000308 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005759, test:0.000355 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006125, test:0.000487 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006143, test:0.000361 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006836, test:0.000365 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005815, test:0.000362 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004848, test:0.000327 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005317, test:0.000605 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005778, test:0.000325 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005752, test:0.000435 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004863, test:0.000306 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[69/100] | loss train:0.005353, test:0.000332 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005170, test:0.000350 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008848, test:0.000313 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005564, test:0.000325 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.013450, test:0.000345 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.009140, test:0.000411 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005407, test:0.000365 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005454, test:0.000498 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005790, test:0.000336 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005741, test:0.000367 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005531, test:0.000504 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005425, test:0.000523 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.008075, test:0.000339 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006304, test:0.000323 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005987, test:0.000356 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004524, test:0.000359 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004786, test:0.000353 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005045, test:0.000305 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005015, test:0.000369 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005234, test:0.000341 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005247, test:0.000344 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004711, test:0.000329 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005163, test:0.000309 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005971, test:0.000324 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005706, test:0.000402 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004846, test:0.000362 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005176, test:0.000323 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005198, test:0.000325 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006722, test:0.000334 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006181, test:0.000329 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004511, test:0.000316 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005661, test:0.000362 | lr:0.000100\n",
      "Mean absolute error:  1.7008094322862097\n",
      "Root mean squared error:  5.808034223496326\n",
      "Epoch[1/100] | loss train:0.054652, test:0.001705 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018167, test:0.006341 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014220, test:0.003185 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012934, test:0.001844 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008389, test:0.001655 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008643, test:0.001332 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007089, test:0.000302 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009057, test:0.003947 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009660, test:0.006026 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009608, test:0.000515 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008647, test:0.003317 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013436, test:0.003243 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011436, test:0.000485 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007209, test:0.000492 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007180, test:0.001086 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007608, test:0.000366 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008244, test:0.000484 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009084, test:0.003141 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007188, test:0.000387 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008861, test:0.001962 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007787, test:0.000576 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007146, test:0.000711 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006864, test:0.000841 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007906, test:0.001078 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007454, test:0.000732 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007262, test:0.010407 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010545, test:0.001184 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007761, test:0.004295 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008809, test:0.001307 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006572, test:0.001264 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009444, test:0.002687 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006953, test:0.003802 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007370, test:0.000697 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008119, test:0.003833 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008406, test:0.004711 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006296, test:0.001698 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006769, test:0.000549 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007701, test:0.002913 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009091, test:0.000424 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007122, test:0.003737 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006417, test:0.000476 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005901, test:0.000346 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005399, test:0.000358 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005576, test:0.000427 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006670, test:0.000472 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005963, test:0.000384 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005667, test:0.000346 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004495, test:0.000365 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006160, test:0.001141 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005559, test:0.000722 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005017, test:0.000342 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005412, test:0.000381 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006669, test:0.000444 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005447, test:0.000309 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005189, test:0.000331 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005587, test:0.000580 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005368, test:0.000368 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005265, test:0.000361 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004991, test:0.000325 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007315, test:0.000321 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004755, test:0.000551 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005793, test:0.000360 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.009094, test:0.000318 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005330, test:0.000327 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005282, test:0.000290 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005248, test:0.000318 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005378, test:0.000278 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005868, test:0.000346 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005247, test:0.000302 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007182, test:0.000301 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005444, test:0.000372 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005658, test:0.000334 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005429, test:0.000354 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004992, test:0.000303 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005788, test:0.000335 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005073, test:0.000285 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007696, test:0.000333 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006551, test:0.000287 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006695, test:0.000314 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004993, test:0.000337 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004961, test:0.000287 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004795, test:0.000296 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005514, test:0.000303 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005978, test:0.000329 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004643, test:0.000278 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005165, test:0.000308 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006260, test:0.000290 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004823, test:0.000277 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005502, test:0.000296 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005265, test:0.000339 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004872, test:0.000318 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005003, test:0.000308 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005290, test:0.000311 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[94/100] | loss train:0.005228, test:0.000308 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005174, test:0.000307 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005224, test:0.000293 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004835, test:0.000295 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006352, test:0.000304 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006201, test:0.000324 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005258, test:0.000327 | lr:0.000100\n",
      "Mean absolute error:  1.7152508189848195\n",
      "Root mean squared error:  5.816513463652906\n",
      "Epoch[1/100] | loss train:0.056216, test:0.002037 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011544, test:0.000693 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009435, test:0.004505 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011346, test:0.006865 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009478, test:0.016095 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010034, test:0.000550 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008277, test:0.000410 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007655, test:0.000365 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011358, test:0.002493 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009182, test:0.000326 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007998, test:0.000336 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008598, test:0.000418 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010694, test:0.001798 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009685, test:0.004497 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008058, test:0.000447 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008208, test:0.000317 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007547, test:0.000795 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007814, test:0.000385 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008643, test:0.002629 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008080, test:0.002156 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008513, test:0.000417 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007607, test:0.000592 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008220, test:0.001058 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008065, test:0.000428 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008410, test:0.000381 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006151, test:0.000447 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008659, test:0.000646 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007238, test:0.001760 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007979, test:0.001071 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008642, test:0.001928 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007428, test:0.000595 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008170, test:0.000528 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007737, test:0.000896 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.017447, test:0.004416 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008991, test:0.001322 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006728, test:0.000528 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007234, test:0.001115 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009000, test:0.000553 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008338, test:0.001761 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009972, test:0.000592 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007083, test:0.000457 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006169, test:0.000521 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005451, test:0.000411 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005676, test:0.000448 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005459, test:0.000474 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005675, test:0.000475 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005707, test:0.000429 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005688, test:0.000405 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005577, test:0.000667 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008763, test:0.000472 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006035, test:0.000397 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005351, test:0.000458 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006241, test:0.001049 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008060, test:0.000392 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005426, test:0.000398 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005314, test:0.000508 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005392, test:0.000451 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006934, test:0.000388 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005859, test:0.000367 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005875, test:0.000487 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005261, test:0.000382 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005304, test:0.000357 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006284, test:0.000479 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005737, test:0.000340 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005537, test:0.000608 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005550, test:0.000374 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.010791, test:0.000353 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005172, test:0.000425 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005353, test:0.000371 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006059, test:0.000390 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007119, test:0.000379 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005925, test:0.001165 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005653, test:0.000375 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006007, test:0.000567 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005551, test:0.000402 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006527, test:0.000339 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005636, test:0.000371 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006342, test:0.000376 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005999, test:0.000363 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005038, test:0.000332 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005632, test:0.000315 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006156, test:0.000365 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005244, test:0.000347 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005471, test:0.000366 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.008700, test:0.000383 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004858, test:0.000386 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005695, test:0.000330 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005329, test:0.000329 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005767, test:0.000356 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005354, test:0.000364 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005894, test:0.000352 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006148, test:0.000349 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005107, test:0.000309 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005873, test:0.000349 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005608, test:0.000347 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005455, test:0.000339 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006559, test:0.000340 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005137, test:0.000328 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005262, test:0.000378 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005737, test:0.000337 | lr:0.000100\n",
      "Mean absolute error:  1.6630606889706125\n",
      "Root mean squared error:  5.794895213267174\n",
      "Epoch[1/100] | loss train:0.055022, test:0.003524 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015098, test:0.000337 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011235, test:0.001712 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009264, test:0.000698 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008834, test:0.004496 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010974, test:0.001209 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009836, test:0.001452 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008629, test:0.000876 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008441, test:0.000314 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008156, test:0.004841 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007066, test:0.004417 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008976, test:0.000394 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008536, test:0.001537 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007874, test:0.002194 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007948, test:0.000506 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006882, test:0.002277 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006934, test:0.000471 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[18/100] | loss train:0.008367, test:0.001856 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007296, test:0.000329 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006881, test:0.002996 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007374, test:0.001311 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010401, test:0.006449 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006920, test:0.000357 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006990, test:0.002446 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008963, test:0.001558 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007217, test:0.000673 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007488, test:0.000809 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007935, test:0.001452 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007690, test:0.000509 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007380, test:0.000530 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007335, test:0.000782 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006807, test:0.000907 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008698, test:0.000338 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007516, test:0.002490 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009800, test:0.000670 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006378, test:0.001025 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006955, test:0.000336 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011152, test:0.000870 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009428, test:0.000642 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007203, test:0.000443 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005967, test:0.000622 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005409, test:0.000324 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005560, test:0.000380 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005120, test:0.000379 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005204, test:0.000360 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005606, test:0.000362 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005344, test:0.000319 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005351, test:0.000432 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005733, test:0.000345 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005232, test:0.000323 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006267, test:0.000319 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005545, test:0.000303 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004989, test:0.000278 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005539, test:0.000300 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005444, test:0.000375 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005253, test:0.000355 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005990, test:0.000445 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005140, test:0.000317 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005102, test:0.000377 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005706, test:0.000440 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006717, test:0.000403 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006102, test:0.000353 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005165, test:0.000290 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005192, test:0.000371 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005290, test:0.000395 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004923, test:0.000306 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005642, test:0.000351 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005324, test:0.000778 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006119, test:0.000373 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.011673, test:0.000484 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004937, test:0.000285 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005079, test:0.000549 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005794, test:0.000376 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004875, test:0.000444 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005012, test:0.000294 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005906, test:0.000350 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005773, test:0.000271 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004650, test:0.000349 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006388, test:0.000322 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005637, test:0.000283 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004995, test:0.000301 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005126, test:0.000284 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004408, test:0.000299 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005597, test:0.000312 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005631, test:0.000307 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004889, test:0.000309 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005269, test:0.000279 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005020, test:0.000307 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.007470, test:0.000300 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004727, test:0.000304 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005244, test:0.000281 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.011415, test:0.000286 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005046, test:0.000294 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005148, test:0.000314 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004406, test:0.000313 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005058, test:0.000309 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004831, test:0.000282 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005016, test:0.000292 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006692, test:0.000284 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.006803, test:0.000282 | lr:0.000100\n",
      "Mean absolute error:  1.6722576611314506\n",
      "Root mean squared error:  5.779255710278109\n",
      "Epoch[1/100] | loss train:0.061479, test:0.008406 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013168, test:0.000407 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009638, test:0.000304 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013733, test:0.001916 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009530, test:0.003295 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009851, test:0.002161 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009505, test:0.006873 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011741, test:0.001283 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008858, test:0.000304 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008432, test:0.001812 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009820, test:0.007252 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009164, test:0.001109 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009283, test:0.005526 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008803, test:0.000929 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009123, test:0.000689 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008424, test:0.001309 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007741, test:0.000495 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008070, test:0.000735 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008601, test:0.000730 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008440, test:0.001458 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007959, test:0.000695 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010877, test:0.001605 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010086, test:0.000424 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008892, test:0.000503 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008179, test:0.001015 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008918, test:0.001595 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008377, test:0.001021 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008482, test:0.000920 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008056, test:0.000567 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009645, test:0.001937 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011157, test:0.002004 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009216, test:0.000525 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007046, test:0.003884 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008218, test:0.000432 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006733, test:0.002701 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009416, test:0.000636 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008837, test:0.003368 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008350, test:0.001039 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008628, test:0.000712 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.013653, test:0.004899 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007117, test:0.000566 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.011055, test:0.000443 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/100] | loss train:0.007240, test:0.000437 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005930, test:0.000423 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005737, test:0.000447 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006735, test:0.000398 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005480, test:0.000399 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006508, test:0.000395 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006780, test:0.000420 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006359, test:0.000640 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006299, test:0.000400 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005917, test:0.000360 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005836, test:0.000506 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005349, test:0.000636 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006400, test:0.000403 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005978, test:0.000566 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007174, test:0.000420 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006472, test:0.000393 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007182, test:0.000467 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007405, test:0.000576 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005676, test:0.000376 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005054, test:0.000467 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005430, test:0.000396 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005649, test:0.000356 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006178, test:0.000411 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005836, test:0.000412 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006616, test:0.000553 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005391, test:0.000397 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006204, test:0.000585 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006228, test:0.000549 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005965, test:0.000446 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006374, test:0.000373 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005616, test:0.000382 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005932, test:0.000365 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005709, test:0.000340 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005756, test:0.000401 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006755, test:0.000403 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006058, test:0.000354 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005601, test:0.000667 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005652, test:0.000386 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005870, test:0.000344 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.008536, test:0.000377 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.008276, test:0.000380 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005610, test:0.000381 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005305, test:0.000403 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005460, test:0.000393 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005312, test:0.000395 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005861, test:0.000409 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.014385, test:0.000363 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006029, test:0.000348 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005506, test:0.000350 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.008452, test:0.000381 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.006259, test:0.000355 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005109, test:0.000365 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005546, test:0.000380 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007294, test:0.000336 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.008296, test:0.000342 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005342, test:0.000341 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006075, test:0.000350 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005687, test:0.000378 | lr:0.000100\n",
      "Mean absolute error:  1.7658950504767645\n",
      "Root mean squared error:  5.855035209853733\n",
      "Epoch[1/100] | loss train:0.045181, test:0.001610 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014147, test:0.000638 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.022792, test:0.006622 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012895, test:0.000298 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009496, test:0.000281 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008775, test:0.000312 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009130, test:0.000822 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009660, test:0.000326 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009070, test:0.000557 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014256, test:0.006594 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010552, test:0.000397 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008564, test:0.000374 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008963, test:0.004954 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007183, test:0.000353 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014755, test:0.000345 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009914, test:0.000937 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007948, test:0.000680 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007253, test:0.000411 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008588, test:0.000566 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008104, test:0.001352 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007948, test:0.000773 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006983, test:0.000368 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007516, test:0.000569 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009682, test:0.002111 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008025, test:0.002228 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008459, test:0.000761 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009089, test:0.001906 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007718, test:0.000482 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007757, test:0.001488 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007104, test:0.000423 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008191, test:0.000535 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006607, test:0.000688 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007278, test:0.000410 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007456, test:0.001119 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010340, test:0.000503 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007160, test:0.000430 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006689, test:0.000694 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006800, test:0.000534 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006462, test:0.001062 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007773, test:0.001990 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006447, test:0.000580 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006008, test:0.000472 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005751, test:0.000601 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005944, test:0.000519 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006299, test:0.000366 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006560, test:0.000391 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006315, test:0.000453 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008005, test:0.000450 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005592, test:0.000382 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005908, test:0.000396 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005269, test:0.000625 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005830, test:0.000385 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005331, test:0.000398 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005324, test:0.000422 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005257, test:0.000370 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009209, test:0.000361 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006252, test:0.000352 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004997, test:0.000377 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005260, test:0.000535 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005746, test:0.000374 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004890, test:0.000364 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005295, test:0.000327 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006464, test:0.000461 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005153, test:0.000337 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006122, test:0.000363 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005351, test:0.000404 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005450, test:0.000429 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[68/100] | loss train:0.004837, test:0.000305 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.010350, test:0.000566 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005315, test:0.000392 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004983, test:0.000495 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004839, test:0.000405 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005350, test:0.000375 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006040, test:0.000600 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004824, test:0.000376 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004980, test:0.000325 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005405, test:0.000400 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005321, test:0.000345 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004886, test:0.000436 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004799, test:0.000377 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005522, test:0.000341 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004701, test:0.000412 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.007232, test:0.000326 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006194, test:0.000324 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004562, test:0.000355 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004762, test:0.000323 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005593, test:0.000347 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004655, test:0.000349 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005765, test:0.000314 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004863, test:0.000307 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005414, test:0.000336 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004987, test:0.000365 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.006043, test:0.000323 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004998, test:0.000347 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005133, test:0.000313 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004928, test:0.000336 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004737, test:0.000327 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005011, test:0.000359 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005043, test:0.000371 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005025, test:0.000337 | lr:0.000100\n",
      "Mean absolute error:  1.7005877685405304\n",
      "Root mean squared error:  5.797591242792637\n",
      "Epoch[1/100] | loss train:0.061725, test:0.005817 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012057, test:0.001124 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010223, test:0.000641 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008648, test:0.001097 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008909, test:0.000286 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009197, test:0.001126 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007473, test:0.006557 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010453, test:0.000381 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007844, test:0.003078 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009959, test:0.000458 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006722, test:0.000518 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008582, test:0.000459 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008425, test:0.000724 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014172, test:0.002349 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010544, test:0.001312 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007421, test:0.000399 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007822, test:0.000868 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008768, test:0.001078 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008452, test:0.000469 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007791, test:0.002088 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007976, test:0.000590 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007381, test:0.002465 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007544, test:0.000414 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009646, test:0.002444 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007300, test:0.000460 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007135, test:0.000527 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006865, test:0.001847 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007190, test:0.000897 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007526, test:0.000960 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006176, test:0.000973 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008880, test:0.001306 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007103, test:0.000476 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007093, test:0.001014 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008310, test:0.000515 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006411, test:0.000407 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006629, test:0.000935 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006578, test:0.003101 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010179, test:0.001005 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006578, test:0.003405 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008544, test:0.000619 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007977, test:0.000379 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008702, test:0.000378 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005553, test:0.000381 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005155, test:0.000385 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005222, test:0.000451 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005425, test:0.000370 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007834, test:0.000371 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005333, test:0.000353 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005297, test:0.000422 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005279, test:0.000412 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008042, test:0.000402 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005369, test:0.000316 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005298, test:0.000299 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006990, test:0.000344 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005644, test:0.000328 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005449, test:0.000575 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006631, test:0.000304 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005395, test:0.000313 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004828, test:0.000360 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004864, test:0.000311 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005335, test:0.000315 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009328, test:0.000313 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005403, test:0.000282 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005878, test:0.000292 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005885, test:0.000398 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005307, test:0.000315 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005917, test:0.000371 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005271, test:0.000457 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005308, test:0.000482 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005280, test:0.000369 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004971, test:0.000489 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006003, test:0.000326 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004893, test:0.000326 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004983, test:0.000345 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006214, test:0.000303 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005577, test:0.000367 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006125, test:0.000293 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005064, test:0.000296 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004566, test:0.000309 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005674, test:0.000295 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005074, test:0.000283 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004687, test:0.000265 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005164, test:0.000280 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005002, test:0.000289 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005201, test:0.000278 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004989, test:0.000279 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005433, test:0.000324 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004805, test:0.000320 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004807, test:0.000339 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005002, test:0.000276 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005175, test:0.000294 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005906, test:0.000288 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[93/100] | loss train:0.005908, test:0.000288 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005586, test:0.000320 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004781, test:0.000293 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004771, test:0.000265 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005064, test:0.000276 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004468, test:0.000322 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004943, test:0.000307 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005448, test:0.000334 | lr:0.000100\n",
      "Mean absolute error:  1.7562081440369912\n",
      "Root mean squared error:  5.829538266562761\n",
      "Epoch[1/100] | loss train:0.047927, test:0.009358 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012273, test:0.001780 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008591, test:0.000330 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008497, test:0.002159 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008981, test:0.000307 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009112, test:0.000360 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009317, test:0.000366 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008295, test:0.001166 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009102, test:0.000642 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008117, test:0.002758 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008094, test:0.000533 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008776, test:0.005729 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008202, test:0.001196 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009327, test:0.001565 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007987, test:0.000719 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007791, test:0.001021 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008663, test:0.000583 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007151, test:0.000970 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007719, test:0.000540 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006966, test:0.005490 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007965, test:0.001816 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007103, test:0.000522 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013474, test:0.003128 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012620, test:0.001318 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009069, test:0.005147 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007496, test:0.001470 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006776, test:0.001925 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009296, test:0.001008 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007347, test:0.000714 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006925, test:0.000901 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009864, test:0.001532 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008341, test:0.000515 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007223, test:0.001220 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007155, test:0.000357 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007455, test:0.000630 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008624, test:0.000381 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007526, test:0.000403 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006959, test:0.002854 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007042, test:0.001435 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006431, test:0.000705 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006449, test:0.000319 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005145, test:0.000354 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008690, test:0.000358 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006828, test:0.000445 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005523, test:0.000283 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004973, test:0.000383 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005415, test:0.000342 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006777, test:0.000439 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008479, test:0.000400 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005262, test:0.000329 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005072, test:0.000300 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005637, test:0.000307 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007440, test:0.000401 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005030, test:0.000384 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005159, test:0.000432 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005339, test:0.000311 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005066, test:0.000504 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006601, test:0.000360 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005205, test:0.000317 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005408, test:0.000339 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005206, test:0.000323 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005433, test:0.000286 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004591, test:0.000293 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005021, test:0.000297 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005308, test:0.000334 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004923, test:0.000278 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005084, test:0.000307 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006573, test:0.000339 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005018, test:0.000321 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005405, test:0.000339 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008051, test:0.000461 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004954, test:0.000283 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005604, test:0.000539 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005388, test:0.000298 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005811, test:0.000308 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005140, test:0.000327 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007306, test:0.000299 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005094, test:0.000290 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005040, test:0.000270 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005098, test:0.000310 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006771, test:0.000312 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006486, test:0.000285 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005419, test:0.000275 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005627, test:0.000287 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004948, test:0.000278 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004756, test:0.000271 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005212, test:0.000270 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004704, test:0.000350 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005142, test:0.000296 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004841, test:0.000276 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005015, test:0.000320 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005191, test:0.000296 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005748, test:0.000283 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005147, test:0.000311 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005840, test:0.000297 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005840, test:0.000279 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005048, test:0.000290 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005231, test:0.000264 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004712, test:0.000334 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004793, test:0.000285 | lr:0.000100\n",
      "Mean absolute error:  1.7078421532917623\n",
      "Root mean squared error:  5.773210331131354\n",
      "Epoch[1/100] | loss train:0.068544, test:0.002594 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012406, test:0.000389 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016882, test:0.012809 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012876, test:0.000714 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007723, test:0.000310 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009609, test:0.001875 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009172, test:0.000915 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007659, test:0.002405 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008994, test:0.002021 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010429, test:0.001860 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007404, test:0.000920 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008141, test:0.001668 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011815, test:0.003693 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009855, test:0.001113 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007670, test:0.000744 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010158, test:0.000468 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/100] | loss train:0.009779, test:0.001877 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007015, test:0.002381 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007657, test:0.000672 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008072, test:0.001649 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007685, test:0.007861 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008994, test:0.000540 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007874, test:0.000486 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008005, test:0.001041 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008351, test:0.001046 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007457, test:0.000491 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007352, test:0.003501 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007337, test:0.000764 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010567, test:0.001051 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008213, test:0.000921 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007052, test:0.002549 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007496, test:0.002093 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008738, test:0.000523 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007986, test:0.003187 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007800, test:0.000482 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006478, test:0.002353 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007725, test:0.000501 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006898, test:0.001267 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006968, test:0.000608 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006959, test:0.000644 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006119, test:0.000468 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005751, test:0.000551 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005824, test:0.000472 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007546, test:0.000461 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006073, test:0.000394 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005402, test:0.000401 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005624, test:0.000395 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004872, test:0.000507 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005102, test:0.000395 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005856, test:0.000751 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006667, test:0.000432 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004762, test:0.000378 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005342, test:0.000380 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005427, test:0.000392 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.011221, test:0.000867 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005552, test:0.000525 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004908, test:0.000555 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006506, test:0.000474 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005028, test:0.000343 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005827, test:0.001776 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004998, test:0.000391 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005955, test:0.000462 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005413, test:0.000374 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005285, test:0.000381 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005368, test:0.000403 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005198, test:0.000331 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004830, test:0.000366 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005055, test:0.000335 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006817, test:0.000568 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005153, test:0.000366 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005040, test:0.000347 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005253, test:0.000448 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.010543, test:0.000368 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005879, test:0.000364 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005084, test:0.000342 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006122, test:0.000572 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005302, test:0.000361 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005455, test:0.000369 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005545, test:0.000362 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005421, test:0.000431 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005133, test:0.000331 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.008331, test:0.000335 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005070, test:0.000352 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005805, test:0.000323 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005137, test:0.000332 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004791, test:0.000350 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006020, test:0.000324 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005192, test:0.000371 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005857, test:0.000330 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005067, test:0.000342 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005360, test:0.000353 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005148, test:0.000338 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004540, test:0.000362 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004748, test:0.000334 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005945, test:0.000331 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.006620, test:0.000321 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004864, test:0.000326 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004931, test:0.000384 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004857, test:0.000354 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.006208, test:0.000347 | lr:0.000100\n",
      "Mean absolute error:  1.6531730120315158\n",
      "Root mean squared error:  5.792983895898579\n",
      "Epoch[1/100] | loss train:0.041361, test:0.001141 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010984, test:0.000434 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011475, test:0.000359 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009371, test:0.000668 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009280, test:0.000600 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011279, test:0.000814 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009035, test:0.000314 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010285, test:0.012471 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009454, test:0.001088 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009154, test:0.000826 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007825, test:0.001453 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009129, test:0.000388 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008305, test:0.001368 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009127, test:0.002106 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011987, test:0.000724 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007073, test:0.000478 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010503, test:0.006597 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008598, test:0.000574 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010970, test:0.002473 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007817, test:0.000396 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007616, test:0.002501 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007824, test:0.001250 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007465, test:0.000464 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006724, test:0.000351 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007825, test:0.002740 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008391, test:0.000507 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007327, test:0.000478 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007171, test:0.000458 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008742, test:0.000565 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008934, test:0.001458 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007897, test:0.001438 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006949, test:0.002412 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007891, test:0.000871 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007729, test:0.001658 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011611, test:0.002559 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008141, test:0.000826 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006859, test:0.000358 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006263, test:0.000455 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006771, test:0.001267 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007138, test:0.000834 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006651, test:0.000332 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[42/100] | loss train:0.005468, test:0.000367 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005297, test:0.000365 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008480, test:0.000344 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006356, test:0.000479 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005222, test:0.000336 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007985, test:0.000314 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004695, test:0.000294 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005168, test:0.000298 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005299, test:0.000323 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005860, test:0.000466 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005054, test:0.000293 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005199, test:0.000307 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005170, test:0.000342 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006172, test:0.000453 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005700, test:0.000329 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005589, test:0.000325 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005924, test:0.000295 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004842, test:0.000385 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004892, test:0.000648 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005543, test:0.000305 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004961, test:0.000291 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005393, test:0.000316 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004859, test:0.000416 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005051, test:0.000320 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005180, test:0.000319 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005763, test:0.000320 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005038, test:0.000298 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006817, test:0.000413 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005376, test:0.000358 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005016, test:0.000339 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005361, test:0.000307 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005050, test:0.000475 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006246, test:0.000496 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004943, test:0.000290 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005684, test:0.000277 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004861, test:0.000351 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004796, test:0.000314 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005490, test:0.000292 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005352, test:0.000332 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004888, test:0.000319 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004936, test:0.000317 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005177, test:0.000287 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004663, test:0.000304 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006469, test:0.000280 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004824, test:0.000305 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004783, test:0.000299 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005681, test:0.000293 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004381, test:0.000305 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004807, test:0.000281 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005003, test:0.000276 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004780, test:0.000298 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005042, test:0.000276 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005653, test:0.000314 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005721, test:0.000312 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004530, test:0.000292 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005270, test:0.000322 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004651, test:0.000317 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004698, test:0.000306 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004987, test:0.000278 | lr:0.000100\n",
      "Mean absolute error:  1.6736345357255948\n",
      "Root mean squared error:  5.776033485271679\n",
      "Epoch[1/100] | loss train:0.059621, test:0.003284 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014258, test:0.000573 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009390, test:0.004496 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014359, test:0.002795 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011238, test:0.000336 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008265, test:0.001923 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008950, test:0.000637 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008604, test:0.002812 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007704, test:0.001296 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009730, test:0.001017 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010525, test:0.001635 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008792, test:0.015680 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008946, test:0.000799 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008580, test:0.001238 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009276, test:0.000369 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008628, test:0.000721 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008399, test:0.000618 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007903, test:0.000934 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007771, test:0.001044 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009098, test:0.000519 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009068, test:0.000401 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011098, test:0.000994 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007467, test:0.000563 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007313, test:0.000389 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006738, test:0.000446 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007134, test:0.000555 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009292, test:0.000617 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007188, test:0.000938 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008108, test:0.000511 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010773, test:0.003891 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007754, test:0.001156 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006798, test:0.000726 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007415, test:0.001546 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007900, test:0.000597 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012905, test:0.003186 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012147, test:0.000624 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007593, test:0.001847 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007451, test:0.000576 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007611, test:0.000450 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008379, test:0.000999 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006173, test:0.000608 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005885, test:0.000451 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005976, test:0.000440 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009344, test:0.000396 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005951, test:0.000375 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004930, test:0.000410 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005365, test:0.000435 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005340, test:0.000372 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006063, test:0.000487 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005458, test:0.000368 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006020, test:0.000459 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005853, test:0.000476 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006784, test:0.000351 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005396, test:0.000416 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005204, test:0.000526 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005398, test:0.000415 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005364, test:0.000371 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005864, test:0.000348 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005551, test:0.000353 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008022, test:0.000553 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004938, test:0.000734 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007168, test:0.000433 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005009, test:0.000379 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005212, test:0.000349 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005125, test:0.000377 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004845, test:0.000367 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[67/100] | loss train:0.005329, test:0.000408 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006763, test:0.000445 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006395, test:0.000363 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005699, test:0.000533 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005058, test:0.000334 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005113, test:0.000397 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005762, test:0.000403 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006092, test:0.000374 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006679, test:0.000376 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005184, test:0.000315 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005759, test:0.000387 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005027, test:0.000522 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005133, test:0.000469 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005425, test:0.000352 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004946, test:0.000329 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005304, test:0.000343 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005013, test:0.000385 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004983, test:0.000401 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005579, test:0.000368 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.006822, test:0.000379 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004776, test:0.000343 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004932, test:0.000361 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004971, test:0.000337 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005526, test:0.000372 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004846, test:0.000348 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005240, test:0.000333 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005094, test:0.000329 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005218, test:0.000323 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005073, test:0.000368 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.008364, test:0.000377 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005060, test:0.000354 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005127, test:0.000351 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004595, test:0.000325 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.006415, test:0.000337 | lr:0.000100\n",
      "Mean absolute error:  1.6885973294551089\n",
      "Root mean squared error:  5.802785187070243\n",
      "Epoch[1/100] | loss train:0.045797, test:0.001494 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015022, test:0.002204 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009918, test:0.000295 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009299, test:0.000315 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010593, test:0.000727 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007615, test:0.003025 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007813, test:0.000345 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008062, test:0.000673 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007784, test:0.001731 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007793, test:0.001465 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007339, test:0.000409 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007104, test:0.000411 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008327, test:0.001666 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008765, test:0.000515 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008329, test:0.001063 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008513, test:0.002574 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008134, test:0.002558 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007116, test:0.001231 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008127, test:0.000972 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007540, test:0.001100 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006959, test:0.000692 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013122, test:0.001177 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008040, test:0.001000 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008705, test:0.000495 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006857, test:0.000456 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007195, test:0.001149 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006499, test:0.000999 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.016508, test:0.000712 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008899, test:0.003600 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007747, test:0.000447 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007216, test:0.000897 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006808, test:0.002126 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007383, test:0.000843 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007492, test:0.001408 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007038, test:0.000484 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007070, test:0.000690 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010802, test:0.000606 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007972, test:0.002631 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010823, test:0.001063 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008335, test:0.000937 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005823, test:0.000369 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005323, test:0.000361 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005866, test:0.000360 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007101, test:0.000328 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005389, test:0.000317 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005166, test:0.000460 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005709, test:0.000510 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005028, test:0.000344 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006623, test:0.000306 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006235, test:0.000499 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004933, test:0.000342 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009305, test:0.000333 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005234, test:0.000353 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004845, test:0.000389 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005436, test:0.000405 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004983, test:0.000364 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004632, test:0.000335 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004993, test:0.000330 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005100, test:0.000319 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005184, test:0.000388 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004845, test:0.000300 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004880, test:0.000324 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006241, test:0.000394 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005270, test:0.000457 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005186, test:0.000484 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004974, test:0.000371 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005407, test:0.000591 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005672, test:0.000448 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005043, test:0.000379 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004691, test:0.000359 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005583, test:0.000473 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.011856, test:0.000433 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005547, test:0.000379 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005638, test:0.000331 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005410, test:0.000345 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005527, test:0.000300 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005148, test:0.000349 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005455, test:0.000306 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005905, test:0.000471 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004647, test:0.000327 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005027, test:0.000346 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004983, test:0.000320 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005036, test:0.000317 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004989, test:0.000340 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005120, test:0.000312 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004522, test:0.000334 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004854, test:0.000324 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004835, test:0.000309 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004713, test:0.000358 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006374, test:0.000310 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004645, test:0.000329 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[92/100] | loss train:0.007654, test:0.000309 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.007086, test:0.000294 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004997, test:0.000322 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004586, test:0.000321 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004372, test:0.000332 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004508, test:0.000324 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005316, test:0.000294 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004769, test:0.000295 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004621, test:0.000322 | lr:0.000100\n",
      "Mean absolute error:  1.779993637189353\n",
      "Root mean squared error:  5.82495565607361\n",
      "Epoch[1/100] | loss train:0.048810, test:0.005461 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010773, test:0.002852 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009780, test:0.000619 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009220, test:0.000433 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010707, test:0.000322 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009332, test:0.005770 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009212, test:0.002100 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009704, test:0.000310 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009341, test:0.001870 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009029, test:0.000394 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007631, test:0.003580 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008462, test:0.001352 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008565, test:0.002972 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007499, test:0.000446 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.022017, test:0.002810 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014720, test:0.000431 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007544, test:0.000515 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007272, test:0.000831 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009752, test:0.002791 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007262, test:0.001678 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007125, test:0.001750 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008914, test:0.000608 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006484, test:0.000842 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006804, test:0.001644 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007088, test:0.001922 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007447, test:0.000418 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006323, test:0.002843 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007322, test:0.000545 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007126, test:0.000306 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007076, test:0.001022 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009583, test:0.001581 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010511, test:0.003125 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007314, test:0.000468 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007798, test:0.000840 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007508, test:0.000833 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006794, test:0.000421 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006109, test:0.000402 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007406, test:0.000575 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006052, test:0.001152 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006393, test:0.000371 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005748, test:0.000281 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005659, test:0.000305 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004725, test:0.000419 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006010, test:0.000350 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005371, test:0.000324 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005313, test:0.000311 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005191, test:0.000330 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005429, test:0.000294 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005290, test:0.000367 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004866, test:0.000297 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005280, test:0.000314 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005173, test:0.000392 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005703, test:0.000334 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005425, test:0.000457 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005031, test:0.000330 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004781, test:0.000299 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006759, test:0.000298 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005091, test:0.000412 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004840, test:0.000361 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010029, test:0.000512 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005448, test:0.000279 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009539, test:0.000282 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005115, test:0.000318 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005399, test:0.000307 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005489, test:0.000502 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005209, test:0.000315 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006770, test:0.000305 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004568, test:0.000312 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004839, test:0.000294 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005628, test:0.000284 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005058, test:0.000280 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005249, test:0.000282 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005032, test:0.000382 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005354, test:0.000293 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005476, test:0.000323 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005171, test:0.000308 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004802, test:0.000314 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007024, test:0.000303 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004945, test:0.000316 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007515, test:0.000480 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005960, test:0.000299 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004836, test:0.000281 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005835, test:0.000294 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004596, test:0.000309 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005253, test:0.000312 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004794, test:0.000311 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004973, test:0.000316 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005417, test:0.000307 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005240, test:0.000302 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004810, test:0.000325 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005278, test:0.000269 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005454, test:0.000319 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005367, test:0.000357 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004600, test:0.000275 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005238, test:0.000317 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004918, test:0.000295 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006365, test:0.000295 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004863, test:0.000307 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004859, test:0.000279 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004919, test:0.000301 | lr:0.000100\n",
      "Mean absolute error:  1.7775452749156309\n",
      "Root mean squared error:  5.80536370576341\n",
      "Epoch[1/100] | loss train:0.066797, test:0.000425 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012583, test:0.001351 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011000, test:0.001952 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012309, test:0.000297 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010365, test:0.003999 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010137, test:0.003776 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009698, test:0.000747 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007401, test:0.000770 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009670, test:0.001035 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008015, test:0.003855 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014232, test:0.006658 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009018, test:0.000667 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007473, test:0.000331 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006723, test:0.000326 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007623, test:0.000589 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[16/100] | loss train:0.008422, test:0.000708 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008339, test:0.000762 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007258, test:0.000861 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012752, test:0.000806 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008491, test:0.000708 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008135, test:0.004333 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006754, test:0.000897 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007788, test:0.002845 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007801, test:0.000448 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009000, test:0.000640 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007377, test:0.000527 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007409, test:0.005662 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007112, test:0.002680 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007510, test:0.002753 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009988, test:0.001552 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010632, test:0.000570 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009608, test:0.000409 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007756, test:0.000602 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009695, test:0.000401 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007454, test:0.003312 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006690, test:0.000427 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006244, test:0.000871 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007890, test:0.000424 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006703, test:0.000470 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005904, test:0.001329 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005688, test:0.000678 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005695, test:0.000360 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007000, test:0.000352 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005070, test:0.000361 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006103, test:0.000340 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009512, test:0.000327 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005139, test:0.000337 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005922, test:0.000463 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005063, test:0.000332 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005322, test:0.000699 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005061, test:0.000339 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005738, test:0.000593 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005607, test:0.000367 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005470, test:0.000373 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005158, test:0.000347 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005559, test:0.000379 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007223, test:0.000386 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005044, test:0.000346 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005699, test:0.000341 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005169, test:0.000339 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005790, test:0.000353 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005197, test:0.000501 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005363, test:0.000346 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005199, test:0.000497 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006139, test:0.000410 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004743, test:0.000373 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005555, test:0.000415 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005089, test:0.000343 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005355, test:0.000533 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005251, test:0.000369 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005490, test:0.000403 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005572, test:0.000384 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004875, test:0.000331 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005262, test:0.000676 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005086, test:0.000342 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005290, test:0.000353 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005783, test:0.000336 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005241, test:0.000320 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005111, test:0.000349 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006488, test:0.000343 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005504, test:0.000310 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.007432, test:0.000334 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005634, test:0.000329 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004974, test:0.000333 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005089, test:0.000325 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004757, test:0.000298 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005491, test:0.000341 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005093, test:0.000334 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004869, test:0.000342 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005234, test:0.000380 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004843, test:0.000318 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005017, test:0.000336 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.007077, test:0.000335 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004503, test:0.000327 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004743, test:0.000325 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005155, test:0.000335 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004551, test:0.000385 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004699, test:0.000348 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005485, test:0.000317 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004661, test:0.000389 | lr:0.000100\n",
      "Mean absolute error:  1.7521986061588701\n",
      "Root mean squared error:  5.836996410332197\n",
      "Epoch[1/100] | loss train:0.071424, test:0.002486 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011944, test:0.002767 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013004, test:0.004018 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010640, test:0.000596 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008963, test:0.000302 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008323, test:0.001699 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007883, test:0.004789 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009666, test:0.002612 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009912, test:0.002828 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008116, test:0.001490 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013653, test:0.004672 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009518, test:0.002249 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008706, test:0.000600 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008134, test:0.000682 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008677, test:0.000683 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008669, test:0.000356 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007763, test:0.001789 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008071, test:0.004110 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008883, test:0.002028 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007511, test:0.000488 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007629, test:0.000362 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007293, test:0.000411 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009564, test:0.000980 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006358, test:0.003331 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007305, test:0.000651 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007536, test:0.000626 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006687, test:0.000623 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007893, test:0.000566 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009289, test:0.000808 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009940, test:0.001138 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008671, test:0.001186 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007104, test:0.001105 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007417, test:0.000535 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008002, test:0.002193 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006898, test:0.000441 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007553, test:0.000388 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006951, test:0.002206 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007056, test:0.001765 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010753, test:0.000810 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007589, test:0.000883 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/100] | loss train:0.006495, test:0.000355 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005274, test:0.000337 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006329, test:0.000348 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.014641, test:0.000348 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005300, test:0.000594 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005464, test:0.000336 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005492, test:0.000380 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004884, test:0.000354 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005652, test:0.000443 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008196, test:0.000365 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007651, test:0.000372 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005307, test:0.000399 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005698, test:0.000391 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006246, test:0.000429 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005473, test:0.000317 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006348, test:0.000348 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006946, test:0.000392 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004741, test:0.000327 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005103, test:0.000463 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005307, test:0.000353 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005870, test:0.000394 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005321, test:0.000353 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005711, test:0.000335 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005734, test:0.000364 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005388, test:0.000519 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005402, test:0.000325 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005079, test:0.000295 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005133, test:0.000392 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005227, test:0.000325 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005937, test:0.000313 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005094, test:0.000385 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005189, test:0.000368 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005539, test:0.000323 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004912, test:0.000310 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005607, test:0.000299 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004891, test:0.000380 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005607, test:0.000342 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005249, test:0.000323 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005701, test:0.000296 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005346, test:0.000327 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005601, test:0.000308 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005641, test:0.000303 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004834, test:0.000309 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.008451, test:0.000330 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005055, test:0.000318 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005073, test:0.000332 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004854, test:0.000324 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005368, test:0.000318 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004770, test:0.000322 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004900, test:0.000294 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004996, test:0.000324 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005497, test:0.000317 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004994, test:0.000284 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005863, test:0.000318 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005365, test:0.000327 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004968, test:0.000306 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004936, test:0.000297 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005081, test:0.000327 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004770, test:0.000308 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005784, test:0.000305 | lr:0.000100\n",
      "Mean absolute error:  1.657556077605216\n",
      "Root mean squared error:  5.772114569451244\n",
      "Epoch[1/100] | loss train:0.066614, test:0.001035 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015351, test:0.001599 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010841, test:0.007934 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010561, test:0.000554 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009925, test:0.000416 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009162, test:0.000367 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010023, test:0.000798 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008381, test:0.000570 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008873, test:0.000522 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008799, test:0.000591 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009501, test:0.003020 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008609, test:0.001916 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008229, test:0.001597 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.016295, test:0.002431 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010727, test:0.002059 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010099, test:0.000458 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007608, test:0.000411 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007410, test:0.000717 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008698, test:0.000549 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008889, test:0.000481 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008112, test:0.000485 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007680, test:0.000835 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008784, test:0.001299 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013519, test:0.000584 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007385, test:0.000437 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009153, test:0.000423 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008410, test:0.000429 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008156, test:0.000864 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007010, test:0.000509 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008276, test:0.000859 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008758, test:0.000588 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006236, test:0.001805 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007547, test:0.001162 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007636, test:0.001143 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010345, test:0.001998 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007897, test:0.001177 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007147, test:0.002887 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008320, test:0.000673 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007496, test:0.000939 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006961, test:0.001594 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005531, test:0.000510 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005490, test:0.000430 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005682, test:0.000461 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005374, test:0.000690 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005327, test:0.000527 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005622, test:0.000444 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005374, test:0.000394 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005573, test:0.000484 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005535, test:0.000479 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005464, test:0.000471 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005047, test:0.000551 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005244, test:0.000378 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005801, test:0.000447 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005563, test:0.000389 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005986, test:0.000385 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005605, test:0.000510 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005336, test:0.000949 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005680, test:0.000362 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005531, test:0.000473 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005229, test:0.000376 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005688, test:0.000702 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006728, test:0.000451 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005182, test:0.000496 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005411, test:0.000355 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006811, test:0.000380 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[66/100] | loss train:0.012307, test:0.000382 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007288, test:0.000381 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006259, test:0.000403 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005516, test:0.000409 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005771, test:0.000405 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006554, test:0.000380 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005602, test:0.000332 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005766, test:0.000350 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004946, test:0.000408 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005608, test:0.000339 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005473, test:0.000643 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005764, test:0.000339 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005647, test:0.000376 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005937, test:0.000320 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006441, test:0.000403 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005327, test:0.000354 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006821, test:0.000336 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005024, test:0.000325 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005155, test:0.000323 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005649, test:0.000304 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005281, test:0.000356 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005259, test:0.000343 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005020, test:0.000335 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005022, test:0.000346 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005304, test:0.000309 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005349, test:0.000345 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005431, test:0.000329 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005219, test:0.000443 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.009039, test:0.000325 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.006115, test:0.000372 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007777, test:0.000369 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.007079, test:0.000351 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005029, test:0.000335 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005200, test:0.000355 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005201, test:0.000353 | lr:0.000100\n",
      "Mean absolute error:  1.722695812118273\n",
      "Root mean squared error:  5.841155267686176\n",
      "Epoch[1/100] | loss train:0.055034, test:0.002143 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011757, test:0.001748 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009664, test:0.017641 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011929, test:0.006449 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011323, test:0.001669 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009111, test:0.000358 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012390, test:0.004534 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010931, test:0.000586 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008123, test:0.001180 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008342, test:0.001657 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008589, test:0.000339 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008790, test:0.000745 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008069, test:0.003895 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007996, test:0.001015 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008019, test:0.002195 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009276, test:0.000681 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008129, test:0.003036 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007747, test:0.000498 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011082, test:0.003097 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012400, test:0.004588 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008725, test:0.000401 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006473, test:0.000590 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010665, test:0.003897 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007352, test:0.000740 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007247, test:0.001485 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008281, test:0.001315 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007289, test:0.000766 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007959, test:0.001141 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006738, test:0.001937 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007308, test:0.000558 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007222, test:0.000806 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007603, test:0.000414 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009104, test:0.006048 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007318, test:0.000441 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006545, test:0.000756 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006838, test:0.000683 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006436, test:0.000678 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006626, test:0.001790 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008557, test:0.001561 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007614, test:0.004402 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006006, test:0.000778 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006100, test:0.000477 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005679, test:0.000666 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008105, test:0.000555 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005467, test:0.000427 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005453, test:0.000392 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005287, test:0.000378 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005070, test:0.000719 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004911, test:0.000433 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005235, test:0.000386 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005392, test:0.000398 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005534, test:0.000384 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006111, test:0.000400 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006038, test:0.000463 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005315, test:0.000394 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005876, test:0.000489 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004937, test:0.000363 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006354, test:0.000353 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006014, test:0.000681 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005090, test:0.000434 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005431, test:0.000413 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007604, test:0.000523 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005684, test:0.000323 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004748, test:0.000364 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005439, test:0.000405 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005663, test:0.000483 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005149, test:0.000337 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005507, test:0.000380 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006884, test:0.000346 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005869, test:0.000310 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005233, test:0.000352 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005112, test:0.001387 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004969, test:0.000416 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005667, test:0.000372 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006430, test:0.000539 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005399, test:0.000485 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005161, test:0.000366 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005452, test:0.000425 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005092, test:0.000330 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007521, test:0.000400 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004707, test:0.000342 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004740, test:0.000388 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004670, test:0.000359 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005104, test:0.000388 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005291, test:0.000389 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005256, test:0.000327 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004762, test:0.000338 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005966, test:0.000340 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005938, test:0.000338 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005368, test:0.000340 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[91/100] | loss train:0.005377, test:0.000354 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005195, test:0.000410 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004972, test:0.000369 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004611, test:0.000411 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.006664, test:0.000325 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004724, test:0.000360 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005062, test:0.000343 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005031, test:0.000364 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006993, test:0.000322 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004927, test:0.000339 | lr:0.000100\n",
      "Mean absolute error:  1.7173128737981744\n",
      "Root mean squared error:  5.814754534285913\n",
      "Epoch[1/100] | loss train:0.057976, test:0.000510 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013613, test:0.000598 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012437, test:0.004981 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010263, test:0.000440 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.017738, test:0.007335 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.017273, test:0.002339 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010239, test:0.000533 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.017693, test:0.002851 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009707, test:0.001786 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010703, test:0.010454 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012368, test:0.001476 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009163, test:0.000413 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007929, test:0.000581 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008898, test:0.002977 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009076, test:0.000431 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008004, test:0.000406 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008722, test:0.000357 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009710, test:0.000337 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007882, test:0.000507 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007411, test:0.000407 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007040, test:0.002798 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012345, test:0.001135 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007446, test:0.001247 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008738, test:0.000408 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009452, test:0.001738 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009284, test:0.000718 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008303, test:0.000718 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007674, test:0.000882 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007948, test:0.000352 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012873, test:0.000377 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009104, test:0.000430 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009607, test:0.000995 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010136, test:0.001883 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007636, test:0.000477 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007151, test:0.000460 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009598, test:0.000363 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010737, test:0.000930 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008787, test:0.001027 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007316, test:0.000991 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008843, test:0.000603 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006480, test:0.000424 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006379, test:0.000400 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006508, test:0.000525 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006029, test:0.000648 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008791, test:0.000455 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005791, test:0.000369 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005454, test:0.000514 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005961, test:0.000593 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005825, test:0.000403 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006282, test:0.000493 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006147, test:0.000561 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005533, test:0.000441 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005745, test:0.000351 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005144, test:0.000575 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005410, test:0.000451 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006060, test:0.000388 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005627, test:0.000360 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005979, test:0.000390 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005733, test:0.000419 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005908, test:0.000318 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005638, test:0.000373 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004935, test:0.000499 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005849, test:0.000346 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005683, test:0.000334 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007695, test:0.000332 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005841, test:0.000494 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005265, test:0.000465 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005712, test:0.001002 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005666, test:0.000361 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005699, test:0.000329 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005903, test:0.000442 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005672, test:0.000490 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006016, test:0.000372 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005957, test:0.000448 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005885, test:0.000386 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005737, test:0.000507 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005909, test:0.000500 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005613, test:0.000286 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005833, test:0.000329 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005521, test:0.000385 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005282, test:0.000355 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005599, test:0.000299 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.008216, test:0.000298 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005578, test:0.000326 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005983, test:0.000372 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004991, test:0.000309 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005444, test:0.000299 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.006395, test:0.000293 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.007291, test:0.000294 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005444, test:0.000316 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005688, test:0.000332 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005538, test:0.000320 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.006812, test:0.000307 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.011030, test:0.000325 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005452, test:0.000337 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.006330, test:0.000310 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005393, test:0.000317 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005538, test:0.000296 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005170, test:0.000337 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.006412, test:0.000316 | lr:0.000100\n",
      "Mean absolute error:  1.6563207298535179\n",
      "Root mean squared error:  5.783776780953786\n",
      "Epoch[1/100] | loss train:0.050036, test:0.001200 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015179, test:0.012307 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013055, test:0.001109 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010556, test:0.001959 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008752, test:0.002644 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011634, test:0.000799 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008355, test:0.002723 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007707, test:0.000610 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007371, test:0.000655 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009315, test:0.003740 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013513, test:0.005267 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012078, test:0.002559 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008768, test:0.000406 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010144, test:0.002497 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/100] | loss train:0.009581, test:0.001409 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007560, test:0.004125 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011314, test:0.000707 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007654, test:0.002108 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008535, test:0.000413 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007339, test:0.000484 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007050, test:0.002803 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007741, test:0.000369 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009287, test:0.004247 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008894, test:0.000922 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012293, test:0.000757 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006750, test:0.000419 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010332, test:0.002663 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008003, test:0.000719 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006672, test:0.000587 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009426, test:0.000534 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007439, test:0.003288 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007017, test:0.001825 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008718, test:0.000620 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008539, test:0.000447 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007009, test:0.000608 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010339, test:0.000724 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008660, test:0.000957 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007786, test:0.001117 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006782, test:0.000359 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007824, test:0.001342 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006553, test:0.000439 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006980, test:0.000446 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005106, test:0.000563 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005226, test:0.000403 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005172, test:0.000390 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005156, test:0.000492 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005278, test:0.000399 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005495, test:0.000377 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005418, test:0.000358 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005474, test:0.000339 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005393, test:0.000525 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005674, test:0.000346 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005399, test:0.000502 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005291, test:0.000375 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007819, test:0.000514 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005647, test:0.000399 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005943, test:0.000405 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005694, test:0.000411 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006000, test:0.000377 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005414, test:0.000499 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005545, test:0.000440 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005676, test:0.000391 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005971, test:0.000390 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005470, test:0.000420 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007672, test:0.000581 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005038, test:0.000425 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005447, test:0.000558 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005461, test:0.000417 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006485, test:0.000355 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005621, test:0.000355 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005689, test:0.000296 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005268, test:0.000423 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005347, test:0.000415 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006758, test:0.000368 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005289, test:0.000315 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006431, test:0.000342 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005427, test:0.000334 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005711, test:0.000350 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005575, test:0.000386 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005655, test:0.000347 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005036, test:0.000349 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005030, test:0.000332 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005709, test:0.000376 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006680, test:0.000353 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005831, test:0.000325 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005173, test:0.000366 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005171, test:0.000362 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004982, test:0.000420 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005046, test:0.000369 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005600, test:0.000358 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005180, test:0.000360 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006302, test:0.000327 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005394, test:0.000379 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.013094, test:0.000364 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005399, test:0.000321 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005321, test:0.000322 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005353, test:0.000337 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005043, test:0.000343 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004949, test:0.000355 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005357, test:0.000397 | lr:0.000100\n",
      "Mean absolute error:  1.7510373925565803\n",
      "Root mean squared error:  5.849891486606795\n",
      "Epoch[1/100] | loss train:0.058618, test:0.000905 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012438, test:0.000702 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011511, test:0.000327 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009500, test:0.002449 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010436, test:0.001895 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007875, test:0.000305 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007705, test:0.002872 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008511, test:0.006552 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009732, test:0.004683 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008189, test:0.000655 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008505, test:0.004520 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011082, test:0.002567 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008082, test:0.000585 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007672, test:0.000695 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007752, test:0.000377 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009461, test:0.002127 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008384, test:0.000507 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007244, test:0.000504 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008452, test:0.000951 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008287, test:0.000437 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007914, test:0.002123 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007497, test:0.001206 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006863, test:0.000403 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008358, test:0.003129 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007841, test:0.001134 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008528, test:0.003812 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008596, test:0.000430 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008282, test:0.001028 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007521, test:0.000668 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006893, test:0.001770 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006982, test:0.000562 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007341, test:0.000839 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007453, test:0.000824 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007422, test:0.002433 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007958, test:0.001959 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007102, test:0.004474 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007468, test:0.000403 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009835, test:0.000988 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007821, test:0.000874 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[40/100] | loss train:0.007513, test:0.000763 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008510, test:0.000373 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005204, test:0.000394 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005740, test:0.000384 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005662, test:0.000469 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004973, test:0.000454 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005743, test:0.000345 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005345, test:0.000339 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005200, test:0.000478 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009398, test:0.000427 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006157, test:0.000410 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005810, test:0.000936 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007088, test:0.000365 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005620, test:0.000402 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005168, test:0.000438 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005142, test:0.000377 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005188, test:0.000335 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005889, test:0.000371 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005685, test:0.000371 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004851, test:0.000436 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006434, test:0.000344 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005082, test:0.000350 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005707, test:0.000515 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004992, test:0.000477 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005434, test:0.000350 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006291, test:0.000405 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004795, test:0.000339 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005243, test:0.000436 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005217, test:0.000512 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005059, test:0.000656 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005342, test:0.000389 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006040, test:0.000493 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004997, test:0.000324 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005304, test:0.000397 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005333, test:0.000329 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004741, test:0.000431 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005551, test:0.000338 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005550, test:0.000352 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005210, test:0.000350 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005155, test:0.000316 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005523, test:0.000395 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.007014, test:0.000353 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005307, test:0.000328 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004943, test:0.000319 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004811, test:0.000314 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006382, test:0.000322 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004794, test:0.000350 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006264, test:0.000364 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005402, test:0.000339 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005349, test:0.000309 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005573, test:0.000312 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.006033, test:0.000297 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004908, test:0.000344 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005125, test:0.000335 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005175, test:0.000319 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004517, test:0.000384 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007426, test:0.000330 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005182, test:0.000305 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006368, test:0.000366 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005173, test:0.000353 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004810, test:0.000345 | lr:0.000100\n",
      "Mean absolute error:  1.6764537426103352\n",
      "Root mean squared error:  5.793925897125784\n",
      "Epoch[1/100] | loss train:0.036409, test:0.002866 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010796, test:0.003677 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009584, test:0.005661 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010167, test:0.002323 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009196, test:0.002164 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009258, test:0.004683 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010664, test:0.000876 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008058, test:0.000546 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009575, test:0.002688 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009318, test:0.003738 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008694, test:0.000427 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009942, test:0.001774 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007849, test:0.001534 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008143, test:0.004886 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013525, test:0.000823 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011080, test:0.000754 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007218, test:0.000948 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008567, test:0.000453 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009492, test:0.000987 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007109, test:0.001536 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008176, test:0.000535 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008475, test:0.002938 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007641, test:0.000683 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007654, test:0.001121 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007457, test:0.000803 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008138, test:0.001605 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008073, test:0.001867 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009654, test:0.002832 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008194, test:0.002712 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008175, test:0.000950 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007722, test:0.000889 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007423, test:0.000402 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006941, test:0.002440 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007164, test:0.000420 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006265, test:0.000426 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007153, test:0.001104 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007212, test:0.005098 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009176, test:0.000828 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006533, test:0.000756 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007304, test:0.000402 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005811, test:0.000493 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005299, test:0.000420 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005132, test:0.000428 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004790, test:0.000340 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005139, test:0.000382 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006611, test:0.000338 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005663, test:0.000291 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005881, test:0.000314 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005072, test:0.000305 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005085, test:0.000319 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006192, test:0.000312 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007608, test:0.000314 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005300, test:0.000321 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005888, test:0.000358 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005189, test:0.000301 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005306, test:0.000316 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006267, test:0.000355 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005866, test:0.000334 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007157, test:0.000387 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005157, test:0.000386 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005559, test:0.000360 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004840, test:0.000482 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005310, test:0.000531 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004841, test:0.000313 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[65/100] | loss train:0.005428, test:0.000485 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006716, test:0.000323 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004844, test:0.000378 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004964, test:0.000311 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004936, test:0.000277 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004763, test:0.000355 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005941, test:0.000339 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005227, test:0.000291 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005115, test:0.000331 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005146, test:0.000484 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006404, test:0.000495 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005424, test:0.000491 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005210, test:0.000444 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007702, test:0.000335 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004967, test:0.000318 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005087, test:0.000324 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006836, test:0.000300 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005001, test:0.000272 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004809, test:0.000310 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004567, test:0.000285 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005054, test:0.000285 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005827, test:0.000316 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004866, test:0.000332 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.007227, test:0.000282 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005721, test:0.000293 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005219, test:0.000302 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005730, test:0.000281 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005096, test:0.000314 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004838, test:0.000286 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005464, test:0.000301 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005634, test:0.000273 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004934, test:0.000310 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004682, test:0.000279 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004686, test:0.000310 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006123, test:0.000276 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.008996, test:0.000297 | lr:0.000100\n",
      "Mean absolute error:  1.6547551681595174\n",
      "Root mean squared error:  5.769459902324024\n",
      "Epoch[1/100] | loss train:0.053715, test:0.001350 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012960, test:0.003171 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012444, test:0.002002 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011202, test:0.001675 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010719, test:0.004276 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012051, test:0.000370 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010099, test:0.000335 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.018100, test:0.000339 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008514, test:0.000408 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011182, test:0.002039 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009039, test:0.002528 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008159, test:0.000524 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013049, test:0.000579 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007861, test:0.000356 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011966, test:0.001247 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010423, test:0.000366 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007621, test:0.000479 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010829, test:0.001161 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009197, test:0.001306 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009000, test:0.001837 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008686, test:0.001188 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007222, test:0.001003 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006972, test:0.000739 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010900, test:0.001560 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007932, test:0.001465 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007519, test:0.000407 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008027, test:0.001914 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009425, test:0.001876 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008453, test:0.000966 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007371, test:0.000392 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007584, test:0.001253 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006737, test:0.000954 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012035, test:0.000569 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007437, test:0.000852 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007116, test:0.000371 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.013347, test:0.006126 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007734, test:0.001875 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006701, test:0.000391 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007144, test:0.000955 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007257, test:0.000785 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005623, test:0.000404 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005877, test:0.000586 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006972, test:0.000356 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006320, test:0.000439 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005488, test:0.000372 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005967, test:0.000578 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005569, test:0.000407 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005428, test:0.000436 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005532, test:0.000544 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005521, test:0.000702 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006562, test:0.000416 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005304, test:0.000406 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005187, test:0.000465 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005015, test:0.000381 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007101, test:0.000345 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005848, test:0.000372 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005636, test:0.000661 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005331, test:0.000382 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006826, test:0.000492 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005351, test:0.000404 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005984, test:0.000348 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005421, test:0.000499 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006267, test:0.000401 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005608, test:0.000493 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007068, test:0.000347 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005032, test:0.000504 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005399, test:0.000397 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005288, test:0.000357 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005588, test:0.000338 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005397, test:0.000420 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005803, test:0.000314 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005657, test:0.000323 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005475, test:0.000346 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005250, test:0.000375 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005289, test:0.000382 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.009622, test:0.000322 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005193, test:0.000465 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005337, test:0.000353 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005323, test:0.000529 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005235, test:0.000335 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005317, test:0.000305 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005867, test:0.000353 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005438, test:0.000324 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005083, test:0.000339 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005085, test:0.000354 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005117, test:0.000313 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005619, test:0.000338 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005605, test:0.000314 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.006138, test:0.000311 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[90/100] | loss train:0.006482, test:0.000328 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005043, test:0.000332 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005338, test:0.000319 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005311, test:0.000332 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005828, test:0.000309 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004937, test:0.000312 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005085, test:0.000349 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006213, test:0.000365 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.012680, test:0.000345 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005863, test:0.000335 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005102, test:0.000325 | lr:0.000100\n",
      "Mean absolute error:  1.697859859910136\n",
      "Root mean squared error:  5.793472830055685\n",
      "Epoch[1/100] | loss train:0.052067, test:0.008178 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012729, test:0.000391 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009444, test:0.000461 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008728, test:0.002568 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009167, test:0.001852 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009420, test:0.001926 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007954, test:0.002361 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007171, test:0.000838 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007405, test:0.001117 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007439, test:0.000475 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008959, test:0.000324 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008114, test:0.007791 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008837, test:0.001270 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006645, test:0.000410 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007901, test:0.001925 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007389, test:0.001398 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009508, test:0.000945 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006784, test:0.000637 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007774, test:0.000928 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008232, test:0.001607 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007489, test:0.005529 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007935, test:0.001250 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007695, test:0.000854 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006960, test:0.000430 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007505, test:0.000414 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006744, test:0.000597 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007428, test:0.002465 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007240, test:0.000451 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007178, test:0.000373 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006590, test:0.002178 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006720, test:0.000948 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007348, test:0.000674 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007754, test:0.002519 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006536, test:0.000453 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007135, test:0.001208 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008560, test:0.000425 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008195, test:0.000350 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006671, test:0.000891 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007061, test:0.000546 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006893, test:0.001908 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005662, test:0.000569 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005573, test:0.000456 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006390, test:0.000429 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005373, test:0.000431 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006143, test:0.000664 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005575, test:0.000514 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004956, test:0.000422 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004874, test:0.000457 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005090, test:0.000380 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007711, test:0.000400 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005207, test:0.000466 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005123, test:0.000405 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005096, test:0.000373 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005588, test:0.000384 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005452, test:0.000377 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005157, test:0.000385 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005283, test:0.000537 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005502, test:0.000648 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005644, test:0.000332 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005108, test:0.000418 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005266, test:0.000406 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005338, test:0.000352 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005020, test:0.000381 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005365, test:0.000359 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004767, test:0.000422 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005379, test:0.000337 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005230, test:0.000376 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005194, test:0.000353 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008944, test:0.000320 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005653, test:0.000394 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005848, test:0.000321 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004826, test:0.000370 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005740, test:0.000375 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005170, test:0.000337 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005473, test:0.000324 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004779, test:0.000454 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005216, test:0.000341 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004795, test:0.000674 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007548, test:0.000436 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004681, test:0.000431 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005098, test:0.000323 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.010857, test:0.000360 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004646, test:0.000334 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006312, test:0.000350 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005017, test:0.000329 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005075, test:0.000347 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005165, test:0.000344 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005502, test:0.000380 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005210, test:0.000387 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005092, test:0.000329 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004933, test:0.000339 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004977, test:0.000327 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005000, test:0.000330 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004938, test:0.000346 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004565, test:0.000366 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004721, test:0.000342 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004775, test:0.000381 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004956, test:0.000315 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004693, test:0.000356 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004985, test:0.000320 | lr:0.000100\n",
      "Mean absolute error:  1.7498460589030318\n",
      "Root mean squared error:  5.817568605320074\n",
      "Epoch[1/100] | loss train:0.064236, test:0.003013 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013929, test:0.000752 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011404, test:0.000947 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013875, test:0.001439 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009441, test:0.001911 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009561, test:0.001938 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009621, test:0.000310 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010192, test:0.000614 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013770, test:0.000446 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008317, test:0.000530 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008618, test:0.001182 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008617, test:0.000750 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008357, test:0.000408 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[14/100] | loss train:0.016170, test:0.008396 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011448, test:0.000784 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007760, test:0.000812 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008367, test:0.000969 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006900, test:0.000847 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007762, test:0.000395 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007937, test:0.002963 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007626, test:0.000373 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006880, test:0.000450 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007870, test:0.000467 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008964, test:0.000339 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006261, test:0.004839 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008319, test:0.000980 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006752, test:0.000391 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007229, test:0.000584 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006799, test:0.000823 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007495, test:0.000897 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006705, test:0.000378 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006851, test:0.000349 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006615, test:0.002513 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007461, test:0.000609 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007799, test:0.000531 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008124, test:0.001138 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007299, test:0.001386 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007197, test:0.000488 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008219, test:0.000796 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008472, test:0.000429 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005420, test:0.000379 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005542, test:0.000396 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005184, test:0.000288 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005419, test:0.000339 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005276, test:0.000323 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005797, test:0.000289 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005764, test:0.000365 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004898, test:0.000401 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006782, test:0.000380 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005726, test:0.000345 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005321, test:0.000322 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004876, test:0.000324 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005374, test:0.000358 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004856, test:0.000411 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004910, test:0.000350 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004765, test:0.000379 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005871, test:0.000408 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005023, test:0.000306 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005660, test:0.000461 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005951, test:0.000294 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005212, test:0.000302 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004873, test:0.000301 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005123, test:0.000406 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004351, test:0.000316 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006002, test:0.000296 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005328, test:0.000322 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005228, test:0.000599 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006476, test:0.000317 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005476, test:0.000311 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005003, test:0.000321 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005141, test:0.000369 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005090, test:0.000313 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005391, test:0.000355 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005299, test:0.000403 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004547, test:0.000349 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005030, test:0.000411 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005179, test:0.000297 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005534, test:0.000293 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005691, test:0.000347 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005017, test:0.000334 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004876, test:0.000297 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004734, test:0.000274 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004941, test:0.000285 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005178, test:0.000302 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004897, test:0.000282 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004785, test:0.000281 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.006114, test:0.000286 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005741, test:0.000293 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.006297, test:0.000289 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004888, test:0.000283 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004778, test:0.000296 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006340, test:0.000308 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004663, test:0.000287 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004865, test:0.000277 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005162, test:0.000265 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005466, test:0.000283 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005339, test:0.000351 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005158, test:0.000284 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004542, test:0.000263 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005053, test:0.000303 | lr:0.000100\n",
      "Mean absolute error:  1.8927331878222622\n",
      "Root mean squared error:  5.843873524802346\n",
      "Epoch[1/100] | loss train:0.053253, test:0.001617 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011872, test:0.000565 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010117, test:0.001723 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011275, test:0.000332 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009585, test:0.000908 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008557, test:0.000458 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008491, test:0.001270 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008559, test:0.001065 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008673, test:0.001098 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008417, test:0.005348 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008952, test:0.002905 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007658, test:0.000515 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007116, test:0.000434 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009018, test:0.000392 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009510, test:0.000500 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009384, test:0.002333 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010417, test:0.000461 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008551, test:0.000428 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007947, test:0.003608 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007552, test:0.001129 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010816, test:0.000850 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007838, test:0.001183 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007594, test:0.000882 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011047, test:0.000490 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007682, test:0.000618 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007258, test:0.002690 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008300, test:0.001141 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009015, test:0.000631 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010065, test:0.000469 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007094, test:0.001978 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010941, test:0.005771 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009807, test:0.000409 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007247, test:0.001441 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007316, test:0.000685 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008942, test:0.001437 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007099, test:0.000486 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006266, test:0.000399 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006696, test:0.000367 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/100] | loss train:0.007483, test:0.000972 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007321, test:0.001772 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006208, test:0.000448 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006177, test:0.000564 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005185, test:0.000400 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005741, test:0.000903 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005196, test:0.000384 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006633, test:0.000466 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006486, test:0.000436 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006834, test:0.000446 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006119, test:0.000388 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005410, test:0.000402 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005137, test:0.000381 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005428, test:0.000388 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005163, test:0.000375 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005819, test:0.000431 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005453, test:0.000530 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005547, test:0.000409 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005967, test:0.000385 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006051, test:0.000395 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005197, test:0.000476 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005805, test:0.000430 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005312, test:0.000490 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007964, test:0.000383 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005770, test:0.000356 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005656, test:0.000367 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005392, test:0.000384 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006002, test:0.000493 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005025, test:0.000371 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005138, test:0.000378 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005062, test:0.000381 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005142, test:0.000512 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006609, test:0.000366 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005261, test:0.000382 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005978, test:0.000660 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005237, test:0.000361 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005916, test:0.000416 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005354, test:0.000376 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005886, test:0.000379 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005599, test:0.000321 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005284, test:0.000354 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005605, test:0.000504 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004995, test:0.000426 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005917, test:0.000349 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004823, test:0.000321 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005555, test:0.000387 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004825, test:0.000350 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.006207, test:0.000331 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004887, test:0.000338 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005251, test:0.000350 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.006683, test:0.000338 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004664, test:0.000327 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005418, test:0.000349 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005392, test:0.000319 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005387, test:0.000321 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004655, test:0.000363 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005177, test:0.000340 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005296, test:0.000327 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004760, test:0.000346 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005676, test:0.000366 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005064, test:0.000368 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005222, test:0.000372 | lr:0.000100\n",
      "Mean absolute error:  1.7890195284676438\n",
      "Root mean squared error:  5.881232713496093\n",
      "Epoch[1/100] | loss train:0.069721, test:0.000722 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011203, test:0.000670 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010593, test:0.004813 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009886, test:0.000321 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008359, test:0.000582 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011374, test:0.000364 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010409, test:0.000350 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010236, test:0.005755 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013906, test:0.000497 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009587, test:0.001162 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009078, test:0.000359 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008060, test:0.000915 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008550, test:0.000364 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007321, test:0.000980 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006824, test:0.000639 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008931, test:0.000910 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008188, test:0.000444 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008554, test:0.001273 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007143, test:0.001107 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009127, test:0.000524 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007474, test:0.001124 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007297, test:0.000454 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007349, test:0.000938 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008335, test:0.001083 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008734, test:0.000528 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007357, test:0.001056 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007333, test:0.000460 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008986, test:0.002212 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008451, test:0.001471 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006998, test:0.000585 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007557, test:0.000519 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007992, test:0.000455 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007546, test:0.000401 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006992, test:0.000488 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007061, test:0.001131 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.015982, test:0.001814 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009343, test:0.003473 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006921, test:0.000569 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.016084, test:0.000415 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008997, test:0.000443 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006081, test:0.000405 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005753, test:0.000409 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006136, test:0.000425 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005820, test:0.000477 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005820, test:0.000422 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005839, test:0.000455 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005738, test:0.000561 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005395, test:0.000397 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005498, test:0.000717 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005914, test:0.000414 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006795, test:0.000399 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005844, test:0.000393 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005506, test:0.000369 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005291, test:0.000481 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006085, test:0.000466 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006152, test:0.000420 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005416, test:0.000377 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005503, test:0.000360 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005911, test:0.000379 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005704, test:0.000345 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006080, test:0.000374 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005608, test:0.000418 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005419, test:0.000587 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[64/100] | loss train:0.005765, test:0.000463 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005299, test:0.000376 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.014184, test:0.000336 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005072, test:0.000408 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.009251, test:0.000443 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005063, test:0.000413 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006710, test:0.000773 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006750, test:0.000410 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005771, test:0.000358 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006487, test:0.000488 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004704, test:0.000347 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005620, test:0.000393 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007795, test:0.000378 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007016, test:0.000750 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006618, test:0.000320 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.008095, test:0.000391 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005272, test:0.000423 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.010833, test:0.000359 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005592, test:0.000349 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005330, test:0.000335 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005814, test:0.000345 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005521, test:0.000346 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005709, test:0.000332 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005339, test:0.000315 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005778, test:0.000337 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.008576, test:0.000329 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005388, test:0.000320 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004849, test:0.000378 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004880, test:0.000322 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005268, test:0.000334 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004974, test:0.000309 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005095, test:0.000310 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.010004, test:0.000328 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005125, test:0.000320 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005293, test:0.000321 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006007, test:0.000326 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005401, test:0.000324 | lr:0.000100\n",
      "Mean absolute error:  1.6896534084631911\n",
      "Root mean squared error:  5.7952531778882275\n",
      "Epoch[1/100] | loss train:0.045086, test:0.000528 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010743, test:0.000602 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012624, test:0.006980 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014348, test:0.002458 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009848, test:0.001797 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009397, test:0.000387 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009175, test:0.001395 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009090, test:0.000335 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008721, test:0.000328 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007914, test:0.001606 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008541, test:0.001171 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007932, test:0.003668 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008128, test:0.000540 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009098, test:0.000774 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007071, test:0.001572 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007448, test:0.001893 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007494, test:0.000304 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008976, test:0.002510 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008371, test:0.000440 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006279, test:0.000483 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007181, test:0.000299 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006783, test:0.004324 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007996, test:0.001078 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008383, test:0.001960 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008402, test:0.001145 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009153, test:0.001463 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006465, test:0.000443 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006558, test:0.002813 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006868, test:0.000461 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008384, test:0.000504 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006899, test:0.000412 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006807, test:0.000663 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010735, test:0.000780 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007380, test:0.001397 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006729, test:0.000311 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007487, test:0.000304 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006429, test:0.002339 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006704, test:0.002133 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009694, test:0.000286 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006577, test:0.000922 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005319, test:0.000313 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005239, test:0.000284 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005269, test:0.000317 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005375, test:0.000297 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004730, test:0.000271 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008945, test:0.001088 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005341, test:0.000396 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005122, test:0.000291 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005509, test:0.000326 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005171, test:0.000391 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005504, test:0.000269 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005022, test:0.000457 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005959, test:0.000275 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005691, test:0.000285 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005606, test:0.000288 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005612, test:0.000299 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005704, test:0.000378 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004982, test:0.000302 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004784, test:0.000280 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005226, test:0.000313 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005064, test:0.000300 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010934, test:0.000458 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005234, test:0.000360 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005543, test:0.000319 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004796, test:0.000450 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006010, test:0.000341 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005093, test:0.000323 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006003, test:0.000347 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005017, test:0.000297 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004804, test:0.000261 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005422, test:0.000375 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006159, test:0.000342 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005028, test:0.000343 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004954, test:0.000372 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005193, test:0.000300 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004780, test:0.000316 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005046, test:0.000295 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005567, test:0.000316 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005402, test:0.000311 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.009168, test:0.000277 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005121, test:0.000284 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004916, test:0.000299 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.011998, test:0.000296 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005384, test:0.000281 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005182, test:0.000275 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004605, test:0.000281 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004757, test:0.000267 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005663, test:0.000275 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[89/100] | loss train:0.005890, test:0.000295 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005138, test:0.000284 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004895, test:0.000280 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004819, test:0.000277 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004924, test:0.000278 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005180, test:0.000292 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005405, test:0.000311 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005070, test:0.000299 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005139, test:0.000299 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004666, test:0.000284 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004681, test:0.000274 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005052, test:0.000296 | lr:0.000100\n",
      "Mean absolute error:  1.643675020697297\n",
      "Root mean squared error:  5.756958717084156\n",
      "Epoch[1/100] | loss train:0.047004, test:0.002770 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012415, test:0.003315 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010829, test:0.000651 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008207, test:0.002410 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010809, test:0.000972 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008672, test:0.000676 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009276, test:0.000364 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008732, test:0.000674 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007336, test:0.002275 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008076, test:0.000303 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012962, test:0.000643 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.016535, test:0.001072 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010246, test:0.000325 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008014, test:0.001119 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007455, test:0.001718 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010066, test:0.002180 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008248, test:0.000629 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009032, test:0.002005 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007331, test:0.000484 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007292, test:0.002645 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007940, test:0.000338 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008052, test:0.000391 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006937, test:0.003281 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006896, test:0.003160 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006990, test:0.000381 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009679, test:0.001194 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008149, test:0.001478 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.020075, test:0.000619 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008628, test:0.000694 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006229, test:0.000392 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007630, test:0.000839 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008924, test:0.004300 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009365, test:0.000406 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010540, test:0.000725 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007570, test:0.000493 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006924, test:0.000714 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006586, test:0.000356 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008254, test:0.000544 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008029, test:0.003571 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007630, test:0.000559 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005287, test:0.000486 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005941, test:0.000633 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005628, test:0.000743 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005890, test:0.000528 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005073, test:0.000463 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005068, test:0.000487 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005754, test:0.000461 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005421, test:0.000457 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005249, test:0.000476 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006054, test:0.000431 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005220, test:0.000480 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004894, test:0.000355 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005539, test:0.000382 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006151, test:0.000387 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005284, test:0.000464 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005312, test:0.000390 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005466, test:0.000354 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004800, test:0.000364 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005501, test:0.000347 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005615, test:0.000525 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005590, test:0.000415 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004721, test:0.000479 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005352, test:0.000381 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005309, test:0.000722 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004788, test:0.000854 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005457, test:0.000395 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005092, test:0.000366 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007091, test:0.000421 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005061, test:0.000304 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005052, test:0.000512 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004670, test:0.000307 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007710, test:0.000325 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005321, test:0.000306 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004884, test:0.000401 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005706, test:0.001264 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005036, test:0.000533 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.012522, test:0.000316 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006718, test:0.000338 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005278, test:0.000327 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005205, test:0.000338 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005911, test:0.000337 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004724, test:0.000299 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004947, test:0.000345 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.008308, test:0.000315 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004721, test:0.000321 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004950, test:0.000320 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005938, test:0.000352 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004610, test:0.000348 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004867, test:0.000362 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004691, test:0.000356 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004976, test:0.000284 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004581, test:0.000345 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004771, test:0.000319 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005264, test:0.000328 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005905, test:0.000308 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004947, test:0.000331 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004891, test:0.000324 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005205, test:0.000351 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005897, test:0.000334 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005151, test:0.000342 | lr:0.000100\n",
      "Mean absolute error:  1.6895697803409582\n",
      "Root mean squared error:  5.805106176668851\n",
      "Epoch[1/100] | loss train:0.053619, test:0.005209 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009788, test:0.001853 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009165, test:0.003838 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013610, test:0.002765 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010348, test:0.000884 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008864, test:0.000429 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009712, test:0.000386 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007982, test:0.006312 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008750, test:0.000906 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007988, test:0.001470 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009463, test:0.001171 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012503, test:0.005518 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/100] | loss train:0.010420, test:0.000407 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007738, test:0.000438 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007183, test:0.000451 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009861, test:0.002169 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009088, test:0.000933 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007752, test:0.001118 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006925, test:0.000641 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007784, test:0.001667 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007824, test:0.000827 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007364, test:0.001087 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007919, test:0.001023 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007065, test:0.002521 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008852, test:0.000479 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007398, test:0.003111 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007981, test:0.001252 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007066, test:0.000492 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007564, test:0.005730 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008957, test:0.001473 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008079, test:0.001578 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006141, test:0.000534 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007403, test:0.000359 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006213, test:0.004266 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007786, test:0.000400 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007779, test:0.000818 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006352, test:0.000330 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006982, test:0.000387 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006226, test:0.000410 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008203, test:0.001859 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006307, test:0.000320 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005147, test:0.000390 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005244, test:0.000325 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005566, test:0.000332 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005315, test:0.000388 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005354, test:0.000296 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009191, test:0.000346 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005251, test:0.000621 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005326, test:0.000438 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005570, test:0.000306 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004900, test:0.000320 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005067, test:0.000362 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005543, test:0.000316 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005346, test:0.000298 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005622, test:0.000384 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005325, test:0.000361 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005221, test:0.000285 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006139, test:0.000335 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006034, test:0.000288 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005696, test:0.000328 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007750, test:0.000293 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006771, test:0.000271 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004768, test:0.000323 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007468, test:0.000401 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005548, test:0.000345 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005214, test:0.000362 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005428, test:0.000442 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005195, test:0.000477 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005431, test:0.000337 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005085, test:0.000326 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004743, test:0.000326 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006531, test:0.000692 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005622, test:0.000330 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005698, test:0.000506 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004773, test:0.000403 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005183, test:0.000472 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004698, test:0.000292 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005208, test:0.000278 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005619, test:0.000312 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005357, test:0.000452 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004985, test:0.000302 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005182, test:0.000269 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.019799, test:0.000282 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005025, test:0.000286 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006358, test:0.000283 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005129, test:0.000275 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.009142, test:0.000299 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005189, test:0.000288 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004741, test:0.000290 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005996, test:0.000311 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004767, test:0.000284 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004979, test:0.000306 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004940, test:0.000293 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005242, test:0.000287 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004526, test:0.000333 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005610, test:0.000273 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.009732, test:0.000279 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005262, test:0.000286 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005692, test:0.000298 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004638, test:0.000280 | lr:0.000100\n",
      "Mean absolute error:  1.6653694029866337\n",
      "Root mean squared error:  5.776182368314169\n",
      "Epoch[1/100] | loss train:0.077024, test:0.006236 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014070, test:0.001888 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011704, test:0.000469 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010584, test:0.000558 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010531, test:0.000547 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009249, test:0.002163 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009879, test:0.001062 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011732, test:0.002604 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010825, test:0.001789 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009897, test:0.000819 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009264, test:0.003150 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010146, test:0.004258 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008470, test:0.000928 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006742, test:0.001266 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009039, test:0.000365 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008449, test:0.001369 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008412, test:0.001128 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008560, test:0.001270 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008014, test:0.000478 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007373, test:0.000388 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008253, test:0.001066 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007234, test:0.000601 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008469, test:0.001713 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008086, test:0.001461 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010891, test:0.003168 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008431, test:0.000397 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007510, test:0.000720 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007835, test:0.003598 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007186, test:0.003159 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008313, test:0.000580 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007888, test:0.000582 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007383, test:0.001720 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007166, test:0.001062 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008359, test:0.001448 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006512, test:0.000508 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.017955, test:0.000496 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008643, test:0.001745 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[38/100] | loss train:0.007501, test:0.000462 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007785, test:0.000773 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007117, test:0.001508 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.023770, test:0.000432 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006185, test:0.000449 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005927, test:0.000428 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006086, test:0.000389 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007258, test:0.000411 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006088, test:0.000451 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006500, test:0.000411 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006555, test:0.000390 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007846, test:0.000490 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006021, test:0.000405 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008546, test:0.000497 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005640, test:0.000443 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005974, test:0.000509 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005605, test:0.000678 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005870, test:0.000486 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006267, test:0.000477 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005506, test:0.000521 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006049, test:0.000401 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005382, test:0.000371 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009900, test:0.000354 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005812, test:0.000340 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005911, test:0.000435 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005314, test:0.000767 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008236, test:0.000352 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005736, test:0.000401 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005774, test:0.000365 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006855, test:0.000373 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005958, test:0.000347 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.009425, test:0.000357 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006192, test:0.000341 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005303, test:0.000325 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005285, test:0.000360 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005604, test:0.000409 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005243, test:0.000360 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005946, test:0.000338 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005581, test:0.000519 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005155, test:0.000508 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005371, test:0.000483 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006513, test:0.000971 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005820, test:0.000340 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006339, test:0.000362 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006606, test:0.000364 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.006543, test:0.000402 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006557, test:0.000371 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005429, test:0.000328 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005644, test:0.000356 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005151, test:0.000400 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005523, test:0.000333 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005448, test:0.000371 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005545, test:0.000360 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005457, test:0.000381 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005797, test:0.000332 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005438, test:0.000343 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005733, test:0.000371 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005812, test:0.000380 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005600, test:0.000348 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.007089, test:0.000337 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006531, test:0.000340 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005412, test:0.000338 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.006466, test:0.000361 | lr:0.000100\n",
      "Mean absolute error:  1.707548043907623\n",
      "Root mean squared error:  5.834720785843681\n",
      "Epoch[1/100] | loss train:0.062144, test:0.014415 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015251, test:0.000456 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010134, test:0.000318 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009730, test:0.000726 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008737, test:0.000868 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009445, test:0.000583 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008912, test:0.003024 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012472, test:0.004946 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009025, test:0.000900 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007407, test:0.000401 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009969, test:0.000897 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011021, test:0.000402 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010701, test:0.002489 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010862, test:0.001320 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008221, test:0.000795 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008285, test:0.000539 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007277, test:0.000570 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007836, test:0.000333 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008526, test:0.000972 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007966, test:0.000696 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008166, test:0.000707 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009082, test:0.006104 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009422, test:0.001839 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008179, test:0.000976 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006974, test:0.000339 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006679, test:0.001837 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006478, test:0.000573 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007547, test:0.000392 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007295, test:0.000624 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007213, test:0.000469 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006417, test:0.000394 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006470, test:0.001535 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007922, test:0.000345 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007131, test:0.001027 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006934, test:0.000391 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007435, test:0.001543 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006498, test:0.000519 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007090, test:0.001241 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007471, test:0.000342 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007413, test:0.000318 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.012534, test:0.000328 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005456, test:0.000386 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006783, test:0.000309 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005074, test:0.000293 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005876, test:0.000318 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005997, test:0.000304 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006177, test:0.000293 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006197, test:0.000314 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004871, test:0.000381 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006023, test:0.000456 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005018, test:0.000330 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005128, test:0.000324 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005419, test:0.000305 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005409, test:0.000382 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005151, test:0.000316 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005078, test:0.000309 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005094, test:0.000344 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006099, test:0.000280 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005219, test:0.000284 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005032, test:0.000346 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005833, test:0.000289 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007825, test:0.000300 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[63/100] | loss train:0.005972, test:0.000346 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004856, test:0.000354 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005366, test:0.000598 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004864, test:0.000357 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005625, test:0.000329 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004646, test:0.000530 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005016, test:0.000356 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005377, test:0.000276 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005450, test:0.000306 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004830, test:0.000305 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005397, test:0.000330 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005187, test:0.000281 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006468, test:0.000315 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006549, test:0.000371 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005542, test:0.000307 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005048, test:0.000430 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005642, test:0.000283 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004960, test:0.000529 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005007, test:0.000274 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005251, test:0.000278 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004947, test:0.000277 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004942, test:0.000280 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004995, test:0.000289 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004713, test:0.000293 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005539, test:0.000262 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005268, test:0.000272 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004595, test:0.000277 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004752, test:0.000288 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005124, test:0.000283 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005782, test:0.000288 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004871, test:0.000284 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004973, test:0.000283 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005948, test:0.000281 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004784, test:0.000283 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005598, test:0.000289 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005018, test:0.000284 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004653, test:0.000270 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005206, test:0.000271 | lr:0.000100\n",
      "Mean absolute error:  1.6338915478125926\n",
      "Root mean squared error:  5.760468124155758\n",
      "Epoch[1/100] | loss train:0.061169, test:0.000884 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013694, test:0.000488 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010589, test:0.000826 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009898, test:0.000312 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008842, test:0.000338 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008642, test:0.003645 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010447, test:0.000491 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008514, test:0.000332 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009349, test:0.012176 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008014, test:0.000672 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008432, test:0.008486 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009662, test:0.000870 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008513, test:0.004926 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008400, test:0.000420 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015629, test:0.001799 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009573, test:0.000366 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007739, test:0.002666 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007607, test:0.000681 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007238, test:0.000375 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009436, test:0.002081 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009945, test:0.000676 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008931, test:0.000667 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007852, test:0.000400 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007903, test:0.001879 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008366, test:0.001884 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007732, test:0.001985 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008268, test:0.000500 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008264, test:0.000720 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006590, test:0.001495 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009248, test:0.000713 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012053, test:0.001019 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007732, test:0.000922 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006482, test:0.001442 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006469, test:0.000357 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008478, test:0.000502 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011735, test:0.003124 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008501, test:0.001379 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008736, test:0.001875 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008381, test:0.001192 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007428, test:0.001231 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006374, test:0.000397 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006655, test:0.000376 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008372, test:0.000392 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005854, test:0.000840 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005044, test:0.000328 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004810, test:0.000334 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005700, test:0.000361 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005329, test:0.000389 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005256, test:0.000382 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007219, test:0.000361 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006410, test:0.000393 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005289, test:0.000397 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006580, test:0.000313 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005181, test:0.000405 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005225, test:0.000575 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005131, test:0.000411 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006542, test:0.000324 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005743, test:0.000343 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007615, test:0.000386 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005203, test:0.000359 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005712, test:0.000355 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005057, test:0.000395 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006467, test:0.000353 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005123, test:0.000330 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005753, test:0.000600 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005213, test:0.000384 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005105, test:0.000354 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004859, test:0.000354 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006130, test:0.000344 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005639, test:0.000312 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004906, test:0.000395 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005479, test:0.000306 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005980, test:0.000304 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005779, test:0.000504 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006092, test:0.000404 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007277, test:0.000343 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005562, test:0.000346 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005591, test:0.000325 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005186, test:0.000571 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005796, test:0.000379 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005542, test:0.000328 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004881, test:0.000300 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005231, test:0.000325 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004947, test:0.000314 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.008838, test:0.000299 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005405, test:0.000338 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004859, test:0.000372 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[88/100] | loss train:0.004988, test:0.000299 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005123, test:0.000309 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006065, test:0.000320 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005441, test:0.000322 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004521, test:0.000345 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004399, test:0.000319 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004553, test:0.000348 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005416, test:0.000321 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005652, test:0.000338 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005441, test:0.000300 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004966, test:0.000339 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004889, test:0.000325 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005036, test:0.000309 | lr:0.000100\n",
      "Mean absolute error:  1.7047101001827654\n",
      "Root mean squared error:  5.802276440232775\n",
      "Epoch[1/100] | loss train:0.072574, test:0.000508 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011710, test:0.003568 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010202, test:0.001975 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009558, test:0.002411 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007933, test:0.005916 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008688, test:0.000307 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007394, test:0.000356 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007948, test:0.000467 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012343, test:0.002669 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009001, test:0.010667 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008862, test:0.000347 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008959, test:0.000553 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010087, test:0.001112 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008076, test:0.000543 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015334, test:0.002513 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009800, test:0.001782 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009275, test:0.004253 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007595, test:0.000395 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008481, test:0.004861 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008003, test:0.000749 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006944, test:0.000834 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010364, test:0.005412 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007531, test:0.000380 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007344, test:0.000466 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006804, test:0.002910 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007795, test:0.002498 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007324, test:0.001068 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008133, test:0.000385 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007328, test:0.003345 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012983, test:0.000402 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008279, test:0.001755 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006952, test:0.000387 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006244, test:0.000370 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009045, test:0.000757 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008061, test:0.001180 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007644, test:0.002208 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008500, test:0.000392 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006912, test:0.004092 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008567, test:0.000677 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007193, test:0.001060 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005717, test:0.000541 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005981, test:0.000432 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005186, test:0.000540 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005393, test:0.000382 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006828, test:0.000411 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005173, test:0.000395 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005436, test:0.000358 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005486, test:0.000395 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005095, test:0.000332 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005617, test:0.000375 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005327, test:0.000297 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005279, test:0.000349 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005995, test:0.000717 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007318, test:0.000373 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005749, test:0.000624 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005195, test:0.000337 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005331, test:0.000753 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006123, test:0.000487 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005646, test:0.000451 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005292, test:0.000335 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005179, test:0.000374 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005382, test:0.000307 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005520, test:0.000341 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006610, test:0.000408 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005611, test:0.000310 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008618, test:0.000308 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005740, test:0.000319 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005536, test:0.000318 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004784, test:0.000499 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005153, test:0.000421 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005335, test:0.000346 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005629, test:0.000743 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005071, test:0.000654 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006009, test:0.000419 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005950, test:0.000581 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005372, test:0.000390 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005424, test:0.000318 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005000, test:0.000504 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004972, test:0.000361 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005525, test:0.000317 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005234, test:0.000325 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005586, test:0.000326 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005085, test:0.000286 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005428, test:0.000327 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005088, test:0.000340 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004973, test:0.000338 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005282, test:0.000358 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005104, test:0.000302 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005020, test:0.000371 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004771, test:0.000317 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004697, test:0.000342 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004870, test:0.000309 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005111, test:0.000355 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005935, test:0.000279 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005443, test:0.000312 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005333, test:0.000329 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005099, test:0.000364 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005104, test:0.000324 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004964, test:0.000313 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.006105, test:0.000318 | lr:0.000100\n",
      "Mean absolute error:  1.7322776090691199\n",
      "Root mean squared error:  5.7918618601199565\n",
      "Epoch[1/100] | loss train:0.052306, test:0.001031 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011762, test:0.001426 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010467, test:0.005161 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012216, test:0.008188 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013443, test:0.000299 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009467, test:0.002135 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008897, test:0.002711 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007930, test:0.000294 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011193, test:0.001919 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009676, test:0.007148 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008774, test:0.002577 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/100] | loss train:0.012883, test:0.002209 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009574, test:0.001640 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007806, test:0.002067 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007693, test:0.001104 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011435, test:0.000587 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008495, test:0.000328 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007875, test:0.001882 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007507, test:0.000331 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011940, test:0.010254 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009528, test:0.000489 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007859, test:0.000486 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007954, test:0.000590 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008089, test:0.000377 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007137, test:0.000973 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008043, test:0.000337 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007378, test:0.000439 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008316, test:0.001072 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006614, test:0.001197 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007294, test:0.000593 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007336, test:0.003024 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006306, test:0.000945 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007301, test:0.001018 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007102, test:0.001136 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007067, test:0.000370 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011437, test:0.001733 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007193, test:0.000856 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007037, test:0.000794 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007602, test:0.000444 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007133, test:0.000387 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007743, test:0.000375 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005525, test:0.000376 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005302, test:0.000385 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007781, test:0.000381 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005292, test:0.000392 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005811, test:0.001035 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005523, test:0.000363 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005628, test:0.000345 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005149, test:0.000839 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005055, test:0.000390 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005645, test:0.000365 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005058, test:0.000376 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005375, test:0.000331 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005310, test:0.000314 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007633, test:0.000366 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005246, test:0.000357 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005345, test:0.000449 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005103, test:0.000424 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005147, test:0.000474 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004931, test:0.000358 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005215, test:0.000311 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008000, test:0.000317 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005210, test:0.000419 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005717, test:0.000351 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005422, test:0.000515 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005112, test:0.000396 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005373, test:0.000374 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005334, test:0.000323 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006209, test:0.001314 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006101, test:0.000367 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005717, test:0.000383 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005098, test:0.000397 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007147, test:0.000333 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005326, test:0.000660 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006305, test:0.000356 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005023, test:0.000418 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006149, test:0.000447 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005233, test:0.000645 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006177, test:0.000362 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005483, test:0.000319 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004991, test:0.000361 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006839, test:0.000332 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005357, test:0.000318 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005230, test:0.000360 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005019, test:0.000340 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005650, test:0.000339 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004985, test:0.000342 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005118, test:0.000367 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005948, test:0.000325 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005375, test:0.000353 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005630, test:0.000340 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005260, test:0.000344 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005215, test:0.000329 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005483, test:0.000362 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005144, test:0.000311 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007913, test:0.000357 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005293, test:0.000377 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005411, test:0.000318 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005068, test:0.000331 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005351, test:0.000382 | lr:0.000100\n",
      "Mean absolute error:  1.7871204437878334\n",
      "Root mean squared error:  5.864309720490605\n",
      "Epoch[1/100] | loss train:0.050973, test:0.000386 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011427, test:0.003958 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009990, test:0.003133 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010532, test:0.014785 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008781, test:0.003574 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010030, test:0.001849 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006556, test:0.003967 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008605, test:0.002491 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010511, test:0.002899 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008722, test:0.003110 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007940, test:0.000603 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007896, test:0.000365 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010102, test:0.002595 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014062, test:0.002027 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007542, test:0.002248 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007304, test:0.000782 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008419, test:0.000439 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007702, test:0.000474 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008289, test:0.001539 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010678, test:0.005310 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007451, test:0.000488 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007886, test:0.001364 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006349, test:0.000852 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008953, test:0.002418 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007399, test:0.000419 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006773, test:0.000344 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006122, test:0.000642 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007333, test:0.000845 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006610, test:0.001053 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007334, test:0.000448 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007104, test:0.001459 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006920, test:0.001459 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006600, test:0.000379 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006637, test:0.000558 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006776, test:0.000707 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006899, test:0.000378 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/100] | loss train:0.006528, test:0.000424 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006651, test:0.000675 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006209, test:0.000580 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007001, test:0.003413 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006838, test:0.000406 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005387, test:0.000397 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005405, test:0.000380 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005441, test:0.000376 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005485, test:0.000404 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005209, test:0.000403 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004992, test:0.000360 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005666, test:0.000434 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005117, test:0.000653 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.010582, test:0.000415 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005592, test:0.000377 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005054, test:0.000330 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004620, test:0.000359 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006803, test:0.000325 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004881, test:0.000339 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004772, test:0.000344 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004983, test:0.000412 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005434, test:0.000357 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005344, test:0.000343 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005081, test:0.000493 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004900, test:0.000437 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005110, test:0.000488 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005234, test:0.000369 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005164, test:0.000351 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004988, test:0.000366 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004939, test:0.000432 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004978, test:0.000497 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005977, test:0.000418 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005274, test:0.000448 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005072, test:0.000336 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005399, test:0.000373 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005293, test:0.000364 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005417, test:0.000340 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004903, test:0.000399 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005017, test:0.000513 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004951, test:0.000323 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004795, test:0.000550 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005332, test:0.000456 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005034, test:0.000397 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005124, test:0.000335 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005017, test:0.000371 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004906, test:0.000314 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004452, test:0.000301 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004850, test:0.000319 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005154, test:0.000309 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004917, test:0.000339 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005049, test:0.000339 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005200, test:0.000336 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004947, test:0.000333 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004770, test:0.000348 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004915, test:0.000289 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005067, test:0.000340 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005356, test:0.000335 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004796, test:0.000305 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004651, test:0.000318 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004737, test:0.000313 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004702, test:0.000318 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006098, test:0.000305 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005219, test:0.000352 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005213, test:0.000335 | lr:0.000100\n",
      "Mean absolute error:  1.6863004805649284\n",
      "Root mean squared error:  5.797720736131146\n",
      "Epoch[1/100] | loss train:0.043981, test:0.000727 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014761, test:0.002900 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009650, test:0.004318 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008273, test:0.000669 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012698, test:0.000861 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009778, test:0.001777 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010202, test:0.001019 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008701, test:0.000559 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008034, test:0.001140 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007287, test:0.003391 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008454, test:0.002872 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009202, test:0.001203 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007935, test:0.002676 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007822, test:0.004669 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007651, test:0.000654 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007609, test:0.009207 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007989, test:0.000413 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008304, test:0.000994 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007690, test:0.000433 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008205, test:0.000831 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007827, test:0.000436 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008128, test:0.003585 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007375, test:0.000860 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007382, test:0.001096 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007151, test:0.000424 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008004, test:0.002668 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007703, test:0.000449 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007238, test:0.000573 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006136, test:0.000421 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006862, test:0.001358 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006285, test:0.002951 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006554, test:0.000415 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006607, test:0.000363 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007568, test:0.001263 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010184, test:0.001513 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010881, test:0.000784 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007288, test:0.000932 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006676, test:0.000440 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006457, test:0.000455 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008319, test:0.004624 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006096, test:0.000521 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005211, test:0.000689 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006099, test:0.000449 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005340, test:0.000509 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005500, test:0.000428 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005447, test:0.000368 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006469, test:0.000381 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.014130, test:0.000389 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005014, test:0.000342 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004736, test:0.000369 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005818, test:0.000400 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008975, test:0.001283 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005655, test:0.000443 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005323, test:0.000391 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005481, test:0.000353 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005087, test:0.000356 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006819, test:0.000350 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004741, test:0.000466 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007537, test:0.000326 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005140, test:0.000449 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005434, test:0.000317 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[62/100] | loss train:0.005452, test:0.000357 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005126, test:0.000414 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005460, test:0.000360 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007973, test:0.000359 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006882, test:0.000319 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005023, test:0.000341 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007275, test:0.000330 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005590, test:0.000329 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005166, test:0.000344 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005738, test:0.000427 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005324, test:0.000345 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005693, test:0.000632 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005273, test:0.000486 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004967, test:0.000364 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005177, test:0.000420 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005245, test:0.000361 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006698, test:0.000451 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005205, test:0.000797 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005395, test:0.000409 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004677, test:0.000324 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004884, test:0.000355 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.006478, test:0.000340 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004474, test:0.000347 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005033, test:0.000325 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005108, test:0.000328 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005083, test:0.000329 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004877, test:0.000366 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005061, test:0.000335 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.007542, test:0.000345 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004896, test:0.000342 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004630, test:0.000328 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004629, test:0.000328 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005407, test:0.000325 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005080, test:0.000389 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004554, test:0.000339 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004851, test:0.000346 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004595, test:0.000324 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.007536, test:0.000326 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004850, test:0.000314 | lr:0.000100\n",
      "Mean absolute error:  1.7111519766878112\n",
      "Root mean squared error:  5.803672108417846\n",
      "Epoch[1/100] | loss train:0.063737, test:0.000523 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012982, test:0.003346 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012081, test:0.000303 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009975, test:0.005450 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009089, test:0.000641 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009236, test:0.001881 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010143, test:0.001047 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008105, test:0.001540 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009387, test:0.001713 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009990, test:0.000664 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009297, test:0.002570 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008204, test:0.001889 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008399, test:0.008109 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009979, test:0.004567 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007827, test:0.001245 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007569, test:0.000432 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008503, test:0.011180 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007250, test:0.002101 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006569, test:0.000819 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007761, test:0.001158 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007089, test:0.000456 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007244, test:0.000767 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006745, test:0.001322 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006680, test:0.001028 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008127, test:0.000551 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006691, test:0.000444 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006717, test:0.000660 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008133, test:0.000584 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006735, test:0.000693 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007545, test:0.000320 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007574, test:0.001941 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006799, test:0.000952 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006601, test:0.001119 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006461, test:0.001942 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008070, test:0.000301 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006975, test:0.000293 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008198, test:0.000322 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006808, test:0.000536 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007084, test:0.000377 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011521, test:0.001429 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006047, test:0.000280 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006010, test:0.000321 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005793, test:0.000290 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005269, test:0.000299 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004927, test:0.000539 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005226, test:0.000339 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005206, test:0.000499 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005109, test:0.000337 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004914, test:0.000324 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005811, test:0.000305 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005145, test:0.000300 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008367, test:0.000299 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005829, test:0.000298 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006913, test:0.000296 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004515, test:0.000323 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005687, test:0.000307 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007005, test:0.000301 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005355, test:0.000294 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007652, test:0.000339 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005274, test:0.000284 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004832, test:0.000333 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005728, test:0.000534 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004868, test:0.000743 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004919, test:0.000479 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005395, test:0.000309 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005432, test:0.000367 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006244, test:0.000287 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004738, test:0.000339 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004915, test:0.000415 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004907, test:0.000319 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005039, test:0.000293 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005360, test:0.000297 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005079, test:0.000333 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005034, test:0.000337 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004851, test:0.000389 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004789, test:0.000327 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005955, test:0.000312 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004610, test:0.000283 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004590, test:0.000321 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004843, test:0.000339 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004904, test:0.000291 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004798, test:0.000279 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005157, test:0.000292 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005013, test:0.000287 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005967, test:0.000298 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004845, test:0.000293 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[87/100] | loss train:0.006433, test:0.000295 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004542, test:0.000304 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.006188, test:0.000293 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004929, test:0.000309 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004912, test:0.000287 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004845, test:0.000296 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004486, test:0.000303 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005345, test:0.000303 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004739, test:0.000271 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004897, test:0.000285 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004599, test:0.000348 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004921, test:0.000279 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004511, test:0.000271 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004867, test:0.000294 | lr:0.000100\n",
      "Mean absolute error:  1.629169977021959\n",
      "Root mean squared error:  5.758665818509584\n",
      "Epoch[1/100] | loss train:0.046385, test:0.002850 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011155, test:0.003848 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011540, test:0.003301 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010125, test:0.001226 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009629, test:0.000339 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008785, test:0.012914 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009678, test:0.002703 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010216, test:0.000336 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008177, test:0.000877 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008775, test:0.005880 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009586, test:0.002010 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009668, test:0.002123 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008569, test:0.000368 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008586, test:0.000612 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009376, test:0.001704 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008725, test:0.002225 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006823, test:0.000540 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008277, test:0.000998 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007946, test:0.009542 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007872, test:0.000700 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007473, test:0.000793 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007506, test:0.000390 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008297, test:0.001029 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007124, test:0.000438 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007735, test:0.000496 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008524, test:0.001214 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007426, test:0.000425 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007652, test:0.000944 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008435, test:0.000600 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007865, test:0.001247 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007241, test:0.004483 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008071, test:0.000957 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008927, test:0.001843 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008720, test:0.000422 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007833, test:0.001735 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007377, test:0.000459 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008014, test:0.000435 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007158, test:0.000937 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006459, test:0.000607 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008333, test:0.002719 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007657, test:0.000370 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005414, test:0.000358 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005857, test:0.000425 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005087, test:0.000406 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005413, test:0.000476 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005756, test:0.000361 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004990, test:0.000387 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004924, test:0.000335 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005125, test:0.000460 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005558, test:0.000422 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005358, test:0.000364 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006310, test:0.000319 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005267, test:0.000329 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005130, test:0.000320 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005790, test:0.000381 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005502, test:0.000571 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005652, test:0.000380 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004824, test:0.000576 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004786, test:0.000315 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006089, test:0.000351 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005374, test:0.000323 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005867, test:0.000353 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004848, test:0.000339 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005499, test:0.000562 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005343, test:0.000537 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007361, test:0.000343 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005380, test:0.000348 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004968, test:0.000383 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005687, test:0.000481 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005179, test:0.000355 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004833, test:0.000324 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004530, test:0.000375 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005043, test:0.000380 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005560, test:0.000327 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004874, test:0.000359 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005324, test:0.000354 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005088, test:0.000338 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005794, test:0.000442 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006812, test:0.000402 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005418, test:0.000627 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006437, test:0.000334 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005809, test:0.000319 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004578, test:0.000306 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004702, test:0.000324 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004796, test:0.000377 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004863, test:0.000355 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004771, test:0.000360 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004606, test:0.000303 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004780, test:0.000299 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005244, test:0.000325 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004856, test:0.000339 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004748, test:0.000337 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004820, test:0.000307 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004843, test:0.000326 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004701, test:0.000328 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004784, test:0.000322 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004642, test:0.000363 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.012853, test:0.000328 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005338, test:0.000321 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005045, test:0.000334 | lr:0.000100\n",
      "Mean absolute error:  1.6659108304439183\n",
      "Root mean squared error:  5.786893969866083\n",
      "Epoch[1/100] | loss train:0.051489, test:0.000817 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012054, test:0.000361 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010812, test:0.000331 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013009, test:0.000332 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008711, test:0.004670 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012496, test:0.000581 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008626, test:0.001492 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008117, test:0.000564 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007956, test:0.000381 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007517, test:0.001178 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/100] | loss train:0.008923, test:0.010125 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008186, test:0.000488 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009064, test:0.000502 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007719, test:0.000325 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.018147, test:0.005902 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008857, test:0.000499 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008719, test:0.002661 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007482, test:0.004008 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008737, test:0.002683 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008078, test:0.000402 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007670, test:0.000596 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008875, test:0.000477 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007171, test:0.000359 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007537, test:0.000471 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007790, test:0.000981 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007886, test:0.000377 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008136, test:0.000342 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007196, test:0.000867 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006883, test:0.000511 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007773, test:0.000710 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006954, test:0.000323 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006353, test:0.000473 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007324, test:0.002442 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006603, test:0.000666 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007722, test:0.002088 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007874, test:0.000691 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008136, test:0.001156 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008591, test:0.000352 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007328, test:0.002245 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007093, test:0.000660 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005766, test:0.000380 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007262, test:0.000369 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005671, test:0.000376 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005053, test:0.000331 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005642, test:0.000311 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006194, test:0.000433 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004891, test:0.000328 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005351, test:0.000317 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006988, test:0.000331 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005233, test:0.000296 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005942, test:0.000357 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005934, test:0.000308 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005142, test:0.000492 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005416, test:0.000425 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005961, test:0.000307 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005765, test:0.000297 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004742, test:0.000354 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005043, test:0.000361 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005397, test:0.000322 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005703, test:0.000588 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004944, test:0.000421 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005432, test:0.000291 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005137, test:0.000336 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005425, test:0.000304 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005816, test:0.000313 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005046, test:0.000318 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004945, test:0.000353 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005100, test:0.000440 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005572, test:0.000367 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005325, test:0.000436 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005438, test:0.000627 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005295, test:0.000293 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007810, test:0.000357 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005947, test:0.000300 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005397, test:0.000282 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004922, test:0.000372 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005123, test:0.000288 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005343, test:0.000310 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005305, test:0.000602 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005881, test:0.000394 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005211, test:0.000328 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004821, test:0.000328 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005523, test:0.000305 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.007673, test:0.000284 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005224, test:0.000287 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005223, test:0.000300 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004574, test:0.000294 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004977, test:0.000383 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004991, test:0.000314 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004833, test:0.000327 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005173, test:0.000313 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005074, test:0.000302 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005177, test:0.000318 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004757, test:0.000286 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005697, test:0.000294 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004936, test:0.000303 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004667, test:0.000302 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004797, test:0.000297 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005481, test:0.000299 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005363, test:0.000341 | lr:0.000100\n",
      "Mean absolute error:  1.799200667000606\n",
      "Root mean squared error:  5.835165201031396\n",
      "Epoch[1/100] | loss train:0.061632, test:0.000360 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010899, test:0.002058 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011946, test:0.002557 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.042560, test:0.002450 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013437, test:0.000673 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009383, test:0.000571 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013780, test:0.003732 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010895, test:0.001188 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008229, test:0.003341 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010270, test:0.000794 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012373, test:0.000408 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008763, test:0.000451 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007878, test:0.000817 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014289, test:0.003301 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009289, test:0.000787 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008063, test:0.000447 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008042, test:0.003454 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008130, test:0.000441 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007427, test:0.002281 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007365, test:0.002289 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009810, test:0.005218 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009147, test:0.001029 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006762, test:0.001268 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007249, test:0.000491 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009164, test:0.005597 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008696, test:0.001686 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006880, test:0.001309 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007494, test:0.001967 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007590, test:0.004031 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008329, test:0.000368 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012339, test:0.001042 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007908, test:0.000572 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008843, test:0.001489 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008748, test:0.000921 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007789, test:0.000778 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[36/100] | loss train:0.008193, test:0.001019 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007288, test:0.001304 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008768, test:0.001374 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006615, test:0.002434 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007410, test:0.002842 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009148, test:0.000374 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006129, test:0.000456 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005679, test:0.000440 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005081, test:0.000402 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005056, test:0.000405 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005025, test:0.000455 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004687, test:0.000463 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005788, test:0.000358 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005283, test:0.000352 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005324, test:0.000339 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005444, test:0.000437 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005303, test:0.000555 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005532, test:0.000455 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004896, test:0.000398 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005162, test:0.000415 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004921, test:0.000401 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005626, test:0.000428 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004886, test:0.000394 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004973, test:0.000386 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005315, test:0.000408 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004888, test:0.000323 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005796, test:0.000324 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005851, test:0.000422 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005452, test:0.000350 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005086, test:0.000341 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005482, test:0.000520 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004733, test:0.000347 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005367, test:0.000458 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004972, test:0.000398 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004822, test:0.000382 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005084, test:0.000342 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004811, test:0.000344 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005485, test:0.000343 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004645, test:0.000420 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.012433, test:0.000456 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004897, test:0.000306 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004893, test:0.000324 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005576, test:0.000346 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004917, test:0.000304 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006089, test:0.000342 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004912, test:0.000314 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005944, test:0.000339 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005142, test:0.000284 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005366, test:0.000347 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005089, test:0.000338 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005299, test:0.000360 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004705, test:0.000299 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005321, test:0.000336 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005064, test:0.000335 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004738, test:0.000328 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005073, test:0.000347 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005863, test:0.000324 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004995, test:0.000316 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005227, test:0.000338 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004878, test:0.000324 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004913, test:0.000293 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006268, test:0.000338 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005389, test:0.000326 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004712, test:0.000346 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005240, test:0.000333 | lr:0.000100\n",
      "Mean absolute error:  1.694769696063357\n",
      "Root mean squared error:  5.799997030819708\n",
      "Epoch[1/100] | loss train:0.051741, test:0.000578 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010607, test:0.003395 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013287, test:0.000342 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010652, test:0.001163 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011031, test:0.005025 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010389, test:0.000526 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008212, test:0.001370 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009132, test:0.001935 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007989, test:0.001735 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007095, test:0.001066 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012073, test:0.000446 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.016752, test:0.001368 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010959, test:0.000566 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008156, test:0.000332 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008991, test:0.001856 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008969, test:0.000341 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008579, test:0.003135 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009700, test:0.000458 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007503, test:0.001000 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008505, test:0.003509 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010083, test:0.001086 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007796, test:0.000406 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006942, test:0.000515 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007238, test:0.000867 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007941, test:0.001104 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007998, test:0.002558 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007264, test:0.002120 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008730, test:0.002448 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008231, test:0.000364 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008045, test:0.001019 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006846, test:0.002568 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006118, test:0.000933 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007694, test:0.001857 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006643, test:0.002058 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008214, test:0.000447 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007705, test:0.000374 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008263, test:0.001422 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006723, test:0.000525 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007183, test:0.000487 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006409, test:0.000703 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005741, test:0.000450 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005778, test:0.000388 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005534, test:0.000367 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005463, test:0.000410 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005190, test:0.000393 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008262, test:0.000409 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004824, test:0.000370 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004934, test:0.000412 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004793, test:0.000379 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005924, test:0.000525 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004969, test:0.000353 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005706, test:0.000386 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005263, test:0.000576 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004823, test:0.000386 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005840, test:0.000363 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005651, test:0.000361 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005081, test:0.000345 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004660, test:0.000311 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005441, test:0.000794 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005664, test:0.000448 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[61/100] | loss train:0.005033, test:0.000572 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005088, test:0.000731 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006392, test:0.000524 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005808, test:0.000365 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004881, test:0.000522 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005875, test:0.000335 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005242, test:0.000393 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005027, test:0.000355 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004730, test:0.000416 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005453, test:0.000346 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005205, test:0.000391 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005204, test:0.000528 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005017, test:0.000313 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005015, test:0.000313 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005186, test:0.000393 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006776, test:0.000475 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005307, test:0.000413 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005413, test:0.000301 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004851, test:0.000299 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005318, test:0.000353 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005620, test:0.000336 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.009791, test:0.000319 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004966, test:0.000331 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005138, test:0.000314 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005956, test:0.000306 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004923, test:0.000347 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004916, test:0.000368 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005574, test:0.000339 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004912, test:0.000349 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006104, test:0.000377 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005372, test:0.000340 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005067, test:0.000315 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005279, test:0.000316 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004812, test:0.000338 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004757, test:0.000318 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005003, test:0.000324 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005195, test:0.000301 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005590, test:0.000346 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004496, test:0.000348 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004943, test:0.000325 | lr:0.000100\n",
      "Mean absolute error:  1.70050568955886\n",
      "Root mean squared error:  5.792411442820391\n",
      "Epoch[1/100] | loss train:0.057195, test:0.000518 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012176, test:0.003164 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010353, test:0.000960 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009787, test:0.015970 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011630, test:0.000660 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008982, test:0.001029 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009833, test:0.000422 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010317, test:0.001010 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009166, test:0.001071 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007812, test:0.001277 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008087, test:0.001376 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007558, test:0.002616 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007444, test:0.001087 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008543, test:0.000400 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009143, test:0.000472 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007254, test:0.000396 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007807, test:0.001939 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006861, test:0.003949 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007757, test:0.001880 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007037, test:0.001356 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009862, test:0.001327 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014929, test:0.000611 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008788, test:0.000704 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007956, test:0.000488 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006798, test:0.002985 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007584, test:0.000974 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008395, test:0.000526 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007627, test:0.001257 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006833, test:0.000351 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008567, test:0.000481 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009116, test:0.001559 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008023, test:0.000509 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009048, test:0.002757 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008136, test:0.004358 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006826, test:0.000428 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006812, test:0.000742 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007482, test:0.000462 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007364, test:0.001292 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007632, test:0.000380 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006934, test:0.000610 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006066, test:0.000390 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005194, test:0.000460 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005510, test:0.000335 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006818, test:0.000374 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006257, test:0.000347 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005214, test:0.000415 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005180, test:0.000330 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005022, test:0.000322 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006275, test:0.000475 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005148, test:0.000456 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004918, test:0.000356 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005380, test:0.000355 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005137, test:0.000438 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005351, test:0.000376 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005232, test:0.000322 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005340, test:0.000344 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005589, test:0.000374 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005244, test:0.000372 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005772, test:0.000333 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005123, test:0.000475 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005396, test:0.000314 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005516, test:0.000323 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005438, test:0.000339 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006223, test:0.000367 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005447, test:0.000288 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005113, test:0.000310 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005363, test:0.000408 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007442, test:0.000322 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005211, test:0.000322 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007425, test:0.000364 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004830, test:0.000321 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005149, test:0.000457 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005712, test:0.000466 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005290, test:0.000440 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005177, test:0.000417 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005566, test:0.000365 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005466, test:0.000359 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004928, test:0.000439 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005529, test:0.000352 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005528, test:0.000364 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006467, test:0.000338 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005406, test:0.000307 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005482, test:0.000320 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005311, test:0.000322 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005196, test:0.000312 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[86/100] | loss train:0.004923, test:0.000296 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005192, test:0.000286 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005252, test:0.000322 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004847, test:0.000356 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004872, test:0.000312 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005034, test:0.000299 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005063, test:0.000306 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.006463, test:0.000317 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005085, test:0.000299 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.007010, test:0.000308 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.006002, test:0.000300 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.012327, test:0.000303 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.006496, test:0.000306 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.007167, test:0.000307 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004925, test:0.000313 | lr:0.000100\n",
      "Mean absolute error:  1.660254390959466\n",
      "Root mean squared error:  5.77362477223558\n",
      "Epoch[1/100] | loss train:0.060450, test:0.002017 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013370, test:0.002123 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011586, test:0.000443 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008632, test:0.001118 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016374, test:0.005744 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013134, test:0.000794 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007753, test:0.001119 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008357, test:0.001936 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009935, test:0.002151 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007976, test:0.000424 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.020666, test:0.006773 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009743, test:0.000501 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007942, test:0.002649 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008776, test:0.001185 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006293, test:0.001766 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008331, test:0.003837 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012312, test:0.005522 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010580, test:0.000553 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008235, test:0.000396 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007466, test:0.000760 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007874, test:0.001009 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007460, test:0.001322 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008539, test:0.002019 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008380, test:0.000410 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006746, test:0.000689 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009648, test:0.000427 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006911, test:0.000500 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006930, test:0.002773 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006817, test:0.000304 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007133, test:0.000394 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007590, test:0.001062 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006797, test:0.000899 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008319, test:0.000618 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006967, test:0.000896 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009179, test:0.000311 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.014465, test:0.002222 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009042, test:0.002898 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006213, test:0.000325 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006494, test:0.000296 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006437, test:0.000431 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005435, test:0.000423 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005321, test:0.000484 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005150, test:0.000287 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005778, test:0.000339 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005758, test:0.000505 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005258, test:0.000335 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.011034, test:0.000301 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004964, test:0.000300 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005826, test:0.000286 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005165, test:0.000265 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005092, test:0.000333 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004829, test:0.000289 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004846, test:0.000290 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004929, test:0.000282 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005106, test:0.000401 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005412, test:0.000294 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004957, test:0.000285 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005146, test:0.000339 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005030, test:0.000346 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004840, test:0.000322 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005036, test:0.000272 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005263, test:0.000323 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005148, test:0.000461 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005551, test:0.000285 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005111, test:0.000281 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004850, test:0.000305 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006034, test:0.000315 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004788, test:0.000301 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005172, test:0.000323 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005083, test:0.000413 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004820, test:0.000372 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004875, test:0.000383 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005336, test:0.000619 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005318, test:0.000281 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005354, test:0.000402 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005312, test:0.000497 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006515, test:0.000279 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004908, test:0.000400 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005585, test:0.000279 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005495, test:0.000321 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005439, test:0.000311 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004690, test:0.000277 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005039, test:0.000277 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004927, test:0.000300 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004842, test:0.000302 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005190, test:0.000279 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005104, test:0.000273 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004709, test:0.000285 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004679, test:0.000297 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004831, test:0.000307 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.007332, test:0.000276 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005069, test:0.000280 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005077, test:0.000290 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004664, test:0.000306 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005086, test:0.000293 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004779, test:0.000282 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004775, test:0.000297 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005278, test:0.000379 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005427, test:0.000318 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004889, test:0.000277 | lr:0.000100\n",
      "Mean absolute error:  1.6358361099122443\n",
      "Root mean squared error:  5.758693847537745\n",
      "Epoch[1/100] | loss train:0.053509, test:0.000491 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011407, test:0.000363 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011375, test:0.000572 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009691, test:0.001002 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010485, test:0.002970 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010487, test:0.000920 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008164, test:0.001604 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007787, test:0.000734 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007386, test:0.001350 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10/100] | loss train:0.008670, test:0.011659 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008787, test:0.007038 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008297, test:0.003308 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008268, test:0.000476 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007285, test:0.001853 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006593, test:0.000681 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007167, test:0.000349 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008518, test:0.000784 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008414, test:0.000489 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008167, test:0.002427 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007673, test:0.000514 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007263, test:0.003286 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010735, test:0.000523 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007141, test:0.000361 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007241, test:0.001234 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009699, test:0.000434 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008734, test:0.000476 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006371, test:0.000793 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007883, test:0.001718 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009293, test:0.000500 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006961, test:0.000384 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007560, test:0.001202 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011731, test:0.001213 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008221, test:0.000388 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006729, test:0.002460 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007058, test:0.001983 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008263, test:0.000341 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007389, test:0.001026 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006247, test:0.000382 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006495, test:0.000625 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006348, test:0.000372 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006219, test:0.000361 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005267, test:0.000453 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006098, test:0.000544 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005273, test:0.000328 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006207, test:0.000442 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005038, test:0.000351 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004925, test:0.000304 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005238, test:0.000276 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005041, test:0.000310 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005181, test:0.000287 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005800, test:0.000297 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007341, test:0.000264 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006024, test:0.000328 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004942, test:0.000353 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005132, test:0.000291 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005667, test:0.000275 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006069, test:0.000280 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007775, test:0.000738 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005893, test:0.000313 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009802, test:0.000298 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005844, test:0.000405 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004841, test:0.000291 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004821, test:0.000355 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.009315, test:0.000290 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005892, test:0.000357 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006215, test:0.000365 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005300, test:0.000289 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004861, test:0.000318 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005569, test:0.000260 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004844, test:0.000276 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004612, test:0.000342 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004995, test:0.000358 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004774, test:0.000303 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005012, test:0.000523 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006470, test:0.000682 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005569, test:0.000486 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004583, test:0.000296 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005272, test:0.000358 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005425, test:0.000319 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005220, test:0.000299 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004683, test:0.000294 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004801, test:0.000298 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004637, test:0.000284 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005968, test:0.000272 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005539, test:0.000304 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004945, test:0.000277 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004758, test:0.000292 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.006185, test:0.000292 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005085, test:0.000281 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005237, test:0.000270 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004879, test:0.000292 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005228, test:0.000299 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004655, test:0.000329 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005146, test:0.000272 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004786, test:0.000272 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004806, test:0.000357 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.007223, test:0.000271 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004815, test:0.000279 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005505, test:0.000270 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004921, test:0.000313 | lr:0.000100\n",
      "Mean absolute error:  1.6733065679569488\n",
      "Root mean squared error:  5.763606831553237\n",
      "Epoch[1/100] | loss train:0.062458, test:0.000340 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012649, test:0.000982 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009758, test:0.002742 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008490, test:0.001227 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011860, test:0.000521 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008783, test:0.000376 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008286, test:0.003469 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010253, test:0.003452 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007991, test:0.005046 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008395, test:0.002634 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007998, test:0.001750 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007111, test:0.000821 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007640, test:0.001817 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007406, test:0.002940 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007754, test:0.002042 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009882, test:0.009429 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008544, test:0.000733 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009666, test:0.000405 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008744, test:0.000439 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006638, test:0.000512 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007132, test:0.001352 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007836, test:0.000569 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007831, test:0.002295 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008760, test:0.000665 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010936, test:0.002959 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009198, test:0.000707 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009523, test:0.002122 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008811, test:0.001239 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006393, test:0.002007 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007927, test:0.000495 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007811, test:0.000800 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008068, test:0.000598 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008961, test:0.000398 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007391, test:0.002662 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/100] | loss train:0.006941, test:0.000519 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006527, test:0.000474 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006855, test:0.001373 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006380, test:0.000557 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006939, test:0.000437 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006525, test:0.001646 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006310, test:0.000435 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005396, test:0.000444 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006218, test:0.000454 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005141, test:0.000435 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005167, test:0.000360 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006169, test:0.000360 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005082, test:0.000381 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005041, test:0.000415 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004931, test:0.000371 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005155, test:0.000316 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005407, test:0.000518 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005046, test:0.000399 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005367, test:0.000396 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005202, test:0.000549 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005088, test:0.000531 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005179, test:0.000350 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007194, test:0.000339 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005013, test:0.000369 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005235, test:0.000423 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005241, test:0.000364 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004937, test:0.000322 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006701, test:0.000308 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005630, test:0.000327 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005252, test:0.000511 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005058, test:0.000318 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004663, test:0.001001 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005685, test:0.000396 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004937, test:0.000318 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008024, test:0.000329 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004609, test:0.000340 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005369, test:0.000296 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005105, test:0.000383 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005611, test:0.000339 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005041, test:0.000326 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006143, test:0.000528 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007157, test:0.000640 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004976, test:0.000357 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007955, test:0.000317 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004968, test:0.000618 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005143, test:0.000333 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005090, test:0.000309 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004579, test:0.000303 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005146, test:0.000314 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005292, test:0.000316 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004747, test:0.000303 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004858, test:0.000312 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004732, test:0.000332 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004948, test:0.000318 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005273, test:0.000309 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004899, test:0.000375 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004617, test:0.000300 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004632, test:0.000302 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004657, test:0.000358 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004741, test:0.000298 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004776, test:0.000316 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004979, test:0.000297 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004615, test:0.000358 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004660, test:0.000306 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004760, test:0.000321 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004695, test:0.000327 | lr:0.000100\n",
      "Mean absolute error:  1.6495439487685528\n",
      "Root mean squared error:  5.774031519016189\n",
      "Epoch[1/100] | loss train:0.083662, test:0.000465 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012445, test:0.000405 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011616, test:0.002883 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012439, test:0.001051 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008883, test:0.004578 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010317, test:0.000284 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008338, test:0.000937 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008575, test:0.000431 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008707, test:0.009020 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009209, test:0.000981 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008275, test:0.000606 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008478, test:0.000437 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008970, test:0.000920 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008283, test:0.001614 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009123, test:0.000605 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008342, test:0.000440 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008636, test:0.002386 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008705, test:0.000569 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007939, test:0.000727 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008394, test:0.001571 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008209, test:0.000572 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007536, test:0.000365 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014227, test:0.000937 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009196, test:0.000354 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008384, test:0.000520 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008779, test:0.000523 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008974, test:0.000348 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008074, test:0.000409 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007823, test:0.001555 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007454, test:0.000474 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012120, test:0.003879 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009283, test:0.000459 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008589, test:0.003863 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007701, test:0.000538 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007423, test:0.000447 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007300, test:0.000500 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007060, test:0.000608 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007426, test:0.001900 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008286, test:0.002041 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007319, test:0.000717 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006147, test:0.000399 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006293, test:0.000526 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005568, test:0.000421 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005755, test:0.000529 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007778, test:0.000423 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006271, test:0.000456 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005905, test:0.000490 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005546, test:0.000421 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007378, test:0.000364 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005778, test:0.000344 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005269, test:0.000411 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005367, test:0.000560 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006318, test:0.000537 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005610, test:0.000445 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006529, test:0.000377 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005100, test:0.000336 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005498, test:0.000371 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005560, test:0.000335 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008855, test:0.000436 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[60/100] | loss train:0.005740, test:0.000367 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005908, test:0.000340 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006134, test:0.000696 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005438, test:0.000408 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005525, test:0.000355 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005360, test:0.000399 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005801, test:0.000416 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005937, test:0.000297 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005474, test:0.000339 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005177, test:0.000691 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005701, test:0.000429 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005481, test:0.000424 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005292, test:0.000347 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005911, test:0.000350 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006269, test:0.000487 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005438, test:0.000392 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005682, test:0.000364 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005079, test:0.000309 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005568, test:0.000312 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.014174, test:0.000395 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006011, test:0.000405 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005377, test:0.000307 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005885, test:0.000329 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.006779, test:0.000324 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005024, test:0.000380 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005187, test:0.000334 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005106, test:0.000313 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005189, test:0.000373 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005103, test:0.000337 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.006597, test:0.000332 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.006065, test:0.000352 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.006007, test:0.000327 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.010379, test:0.000308 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005372, test:0.000349 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005838, test:0.000356 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004981, test:0.000325 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004892, test:0.000317 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006403, test:0.000358 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004934, test:0.000320 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005051, test:0.000344 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005126, test:0.000357 | lr:0.000100\n",
      "Mean absolute error:  1.7625781195691574\n",
      "Root mean squared error:  5.847514903483964\n",
      "Epoch[1/100] | loss train:0.047509, test:0.006328 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013103, test:0.002120 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012146, test:0.002952 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012173, test:0.001940 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010906, test:0.000472 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009195, test:0.005466 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008200, test:0.001120 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009990, test:0.002151 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008816, test:0.000465 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007882, test:0.000468 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011633, test:0.004013 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008427, test:0.002519 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013389, test:0.000305 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007935, test:0.005246 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007971, test:0.000399 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007755, test:0.002951 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007065, test:0.000478 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007016, test:0.001915 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008123, test:0.000608 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007215, test:0.007449 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008354, test:0.002248 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008858, test:0.003917 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012745, test:0.001472 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008511, test:0.000711 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007607, test:0.001057 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007656, test:0.000519 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006672, test:0.001067 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007846, test:0.000476 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008914, test:0.000797 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008203, test:0.000429 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008703, test:0.004546 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007119, test:0.003571 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007459, test:0.001388 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008235, test:0.000846 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010102, test:0.000434 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007127, test:0.001084 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008357, test:0.000470 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006594, test:0.000401 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011822, test:0.000957 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011905, test:0.000638 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007096, test:0.000486 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005695, test:0.000425 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006811, test:0.000525 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006784, test:0.000496 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006292, test:0.000450 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006665, test:0.000418 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005740, test:0.000388 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005294, test:0.000476 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006186, test:0.000566 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005077, test:0.000348 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005140, test:0.000588 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005207, test:0.000391 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005226, test:0.000486 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005369, test:0.000398 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005310, test:0.000536 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005837, test:0.000439 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005514, test:0.000406 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005164, test:0.000612 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005134, test:0.000516 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005438, test:0.000478 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005937, test:0.000374 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005228, test:0.000375 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005786, test:0.000526 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004978, test:0.000367 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008566, test:0.000359 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005191, test:0.000368 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006093, test:0.000347 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007319, test:0.000412 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006500, test:0.000399 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005088, test:0.000401 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005443, test:0.000327 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005434, test:0.000353 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005774, test:0.000444 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005586, test:0.000464 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005429, test:0.000511 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004828, test:0.000340 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007120, test:0.000327 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005668, test:0.000326 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005366, test:0.000388 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005312, test:0.000349 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005014, test:0.000365 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005392, test:0.000352 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005111, test:0.000379 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005883, test:0.000343 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[85/100] | loss train:0.005065, test:0.000362 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005541, test:0.000335 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005279, test:0.000408 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005687, test:0.000342 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004964, test:0.000323 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005363, test:0.000335 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005402, test:0.000344 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006284, test:0.000347 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005000, test:0.000369 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004997, test:0.000338 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005266, test:0.000329 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005317, test:0.000332 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005179, test:0.000351 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005273, test:0.000327 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004862, test:0.000345 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005412, test:0.000331 | lr:0.000100\n",
      "Mean absolute error:  1.7233110582305808\n",
      "Root mean squared error:  5.829755430082922\n",
      "Epoch[1/100] | loss train:0.055266, test:0.003081 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011432, test:0.005723 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009050, test:0.000410 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008049, test:0.001710 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009292, test:0.001603 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008768, test:0.000549 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008058, test:0.004211 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007733, test:0.000520 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007965, test:0.000911 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007389, test:0.000335 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007701, test:0.004980 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009828, test:0.008795 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008843, test:0.000433 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006766, test:0.000647 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008514, test:0.000806 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009054, test:0.000336 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008655, test:0.000408 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008473, test:0.000425 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007862, test:0.000378 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007811, test:0.000406 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006904, test:0.000330 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007010, test:0.000328 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006106, test:0.002054 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009056, test:0.004551 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009128, test:0.000796 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007163, test:0.000353 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006951, test:0.000843 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006826, test:0.001883 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008619, test:0.001038 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009326, test:0.000459 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008180, test:0.000425 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006769, test:0.000358 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007005, test:0.002060 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009191, test:0.000895 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011388, test:0.001942 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006612, test:0.001250 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006787, test:0.000723 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006438, test:0.000323 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006307, test:0.000932 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006715, test:0.000844 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007525, test:0.000431 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006640, test:0.000297 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005986, test:0.000368 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005187, test:0.000278 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005501, test:0.000538 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005013, test:0.000274 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005223, test:0.000305 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004762, test:0.000373 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004968, test:0.000280 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005502, test:0.000336 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005759, test:0.000306 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005308, test:0.000273 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004816, test:0.000294 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005005, test:0.000276 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004922, test:0.000266 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005621, test:0.000388 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004960, test:0.000278 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004954, test:0.000279 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004676, test:0.000348 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005667, test:0.000331 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005225, test:0.000310 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005147, test:0.000374 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006165, test:0.000355 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005293, test:0.000365 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005954, test:0.000297 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005174, test:0.000577 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004820, test:0.000314 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005277, test:0.000347 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004825, test:0.000345 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004829, test:0.000295 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004842, test:0.000415 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005106, test:0.000316 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004928, test:0.000267 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005664, test:0.000455 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005009, test:0.000323 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005086, test:0.000279 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005146, test:0.000273 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005356, test:0.000267 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004919, test:0.000287 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005160, test:0.000288 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004799, test:0.000293 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005218, test:0.000272 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005074, test:0.000258 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.011650, test:0.000268 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.008939, test:0.000277 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005202, test:0.000246 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005183, test:0.000270 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005842, test:0.000279 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004986, test:0.000250 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004680, test:0.000271 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005796, test:0.000267 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005055, test:0.000288 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005032, test:0.000285 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004972, test:0.000293 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004720, test:0.000277 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004657, test:0.000257 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004808, test:0.000285 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005333, test:0.000309 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005474, test:0.000273 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004578, test:0.000298 | lr:0.000100\n",
      "Mean absolute error:  1.7187121901893925\n",
      "Root mean squared error:  5.794765708910643\n",
      "Epoch[1/100] | loss train:0.056317, test:0.000422 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012402, test:0.007191 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009310, test:0.002402 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014188, test:0.002791 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009099, test:0.000384 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009008, test:0.000754 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010575, test:0.005419 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009203, test:0.001117 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/100] | loss train:0.008483, test:0.014568 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010080, test:0.000541 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007571, test:0.000411 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007305, test:0.001030 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007650, test:0.000454 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007986, test:0.001799 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006506, test:0.000504 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008387, test:0.003507 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007026, test:0.001671 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007350, test:0.001486 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007611, test:0.003299 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007944, test:0.000747 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006980, test:0.000515 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007285, test:0.000787 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006481, test:0.001062 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007442, test:0.001740 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007139, test:0.000534 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006937, test:0.002303 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006798, test:0.000364 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008079, test:0.000470 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007215, test:0.000397 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007244, test:0.001265 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006779, test:0.000835 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007760, test:0.002998 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007069, test:0.000432 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006323, test:0.000505 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006022, test:0.000827 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006855, test:0.002822 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006865, test:0.000342 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006887, test:0.002493 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007881, test:0.000674 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005991, test:0.000387 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005473, test:0.000281 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005102, test:0.000289 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005872, test:0.000347 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005295, test:0.000270 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004952, test:0.000331 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005179, test:0.000351 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005115, test:0.000347 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005082, test:0.000364 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008434, test:0.000287 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004863, test:0.000407 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005064, test:0.000271 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005541, test:0.000311 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005002, test:0.000277 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004936, test:0.000298 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004942, test:0.000366 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006479, test:0.000323 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005320, test:0.000387 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005769, test:0.000318 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005346, test:0.000453 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005640, test:0.000265 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004838, test:0.000269 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004937, test:0.000404 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005002, test:0.000286 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005472, test:0.000298 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005199, test:0.000281 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005374, test:0.000282 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005378, test:0.000386 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005213, test:0.000341 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004946, test:0.000773 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005117, test:0.000278 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.004769, test:0.000285 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.004997, test:0.000274 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004839, test:0.000292 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005066, test:0.000299 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005101, test:0.000479 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.008557, test:0.000278 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006949, test:0.000353 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005109, test:0.000371 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005808, test:0.000296 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006488, test:0.000289 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004361, test:0.000277 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005397, test:0.000291 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004818, test:0.000276 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004924, test:0.000274 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004967, test:0.000293 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004566, test:0.000266 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004700, test:0.000279 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004858, test:0.000264 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004981, test:0.000275 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.008851, test:0.000290 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004903, test:0.000277 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005930, test:0.000275 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004742, test:0.000301 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.007366, test:0.000298 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005096, test:0.000274 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005186, test:0.000287 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004784, test:0.000292 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004772, test:0.000328 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005194, test:0.000294 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004819, test:0.000262 | lr:0.000100\n",
      "Mean absolute error:  1.6645190417622715\n",
      "Root mean squared error:  5.766192273338622\n",
      "Epoch[1/100] | loss train:0.061617, test:0.000747 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013709, test:0.001077 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010254, test:0.001837 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010719, test:0.000594 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010578, test:0.000515 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009543, test:0.001313 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010376, test:0.000933 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.016966, test:0.000627 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012319, test:0.004360 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008889, test:0.001683 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009071, test:0.001083 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008051, test:0.000441 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009747, test:0.000483 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013231, test:0.002321 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007774, test:0.003826 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009862, test:0.002550 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008988, test:0.001472 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008198, test:0.001268 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008245, test:0.001828 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007672, test:0.001261 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008453, test:0.000465 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007637, test:0.002649 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010552, test:0.000397 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009323, test:0.000908 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007980, test:0.000620 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008200, test:0.000428 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008576, test:0.002278 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009003, test:0.000425 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007625, test:0.001339 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007430, test:0.001406 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008294, test:0.001050 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007318, test:0.000336 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007282, test:0.000501 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[34/100] | loss train:0.007650, test:0.001514 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007053, test:0.000532 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009152, test:0.000365 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007801, test:0.000815 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007804, test:0.000413 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006405, test:0.000423 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007421, test:0.000697 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005731, test:0.000403 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005491, test:0.000372 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005512, test:0.000418 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006851, test:0.000377 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006065, test:0.000370 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006087, test:0.000438 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005457, test:0.000355 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005440, test:0.000789 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005883, test:0.000326 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005436, test:0.000527 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005823, test:0.000479 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005501, test:0.000492 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.010043, test:0.000400 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.011101, test:0.000479 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006605, test:0.000358 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006104, test:0.000393 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006048, test:0.000329 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005594, test:0.000339 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005818, test:0.000474 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005120, test:0.000379 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005894, test:0.000583 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.011475, test:0.000414 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005849, test:0.000441 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006124, test:0.000626 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005500, test:0.000301 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005674, test:0.000647 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006351, test:0.000312 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006692, test:0.000335 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005532, test:0.000387 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005853, test:0.000333 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005002, test:0.000337 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005269, test:0.000334 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005371, test:0.000311 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004975, test:0.000369 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005681, test:0.000471 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007330, test:0.000431 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005314, test:0.000335 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005335, test:0.000444 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004934, test:0.000308 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005624, test:0.000329 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004861, test:0.000317 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005575, test:0.000339 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005386, test:0.000316 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005004, test:0.000324 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005088, test:0.000304 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005274, test:0.000293 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005566, test:0.000286 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.008803, test:0.000308 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005508, test:0.000312 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004906, test:0.000295 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005644, test:0.000315 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005675, test:0.000322 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005298, test:0.000324 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004775, test:0.000289 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005510, test:0.000306 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005736, test:0.000326 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005463, test:0.000278 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005054, test:0.000302 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005641, test:0.000312 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.008187, test:0.000368 | lr:0.000100\n",
      "Mean absolute error:  1.8719879943572177\n",
      "Root mean squared error:  5.906154865292168\n",
      "Epoch[1/100] | loss train:0.059453, test:0.003374 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010130, test:0.001970 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009424, test:0.000383 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009032, test:0.002516 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009488, test:0.002651 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008210, test:0.000853 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009664, test:0.001380 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008738, test:0.000522 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009103, test:0.002230 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009103, test:0.008523 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008497, test:0.002412 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008446, test:0.000741 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008882, test:0.001009 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008864, test:0.003368 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008146, test:0.000493 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009205, test:0.001979 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007465, test:0.000861 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006460, test:0.001157 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008754, test:0.001390 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008464, test:0.000550 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007389, test:0.003269 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008240, test:0.000566 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007709, test:0.000936 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007318, test:0.000563 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007927, test:0.000344 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007194, test:0.000567 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007059, test:0.000790 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008725, test:0.000388 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008144, test:0.002199 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007212, test:0.000431 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008425, test:0.005091 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008459, test:0.001996 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008190, test:0.000671 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008633, test:0.000438 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009041, test:0.001328 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007471, test:0.001023 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006738, test:0.004127 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008560, test:0.001907 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006859, test:0.000701 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007220, test:0.000362 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006021, test:0.000503 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006135, test:0.000367 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005166, test:0.000369 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005140, test:0.000509 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006853, test:0.000377 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005135, test:0.000811 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006207, test:0.000339 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005471, test:0.000327 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005245, test:0.000365 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004901, test:0.000563 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004896, test:0.000425 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007831, test:0.000448 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006097, test:0.000330 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005499, test:0.000351 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004860, test:0.000438 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005319, test:0.000319 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007143, test:0.000342 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005400, test:0.000319 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[59/100] | loss train:0.005435, test:0.000324 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005670, test:0.000328 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004824, test:0.000340 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005262, test:0.000387 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005589, test:0.000318 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004932, test:0.000412 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005099, test:0.000298 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005594, test:0.000349 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005260, test:0.000343 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005289, test:0.000310 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004946, test:0.000461 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004858, test:0.000393 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005133, test:0.000313 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005597, test:0.000352 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005231, test:0.000309 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.004983, test:0.000320 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.008189, test:0.000356 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005270, test:0.000390 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004743, test:0.000347 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005734, test:0.000345 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006628, test:0.000323 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006588, test:0.000415 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004766, test:0.000349 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004558, test:0.000378 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005159, test:0.000341 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005004, test:0.000381 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005180, test:0.000327 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005160, test:0.000303 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005455, test:0.000302 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004903, test:0.000315 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005138, test:0.000323 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005002, test:0.000310 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005150, test:0.000335 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005457, test:0.000335 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005259, test:0.000314 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004604, test:0.000338 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004750, test:0.000322 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005047, test:0.000334 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004802, test:0.000298 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005668, test:0.000344 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004861, test:0.000301 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005262, test:0.000316 | lr:0.000100\n",
      "Mean absolute error:  1.6948616274684496\n",
      "Root mean squared error:  5.806148207663716\n",
      "Epoch[1/100] | loss train:0.041525, test:0.001573 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013187, test:0.000298 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009773, test:0.010170 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009246, test:0.001059 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008406, test:0.002277 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008316, test:0.000490 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010583, test:0.009929 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009807, test:0.000475 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008948, test:0.000607 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007715, test:0.003278 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009531, test:0.005252 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008432, test:0.000586 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007632, test:0.000986 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007783, test:0.000435 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009266, test:0.002731 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008696, test:0.000408 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007600, test:0.000739 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010316, test:0.000515 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007292, test:0.000556 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007557, test:0.000414 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008457, test:0.002538 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008060, test:0.000409 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006961, test:0.000480 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006571, test:0.002791 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007550, test:0.000714 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006860, test:0.000450 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006476, test:0.001044 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.014026, test:0.004304 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010328, test:0.000611 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006400, test:0.000321 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011482, test:0.002293 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007579, test:0.000421 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007790, test:0.001826 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007112, test:0.000484 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006856, test:0.000809 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008323, test:0.000559 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006967, test:0.000398 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007086, test:0.001067 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006397, test:0.000612 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006268, test:0.000779 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008179, test:0.000370 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005911, test:0.000395 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005096, test:0.000333 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005736, test:0.000329 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005095, test:0.000380 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007157, test:0.000362 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006353, test:0.000310 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005140, test:0.000414 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005341, test:0.000313 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004904, test:0.000321 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005286, test:0.000511 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005074, test:0.000512 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005271, test:0.000328 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005229, test:0.000320 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006835, test:0.000338 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005426, test:0.000339 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004855, test:0.000416 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005285, test:0.000416 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005188, test:0.000353 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004854, test:0.000350 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006242, test:0.000345 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005041, test:0.000452 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004725, test:0.000372 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005143, test:0.000264 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005280, test:0.000494 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005023, test:0.000339 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005558, test:0.000365 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006111, test:0.000390 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004708, test:0.000299 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005509, test:0.000376 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005435, test:0.000335 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005036, test:0.000439 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005177, test:0.000406 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005657, test:0.000325 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005110, test:0.000267 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005542, test:0.000327 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005871, test:0.000325 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005634, test:0.000647 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005261, test:0.000322 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.004670, test:0.000327 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006784, test:0.000313 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006405, test:0.000308 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005551, test:0.000318 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[84/100] | loss train:0.004597, test:0.000290 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004498, test:0.000311 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004527, test:0.000298 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004966, test:0.000312 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005119, test:0.000302 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005243, test:0.000362 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005609, test:0.000345 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.006331, test:0.000304 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005138, test:0.000311 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004888, test:0.000324 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004972, test:0.000338 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004666, test:0.000318 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004784, test:0.000316 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004845, test:0.000282 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005632, test:0.000302 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005177, test:0.000279 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005759, test:0.000288 | lr:0.000100\n",
      "Mean absolute error:  1.6652398185843273\n",
      "Root mean squared error:  5.780154948807362\n",
      "Epoch[1/100] | loss train:0.045471, test:0.005392 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012026, test:0.000532 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011830, test:0.000313 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010815, test:0.001381 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008560, test:0.006840 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009330, test:0.004495 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008342, test:0.003017 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008692, test:0.001211 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012135, test:0.000838 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008200, test:0.002908 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010053, test:0.000334 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008955, test:0.000468 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007797, test:0.009527 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007688, test:0.000427 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008286, test:0.000405 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008839, test:0.002930 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007840, test:0.000589 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011134, test:0.000516 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010708, test:0.004076 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008238, test:0.003215 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008110, test:0.000343 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007591, test:0.000427 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006981, test:0.001583 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007670, test:0.000902 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006464, test:0.000320 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008442, test:0.000303 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007123, test:0.000645 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006847, test:0.001419 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008463, test:0.001287 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007251, test:0.001108 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008214, test:0.000684 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007406, test:0.000971 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006953, test:0.001673 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008504, test:0.000335 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008086, test:0.000422 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006021, test:0.001397 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007110, test:0.000514 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008238, test:0.002079 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012598, test:0.002551 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009287, test:0.000344 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005416, test:0.000558 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005593, test:0.000333 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006430, test:0.000329 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005180, test:0.000284 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006048, test:0.000301 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005089, test:0.000330 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004998, test:0.000360 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005204, test:0.000352 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004911, test:0.000411 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005151, test:0.000302 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005301, test:0.000301 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005088, test:0.000317 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005824, test:0.000289 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004961, test:0.000322 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005138, test:0.000317 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004929, test:0.000287 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.010064, test:0.000342 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005020, test:0.000352 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005085, test:0.000311 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005114, test:0.000327 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005522, test:0.000393 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005136, test:0.000284 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004915, test:0.000407 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005252, test:0.000309 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005588, test:0.000360 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004935, test:0.000388 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005132, test:0.000350 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.009112, test:0.000393 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005210, test:0.000279 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005332, test:0.000294 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005048, test:0.000282 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005065, test:0.000283 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005376, test:0.000284 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005601, test:0.000294 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005139, test:0.000287 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004756, test:0.000397 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004954, test:0.000286 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005190, test:0.000279 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004950, test:0.000330 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005727, test:0.000343 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005205, test:0.000277 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.004702, test:0.000300 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.010342, test:0.000256 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005059, test:0.000266 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006647, test:0.000279 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004902, test:0.000290 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005591, test:0.000289 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005279, test:0.000258 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004903, test:0.000285 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005116, test:0.000302 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005079, test:0.000281 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005119, test:0.000264 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005235, test:0.000283 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005367, test:0.000281 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.006054, test:0.000258 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004941, test:0.000292 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005733, test:0.000285 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004940, test:0.000292 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005036, test:0.000294 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005631, test:0.000277 | lr:0.000100\n",
      "Mean absolute error:  1.6975506207535118\n",
      "Root mean squared error:  5.765260231584577\n",
      "Epoch[1/100] | loss train:0.057178, test:0.002049 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011774, test:0.000428 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010058, test:0.000895 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010806, test:0.000381 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010814, test:0.000487 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008511, test:0.000420 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010396, test:0.000740 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/100] | loss train:0.008451, test:0.000715 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009451, test:0.000424 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008689, test:0.000335 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009019, test:0.000559 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008396, test:0.001154 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007375, test:0.000463 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011409, test:0.000964 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009922, test:0.000586 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006860, test:0.000889 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008921, test:0.004766 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009890, test:0.000608 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008526, test:0.002483 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006749, test:0.003452 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008334, test:0.001957 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009281, test:0.002149 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008261, test:0.001981 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007164, test:0.000576 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009939, test:0.000555 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008890, test:0.003614 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008889, test:0.000728 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007885, test:0.001997 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007808, test:0.001771 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007907, test:0.000401 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007681, test:0.000362 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008330, test:0.000419 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006436, test:0.002450 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006638, test:0.001679 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007702, test:0.000746 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007736, test:0.000478 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008697, test:0.002129 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007977, test:0.004582 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007927, test:0.001440 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008343, test:0.000474 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006863, test:0.000447 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005875, test:0.000322 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006023, test:0.000456 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005260, test:0.000347 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005850, test:0.000606 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005005, test:0.000356 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005764, test:0.000342 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005249, test:0.000357 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005137, test:0.000315 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005335, test:0.000308 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008097, test:0.000372 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005515, test:0.000311 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005276, test:0.000326 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005426, test:0.000367 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005125, test:0.000307 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005408, test:0.000298 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005299, test:0.000521 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005117, test:0.000385 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005523, test:0.000370 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006126, test:0.000327 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005372, test:0.000348 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005709, test:0.000318 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005777, test:0.000356 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005504, test:0.000303 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004762, test:0.000420 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004977, test:0.000412 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005880, test:0.000331 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005542, test:0.000679 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005541, test:0.000316 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005128, test:0.000319 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005429, test:0.000296 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006124, test:0.000366 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005691, test:0.000306 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005686, test:0.000314 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004646, test:0.000365 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005715, test:0.000338 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005195, test:0.000433 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005212, test:0.000444 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.009264, test:0.000314 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005377, test:0.000412 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004830, test:0.000316 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005079, test:0.000364 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005387, test:0.000310 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005159, test:0.000339 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005568, test:0.000342 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004901, test:0.000351 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005086, test:0.000310 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004898, test:0.000353 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005402, test:0.000331 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004952, test:0.000304 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004860, test:0.000320 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005081, test:0.000310 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004780, test:0.000327 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006130, test:0.000318 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005601, test:0.000337 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004978, test:0.000324 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005493, test:0.000296 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005063, test:0.000328 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004434, test:0.000330 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004821, test:0.000320 | lr:0.000100\n",
      "Mean absolute error:  1.7479267117035961\n",
      "Root mean squared error:  5.804781048946008\n",
      "Epoch[1/100] | loss train:0.048187, test:0.000734 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013191, test:0.000436 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008585, test:0.008925 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012583, test:0.000354 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010894, test:0.000681 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009306, test:0.000387 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011275, test:0.005090 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010277, test:0.000414 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008526, test:0.002444 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015127, test:0.001707 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008869, test:0.001865 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009339, test:0.002713 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009683, test:0.000634 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007376, test:0.001682 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007906, test:0.000943 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008961, test:0.000622 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007772, test:0.000679 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007898, test:0.001641 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007829, test:0.001071 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008533, test:0.000566 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008934, test:0.000935 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008102, test:0.000774 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010675, test:0.000558 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007579, test:0.000391 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007142, test:0.004776 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007233, test:0.003529 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007867, test:0.000521 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007409, test:0.000360 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007298, test:0.000487 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007929, test:0.001941 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010242, test:0.001273 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007438, test:0.002514 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[33/100] | loss train:0.006622, test:0.000446 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010936, test:0.001406 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007179, test:0.000404 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006954, test:0.000496 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007494, test:0.000890 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011080, test:0.000687 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007494, test:0.001127 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010726, test:0.001103 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006367, test:0.000357 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.004955, test:0.000558 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005317, test:0.000435 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005378, test:0.000561 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005381, test:0.000474 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005204, test:0.000352 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005832, test:0.000345 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005150, test:0.000419 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006007, test:0.000423 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004994, test:0.000424 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005381, test:0.000448 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006159, test:0.000367 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006799, test:0.000336 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005330, test:0.000320 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005307, test:0.000328 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005400, test:0.000390 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004976, test:0.000329 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005629, test:0.000407 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005516, test:0.000437 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004996, test:0.000354 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005674, test:0.000337 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005247, test:0.000386 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007965, test:0.000331 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005133, test:0.000310 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005656, test:0.000340 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004970, test:0.000368 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004991, test:0.000346 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005176, test:0.000345 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005352, test:0.000701 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005034, test:0.000320 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005371, test:0.000345 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006314, test:0.000405 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004947, test:0.000352 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005004, test:0.000305 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005345, test:0.000337 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005347, test:0.000325 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005109, test:0.000309 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005879, test:0.000336 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005010, test:0.000330 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005002, test:0.000318 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004848, test:0.000317 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005464, test:0.000330 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005417, test:0.000329 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005369, test:0.000323 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005351, test:0.000327 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.004738, test:0.000309 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004898, test:0.000306 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005313, test:0.000342 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004989, test:0.000310 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005509, test:0.000338 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004879, test:0.000320 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005052, test:0.000312 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005102, test:0.000330 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004596, test:0.000319 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.010250, test:0.000334 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005345, test:0.000318 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006030, test:0.000349 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.007028, test:0.000328 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004805, test:0.000330 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005075, test:0.000378 | lr:0.000100\n",
      "Mean absolute error:  1.8053111532639894\n",
      "Root mean squared error:  5.879305607649965\n",
      "Epoch[1/100] | loss train:0.041735, test:0.001534 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011633, test:0.000516 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009863, test:0.003814 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011611, test:0.002372 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008680, test:0.000331 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009796, test:0.004910 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009444, test:0.002286 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011270, test:0.005649 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016125, test:0.000822 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009688, test:0.000765 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008680, test:0.000416 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007806, test:0.000370 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008942, test:0.004147 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008715, test:0.000544 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010437, test:0.000838 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008495, test:0.000449 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009449, test:0.000413 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007561, test:0.001423 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009120, test:0.000525 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009137, test:0.000803 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007040, test:0.002173 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007438, test:0.002860 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008100, test:0.000360 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008504, test:0.000443 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007662, test:0.000328 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007600, test:0.000616 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008263, test:0.000365 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007230, test:0.003162 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009856, test:0.000596 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008840, test:0.000467 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006817, test:0.002664 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007665, test:0.001443 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007598, test:0.000598 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007088, test:0.000592 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008097, test:0.004898 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007613, test:0.001026 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006835, test:0.000750 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007031, test:0.001682 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007840, test:0.000862 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006949, test:0.000597 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005735, test:0.000642 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005983, test:0.000430 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006079, test:0.000380 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005969, test:0.000410 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007927, test:0.000397 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009214, test:0.000463 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006321, test:0.000519 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007256, test:0.000407 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005011, test:0.000543 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.018027, test:0.000446 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005057, test:0.000390 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005229, test:0.000392 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005665, test:0.000517 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005415, test:0.000433 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005449, test:0.000447 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005090, test:0.000487 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005678, test:0.000405 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[58/100] | loss train:0.005638, test:0.000612 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005189, test:0.000392 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005706, test:0.000417 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005228, test:0.000361 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005273, test:0.000355 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005985, test:0.000432 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005916, test:0.000393 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006016, test:0.000399 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005385, test:0.000507 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004937, test:0.000439 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005615, test:0.000348 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007826, test:0.000375 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007092, test:0.000627 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005883, test:0.000379 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005357, test:0.000386 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005550, test:0.000352 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.007719, test:0.000365 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005754, test:0.000341 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004965, test:0.000494 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006356, test:0.000665 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005200, test:0.000625 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005462, test:0.000622 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.011792, test:0.000403 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005811, test:0.000345 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005583, test:0.000348 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005937, test:0.000350 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004737, test:0.000376 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004914, test:0.000359 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005037, test:0.000370 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005050, test:0.000357 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004892, test:0.000363 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005271, test:0.000339 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005318, test:0.000339 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005695, test:0.000320 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005445, test:0.000359 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005052, test:0.000329 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.004953, test:0.000384 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004822, test:0.000351 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004945, test:0.000368 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005139, test:0.000372 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004832, test:0.000376 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006565, test:0.000382 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004961, test:0.000338 | lr:0.000100\n",
      "Mean absolute error:  1.656811128529846\n",
      "Root mean squared error:  5.798873103337539\n",
      "Epoch[1/100] | loss train:0.059791, test:0.004735 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010773, test:0.001503 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011146, test:0.002596 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010396, test:0.001770 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009287, test:0.000541 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009405, test:0.000335 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008937, test:0.001869 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008535, test:0.000500 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007931, test:0.000614 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008166, test:0.000412 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008320, test:0.000362 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009580, test:0.002094 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007488, test:0.001611 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008210, test:0.000335 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008273, test:0.000389 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008253, test:0.000331 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008157, test:0.000515 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007104, test:0.006585 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007203, test:0.002235 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008381, test:0.000384 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008815, test:0.001636 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007692, test:0.000410 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007242, test:0.000370 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007292, test:0.000419 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008259, test:0.000892 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007851, test:0.000987 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008299, test:0.001304 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007818, test:0.001793 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008544, test:0.000367 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007038, test:0.000405 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006801, test:0.000425 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006658, test:0.003540 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008122, test:0.001921 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007375, test:0.000594 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007114, test:0.000443 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010221, test:0.002652 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007666, test:0.000510 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006671, test:0.004604 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006744, test:0.001374 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006967, test:0.000859 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005688, test:0.000384 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005120, test:0.000433 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005251, test:0.000373 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005073, test:0.000359 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004914, test:0.000395 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005174, test:0.000367 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005222, test:0.000504 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004931, test:0.000341 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005239, test:0.000465 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006482, test:0.000335 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005293, test:0.000356 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004917, test:0.000347 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005797, test:0.000534 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004973, test:0.000396 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004955, test:0.000325 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005302, test:0.000327 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005191, test:0.000317 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005164, test:0.000389 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005103, test:0.000577 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004493, test:0.000396 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006341, test:0.000411 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006840, test:0.000393 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005249, test:0.000360 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005068, test:0.000370 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005481, test:0.000319 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005693, test:0.000398 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005656, test:0.000351 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005069, test:0.000343 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005147, test:0.000461 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005275, test:0.000377 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005609, test:0.000378 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005305, test:0.000340 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006216, test:0.000361 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005625, test:0.000375 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005210, test:0.000313 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005658, test:0.000332 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005534, test:0.000342 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004758, test:0.000299 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.004963, test:0.000335 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006045, test:0.000334 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005008, test:0.000320 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005025, test:0.000346 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[83/100] | loss train:0.004781, test:0.000314 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005748, test:0.000309 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004798, test:0.000349 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005253, test:0.000294 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.007521, test:0.000317 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004968, test:0.000338 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004644, test:0.000348 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005746, test:0.000331 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004755, test:0.000349 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004949, test:0.000352 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004842, test:0.000317 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006342, test:0.000360 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004816, test:0.000315 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005721, test:0.000320 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005643, test:0.000310 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005559, test:0.000295 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005106, test:0.000322 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005173, test:0.000297 | lr:0.000100\n",
      "Mean absolute error:  1.7663179348338554\n",
      "Root mean squared error:  5.817565946068201\n",
      "Epoch[1/100] | loss train:0.061281, test:0.000830 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011219, test:0.000360 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010638, test:0.008887 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011199, test:0.000403 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009076, test:0.000686 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009640, test:0.000750 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009186, test:0.001261 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007605, test:0.000883 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008463, test:0.002459 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007426, test:0.000397 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007753, test:0.000467 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006746, test:0.000841 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007654, test:0.001335 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008430, test:0.001532 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008984, test:0.005080 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009828, test:0.000586 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008120, test:0.000435 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007265, test:0.000394 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006459, test:0.000829 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008343, test:0.000413 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007165, test:0.000905 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008906, test:0.000693 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006352, test:0.001561 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006710, test:0.000478 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007370, test:0.000359 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009644, test:0.001987 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006400, test:0.001641 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007731, test:0.004080 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007631, test:0.003552 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007108, test:0.000375 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008023, test:0.000457 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007552, test:0.001391 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006966, test:0.000361 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007725, test:0.000386 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006092, test:0.001861 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006837, test:0.000825 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006568, test:0.000393 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006435, test:0.000607 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006409, test:0.000538 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006338, test:0.000421 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005180, test:0.000386 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005279, test:0.000343 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005170, test:0.000516 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006608, test:0.000324 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004920, test:0.000324 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004970, test:0.000469 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006448, test:0.000395 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005503, test:0.000303 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005229, test:0.000378 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007389, test:0.000721 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004960, test:0.000328 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005179, test:0.000299 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004862, test:0.000568 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005567, test:0.000399 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005225, test:0.000425 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005335, test:0.000354 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004969, test:0.000322 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007969, test:0.000299 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005067, test:0.000407 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004795, test:0.000337 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006171, test:0.000320 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004924, test:0.000305 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004783, test:0.000315 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004680, test:0.000430 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005121, test:0.000312 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004778, test:0.000307 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005401, test:0.000301 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005735, test:0.000297 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005374, test:0.000368 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005432, test:0.000311 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005425, test:0.000457 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005091, test:0.000303 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.004859, test:0.000304 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005180, test:0.000406 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005414, test:0.000388 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005194, test:0.000311 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005175, test:0.000368 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005140, test:0.000325 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005991, test:0.000319 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005124, test:0.000385 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005009, test:0.000293 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005050, test:0.000327 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004440, test:0.000301 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005702, test:0.000300 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005646, test:0.000288 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005202, test:0.000304 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.005063, test:0.000286 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004512, test:0.000308 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005568, test:0.000291 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.005413, test:0.000273 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005305, test:0.000299 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005556, test:0.000312 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.004967, test:0.000310 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005240, test:0.000275 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004750, test:0.000309 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004835, test:0.000305 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004941, test:0.000284 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005619, test:0.000291 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005396, test:0.000278 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004873, test:0.000296 | lr:0.000100\n",
      "Mean absolute error:  1.7045360698549106\n",
      "Root mean squared error:  5.783101634677772\n",
      "Epoch[1/100] | loss train:0.055638, test:0.007183 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014880, test:0.001047 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012783, test:0.001439 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009742, test:0.000369 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010220, test:0.004321 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015225, test:0.010486 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.016257, test:0.001409 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007867, test:0.000465 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008783, test:0.002359 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008804, test:0.003355 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015035, test:0.007095 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009582, test:0.002828 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008364, test:0.000294 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006996, test:0.001019 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007417, test:0.000349 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008465, test:0.001564 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007558, test:0.000390 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009314, test:0.000716 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008228, test:0.002539 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008777, test:0.000470 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008159, test:0.000566 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009415, test:0.002169 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007785, test:0.003133 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007721, test:0.000818 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007258, test:0.001607 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007000, test:0.000425 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007484, test:0.002318 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009688, test:0.000380 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008129, test:0.000384 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007425, test:0.000448 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007347, test:0.003177 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008290, test:0.000578 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008060, test:0.006340 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010786, test:0.003418 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008161, test:0.000390 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007966, test:0.000676 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007838, test:0.000514 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006604, test:0.004926 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007602, test:0.000380 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007467, test:0.000523 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005793, test:0.000450 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005960, test:0.000456 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005684, test:0.000469 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005379, test:0.000499 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005064, test:0.000460 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010018, test:0.000519 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005382, test:0.000386 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005821, test:0.000506 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005650, test:0.000546 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006953, test:0.000397 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006483, test:0.000364 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005517, test:0.000378 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005558, test:0.000499 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006130, test:0.000364 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005603, test:0.000325 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008245, test:0.000398 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005254, test:0.000430 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005052, test:0.000455 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005874, test:0.000349 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005262, test:0.000460 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005586, test:0.000406 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005448, test:0.000321 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005719, test:0.000322 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005339, test:0.000404 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004889, test:0.000349 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006367, test:0.000371 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.005339, test:0.000343 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005784, test:0.000412 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004998, test:0.000858 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.004773, test:0.000338 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005474, test:0.000408 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006762, test:0.000334 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006074, test:0.000499 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.010192, test:0.000381 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005134, test:0.000338 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005319, test:0.000378 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006324, test:0.000413 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.008276, test:0.000979 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005554, test:0.000389 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005085, test:0.000500 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.005201, test:0.000374 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005612, test:0.000356 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005291, test:0.000333 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005981, test:0.000363 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.005109, test:0.000332 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005547, test:0.000309 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004890, test:0.000328 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005314, test:0.000339 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005406, test:0.000345 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004949, test:0.000348 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004682, test:0.000332 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005428, test:0.000352 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005593, test:0.000342 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005124, test:0.000375 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.005172, test:0.000316 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005157, test:0.000326 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005173, test:0.000369 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.005320, test:0.000332 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005210, test:0.000339 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005344, test:0.000396 | lr:0.000100\n",
      "Mean absolute error:  1.8044723367955182\n",
      "Root mean squared error:  5.882824210434338\n",
      "Epoch[1/100] | loss train:0.055387, test:0.001334 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010727, test:0.000522 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010876, test:0.011125 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009786, test:0.010080 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.021099, test:0.003445 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009620, test:0.001559 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008457, test:0.002253 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007860, test:0.001866 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013101, test:0.004443 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010963, test:0.000368 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008253, test:0.000299 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008118, test:0.002423 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008559, test:0.000352 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007813, test:0.005814 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007795, test:0.000355 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009006, test:0.000534 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007585, test:0.000504 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008317, test:0.008177 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012696, test:0.000954 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008297, test:0.000842 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007882, test:0.000731 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009410, test:0.003438 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007787, test:0.000376 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006934, test:0.000537 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008673, test:0.002340 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006692, test:0.000357 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006781, test:0.003366 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006835, test:0.000465 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006439, test:0.000637 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008455, test:0.001100 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007191, test:0.000424 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[32/100] | loss train:0.007654, test:0.000430 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007150, test:0.000660 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006319, test:0.000812 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008691, test:0.000369 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007390, test:0.000706 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007283, test:0.000851 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009963, test:0.003410 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007239, test:0.000415 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007842, test:0.001395 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005579, test:0.000477 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005814, test:0.000476 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005385, test:0.000426 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.011860, test:0.000453 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005263, test:0.000395 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006140, test:0.000385 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005101, test:0.000429 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005225, test:0.000492 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005577, test:0.000402 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005298, test:0.000623 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005562, test:0.000531 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005851, test:0.000360 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004974, test:0.000358 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005314, test:0.000364 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005599, test:0.000355 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004950, test:0.000461 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005014, test:0.000367 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005543, test:0.000479 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005026, test:0.000363 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005305, test:0.000437 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007756, test:0.000495 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005190, test:0.000364 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005773, test:0.000389 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004917, test:0.000482 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008489, test:0.000344 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.010074, test:0.000344 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006208, test:0.000331 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005484, test:0.001039 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.004726, test:0.000342 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005339, test:0.000365 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005251, test:0.000400 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005303, test:0.000380 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005015, test:0.000416 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005409, test:0.000405 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006858, test:0.000668 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004872, test:0.000334 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.004858, test:0.000322 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005104, test:0.000367 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005296, test:0.000457 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005586, test:0.000329 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004905, test:0.000359 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005558, test:0.000323 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005970, test:0.000331 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005283, test:0.000323 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004719, test:0.000347 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005081, test:0.000344 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004916, test:0.000364 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.005357, test:0.000330 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004600, test:0.000331 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004889, test:0.000362 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005577, test:0.000291 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.004667, test:0.000337 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.009003, test:0.000330 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.007913, test:0.000340 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.006490, test:0.000312 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005296, test:0.000319 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005018, test:0.000348 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004825, test:0.000316 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005327, test:0.000331 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004857, test:0.000346 | lr:0.000100\n",
      "Mean absolute error:  1.7784903601675344\n",
      "Root mean squared error:  5.851087349336259\n",
      "Epoch[1/100] | loss train:0.054964, test:0.003170 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013218, test:0.002227 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008279, test:0.001883 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010845, test:0.001831 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010083, test:0.000380 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008533, test:0.001739 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009689, test:0.000369 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008018, test:0.001566 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009863, test:0.003115 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009795, test:0.000350 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007698, test:0.002880 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008028, test:0.002323 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012380, test:0.003430 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009792, test:0.001012 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007246, test:0.000383 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008465, test:0.000373 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007736, test:0.000515 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007983, test:0.001745 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010360, test:0.002433 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007392, test:0.003040 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009826, test:0.000959 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007249, test:0.000732 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008743, test:0.001784 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006560, test:0.000724 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007792, test:0.000374 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006922, test:0.000728 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008496, test:0.000421 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007220, test:0.002317 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007177, test:0.000594 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007526, test:0.000379 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008576, test:0.000514 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007455, test:0.000930 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006824, test:0.000487 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007378, test:0.001991 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006897, test:0.002782 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006526, test:0.001339 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006730, test:0.000658 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008164, test:0.001310 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007275, test:0.002511 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007474, test:0.002209 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006924, test:0.000608 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005818, test:0.000431 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005758, test:0.000617 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005222, test:0.000540 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005379, test:0.000366 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005208, test:0.000383 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005126, test:0.000315 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005305, test:0.000350 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005092, test:0.000360 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004772, test:0.000334 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006265, test:0.000327 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004860, test:0.000295 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005338, test:0.000299 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005443, test:0.000534 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005395, test:0.000296 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004851, test:0.000353 | lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[57/100] | loss train:0.005203, test:0.000323 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004563, test:0.000492 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005143, test:0.000426 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004926, test:0.000640 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005937, test:0.000385 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005856, test:0.000438 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.005083, test:0.000306 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004892, test:0.000320 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006494, test:0.000327 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004910, test:0.000327 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006375, test:0.000401 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005530, test:0.000308 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.010035, test:0.000476 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005941, test:0.000298 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007179, test:0.000304 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005037, test:0.000339 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005230, test:0.000343 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005199, test:0.000368 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.004860, test:0.000402 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.004878, test:0.000471 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005894, test:0.000409 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.004755, test:0.000332 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.005628, test:0.000463 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005195, test:0.000395 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004721, test:0.000313 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.005207, test:0.000308 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.005160, test:0.000294 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.004684, test:0.000292 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004915, test:0.000291 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005159, test:0.000284 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.004963, test:0.000304 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.004842, test:0.000293 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.005012, test:0.000302 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.004990, test:0.000296 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.004891, test:0.000287 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.005161, test:0.000277 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005178, test:0.000288 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.006473, test:0.000310 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.007965, test:0.000315 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.004929, test:0.000324 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.004952, test:0.000315 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004998, test:0.000291 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.004748, test:0.000306 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.005734, test:0.000292 | lr:0.000100\n",
      "Mean absolute error:  1.6900797320129588\n",
      "Root mean squared error:  5.773533512241212\n",
      "Epoch[1/100] | loss train:0.062505, test:0.001700 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016565, test:0.000487 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010889, test:0.000323 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010114, test:0.000576 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010789, test:0.003375 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009663, test:0.000408 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009286, test:0.001992 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009890, test:0.001466 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009000, test:0.000380 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013499, test:0.000780 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009945, test:0.000296 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008493, test:0.001870 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009491, test:0.001603 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008533, test:0.000616 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010520, test:0.000564 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007530, test:0.001338 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007016, test:0.000493 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008605, test:0.000822 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007480, test:0.001014 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006669, test:0.000883 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007388, test:0.003868 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007860, test:0.001180 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009636, test:0.002048 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008722, test:0.000769 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009028, test:0.002110 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008095, test:0.000612 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006605, test:0.000564 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006824, test:0.004159 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006962, test:0.004896 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007659, test:0.000437 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006842, test:0.000617 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006961, test:0.004481 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010703, test:0.001362 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007433, test:0.000319 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006802, test:0.000958 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008199, test:0.002793 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008903, test:0.001136 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007270, test:0.000294 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006518, test:0.001104 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006913, test:0.000563 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005796, test:0.000282 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005989, test:0.000378 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005517, test:0.000279 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005518, test:0.000291 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005577, test:0.000290 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008948, test:0.000295 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005148, test:0.000360 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005195, test:0.000288 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005341, test:0.000266 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008936, test:0.000343 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005540, test:0.000411 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004808, test:0.000444 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005265, test:0.000336 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005550, test:0.000280 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005616, test:0.000282 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005618, test:0.000395 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008738, test:0.000405 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004940, test:0.000544 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004910, test:0.000277 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005945, test:0.000276 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.005127, test:0.000353 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005317, test:0.000499 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004884, test:0.000266 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005061, test:0.000305 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.005102, test:0.000290 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.005453, test:0.000266 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004965, test:0.000385 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.005283, test:0.000293 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.005209, test:0.000366 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.005488, test:0.000333 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.005357, test:0.000341 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.005209, test:0.000276 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.005466, test:0.000287 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.005481, test:0.000374 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.005742, test:0.000300 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.005433, test:0.000337 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.005030, test:0.000305 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.005556, test:0.000362 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006434, test:0.000298 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.005766, test:0.000282 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.004584, test:0.000285 | lr:0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[82/100] | loss train:0.005010, test:0.000292 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.004888, test:0.000284 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.005327, test:0.000262 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.004839, test:0.000265 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.005105, test:0.000282 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.007856, test:0.000279 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.007017, test:0.000278 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.004996, test:0.000272 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.010338, test:0.000307 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.005341, test:0.000293 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.006618, test:0.000263 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.005105, test:0.000278 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.005358, test:0.000261 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.004733, test:0.000315 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.005018, test:0.000278 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.005236, test:0.000284 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.004987, test:0.000267 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.005182, test:0.000259 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.004792, test:0.000255 | lr:0.000100\n",
      "Mean absolute error:  1.6881596999941313\n",
      "Root mean squared error:  5.766498613099253\n"
     ]
    }
   ],
   "source": [
    "# Univariate Monte Carlo, no early stopping\n",
    "\n",
    "uni_absolute1 = []\n",
    "uni_root1 = []\n",
    "\n",
    "for i in range(n):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "    model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "\n",
    "    for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "        loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "        loss_val, lr_val = run_epoch(val_dataloader)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch[{}/{}] | loss train:{:.6f}, test:{:.6f} | lr:{:.6f}'\n",
    "                  .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, loss_val, lr_train))\n",
    "        \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predicted_train = np.array([])\n",
    "\n",
    "    for idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        out = model(x)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        predicted_train = np.concatenate((predicted_train, out))\n",
    "\n",
    "    predicted_val = np.array([])\n",
    "\n",
    "    for idx, (x, y) in enumerate(val_dataloader):\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        out = model(x)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        predicted_val = np.concatenate((predicted_val, out))\n",
    "\n",
    "    data_y_train_pred = np.zeros(num_data_points)\n",
    "    data_y_val_pred = np.zeros(num_data_points)\n",
    "\n",
    "    data_y_train_pred[config[\"data\"][\"window_size\"]:split_index+config[\"data\"][\"window_size\"]] = scaler.inverse_transform(predicted_train)\n",
    "    data_y_val_pred[split_index+config[\"data\"][\"window_size\"]:] = scaler.inverse_transform(predicted_val)\n",
    "\n",
    "    mae = mean_absolute_error(close_price_data, data_y_train_pred + data_y_val_pred)\n",
    "    print(\"Mean absolute error: \", mae)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(close_price_data, data_y_train_pred+data_y_val_pred))\n",
    "    print(\"Root mean squared error: \", rmse)\n",
    "    \n",
    "    uni_absolute1.append(mae)\n",
    "    uni_root1.append(rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b03675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 30.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mean absolute error, no early stop\n",
    "\n",
    "mean = sum(uni_absolute1) / len(uni_absolute1)\n",
    "variance = sum([((x - mean) ** 2) for x in uni_absolute1]) / len(uni_absolute1)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=uni_absolute1, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Univariate Mean Absolute Error, No Early Stop')\n",
    "maxfreq = m.max()\n",
    "plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}$'.format(mean,sd))\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe832129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 30.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Root mean squared error, no early stop\n",
    "\n",
    "mean = sum(uni_root1) / len(uni_root1)\n",
    "variance = sum([((x - mean) ** 2) for x in uni_root1]) / len(uni_root1)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=uni_root1, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Univariate Root Mean Squared Error, No Early Stop')\n",
    "plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}$'.format(mean,sd))\n",
    "maxfreq = m.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a627e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.059101, test:0.000867 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011438, test:0.000393 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011470, test:0.000549 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012438, test:0.001946 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010105, test:0.000831 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014316, test:0.002139 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008711, test:0.001474 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009950, test:0.002642 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010784, test:0.000523 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007983, test:0.003991 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009023, test:0.001494 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007895, test:0.004613 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.15121168795208\n",
      "Root mean squared error:  10.046701389649245\n",
      "Epoch[1/100] | loss train:0.069372, test:0.001861 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013699, test:0.001561 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010665, test:0.000640 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009549, test:0.000288 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009059, test:0.002450 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010015, test:0.000755 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009220, test:0.000926 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012708, test:0.003666 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009628, test:0.000965 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008153, test:0.001329 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007968, test:0.000601 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009474, test:0.000400 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007553, test:0.001216 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008462, test:0.000833 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.178549665483762\n",
      "Root mean squared error:  7.122646956259671\n",
      "Epoch[1/100] | loss train:0.054961, test:0.000520 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013364, test:0.001175 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011299, test:0.001054 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011421, test:0.000460 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008771, test:0.001300 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009068, test:0.000521 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008877, test:0.001390 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009310, test:0.000392 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008744, test:0.001169 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008770, test:0.000533 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007292, test:0.001773 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007511, test:0.000706 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008439, test:0.000864 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008306, test:0.000475 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010752, test:0.000797 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007016, test:0.003640 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007888, test:0.000810 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010909, test:0.000650 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.879279152831217\n",
      "Root mean squared error:  6.593955553662186\n",
      "Epoch[1/100] | loss train:0.061261, test:0.000595 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016249, test:0.003265 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010884, test:0.001934 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008657, test:0.001713 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011166, test:0.000289 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009571, test:0.002126 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008299, test:0.002277 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009031, test:0.004993 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008544, test:0.000598 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007478, test:0.000372 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008922, test:0.001062 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008021, test:0.001806 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008138, test:0.000424 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010268, test:0.002546 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007668, test:0.000418 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.419542128151605\n",
      "Root mean squared error:  6.054082558218124\n",
      "Epoch[1/100] | loss train:0.046181, test:0.003941 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014525, test:0.000604 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009850, test:0.003860 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011233, test:0.003477 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009866, test:0.007506 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008880, test:0.000290 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013143, test:0.001143 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010867, test:0.002022 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007753, test:0.005504 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011464, test:0.000568 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012585, test:0.000322 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008277, test:0.000603 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009071, test:0.002324 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008885, test:0.000326 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007423, test:0.000845 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008807, test:0.004429 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  8.905930667134088\n",
      "Root mean squared error:  11.954177115781754\n",
      "Epoch[1/100] | loss train:0.054646, test:0.002735 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019574, test:0.004284 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012881, test:0.002364 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009666, test:0.000653 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011359, test:0.006920 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010511, test:0.000312 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009940, test:0.000309 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008260, test:0.004889 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009565, test:0.000344 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011312, test:0.003359 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009653, test:0.000467 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009080, test:0.000398 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009093, test:0.001780 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009465, test:0.001649 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008155, test:0.000656 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007613, test:0.000456 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008263, test:0.000421 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.260845523114961\n",
      "Root mean squared error:  8.407541388286274\n",
      "Epoch[1/100] | loss train:0.059429, test:0.000722 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014481, test:0.000430 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010925, test:0.000603 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010979, test:0.001085 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010244, test:0.010873 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009377, test:0.000490 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010546, test:0.003296 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012025, test:0.005123 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008626, test:0.000427 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008924, test:0.002452 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008097, test:0.000401 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013477, test:0.000734 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008151, test:0.000418 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010387, test:0.000549 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008251, test:0.000469 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008249, test:0.001203 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009366, test:0.000708 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009656, test:0.000545 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009037, test:0.000447 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009454, test:0.000544 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007753, test:0.002502 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.172290085223783\n",
      "Root mean squared error:  8.355407775144862\n",
      "Epoch[1/100] | loss train:0.042457, test:0.001695 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012859, test:0.010457 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011807, test:0.004218 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009349, test:0.000723 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/100] | loss train:0.010872, test:0.002925 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008187, test:0.000301 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010462, test:0.000510 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007753, test:0.000372 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010276, test:0.002770 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008151, test:0.001448 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006962, test:0.001244 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009447, test:0.001604 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007538, test:0.000468 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007805, test:0.000371 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008634, test:0.000365 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007239, test:0.000354 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.0074090697954183\n",
      "Root mean squared error:  6.290431727865969\n",
      "Epoch[1/100] | loss train:0.048171, test:0.000402 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010683, test:0.001067 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009753, test:0.000568 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010342, test:0.000752 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008405, test:0.005685 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010430, test:0.002817 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008759, test:0.002653 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008677, test:0.001380 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010439, test:0.001548 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007165, test:0.000539 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007797, test:0.001562 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.218072902072163\n",
      "Root mean squared error:  7.343950601655562\n",
      "Epoch[1/100] | loss train:0.083522, test:0.002259 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011048, test:0.002715 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011125, test:0.001702 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009809, test:0.000387 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009341, test:0.000925 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008247, test:0.001242 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009296, test:0.003097 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007961, test:0.000654 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008137, test:0.000523 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009893, test:0.000328 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008299, test:0.000682 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007871, test:0.002185 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008179, test:0.000721 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007500, test:0.001585 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008877, test:0.000444 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010456, test:0.001754 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008310, test:0.001347 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008166, test:0.000394 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007560, test:0.000819 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008414, test:0.002245 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.559001894546663\n",
      "Root mean squared error:  8.928396578678022\n",
      "Epoch[1/100] | loss train:0.074937, test:0.001214 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012394, test:0.004237 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012829, test:0.000295 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015671, test:0.004975 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011432, test:0.000353 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009496, test:0.003429 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009133, test:0.001041 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009552, test:0.001447 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009309, test:0.001126 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008450, test:0.000423 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007643, test:0.000447 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009057, test:0.001441 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007539, test:0.001123 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.294425602068397\n",
      "Root mean squared error:  7.169533307976869\n",
      "Epoch[1/100] | loss train:0.044784, test:0.004634 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011234, test:0.000380 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008982, test:0.004917 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.026347, test:0.002108 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010902, test:0.000470 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007533, test:0.000357 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011825, test:0.000597 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009237, test:0.008686 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016491, test:0.000657 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008908, test:0.000337 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009676, test:0.003526 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012989, test:0.000873 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013214, test:0.000680 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009576, test:0.007904 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012474, test:0.001042 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007219, test:0.000723 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010305, test:0.000635 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009470, test:0.004689 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008680, test:0.003181 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013172, test:0.004944 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.095751124852137\n",
      "Root mean squared error:  11.116620836884737\n",
      "Epoch[1/100] | loss train:0.055937, test:0.002299 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010852, test:0.002751 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011245, test:0.000359 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011235, test:0.003980 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010304, test:0.001416 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009390, test:0.002928 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009992, test:0.000432 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008993, test:0.000424 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010287, test:0.000286 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009705, test:0.003605 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007728, test:0.001583 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008890, test:0.001865 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007560, test:0.002347 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007117, test:0.001092 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007284, test:0.000528 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007713, test:0.002451 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006498, test:0.000895 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007252, test:0.000456 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008959, test:0.000839 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.603276119818087\n",
      "Root mean squared error:  8.128725075660226\n",
      "Epoch[1/100] | loss train:0.043694, test:0.000543 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010806, test:0.002245 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010004, test:0.008354 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010682, test:0.000849 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007927, test:0.000716 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008251, test:0.000853 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011858, test:0.003801 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009338, test:0.001796 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009546, test:0.000479 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007528, test:0.000600 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008710, test:0.005339 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008880, test:0.000354 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007669, test:0.004862 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010624, test:0.000369 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007335, test:0.000472 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012442, test:0.000972 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008489, test:0.001025 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007280, test:0.000974 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008058, test:0.000556 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006636, test:0.001471 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008468, test:0.006068 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008051, test:0.001774 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.732921152423761\n",
      "Root mean squared error:  7.8968136816993075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.063507, test:0.007582 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015796, test:0.001527 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010408, test:0.004247 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012790, test:0.003768 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009981, test:0.000660 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009066, test:0.000400 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008976, test:0.000748 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008519, test:0.000336 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007468, test:0.001134 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008318, test:0.004141 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008487, test:0.004088 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008208, test:0.000706 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007880, test:0.004659 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008878, test:0.001040 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008165, test:0.001395 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009194, test:0.000970 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008362, test:0.002852 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007780, test:0.000746 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.511824277620489\n",
      "Root mean squared error:  7.770324735714912\n",
      "Epoch[1/100] | loss train:0.058781, test:0.008265 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014365, test:0.000528 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010352, test:0.002524 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012148, test:0.001367 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011611, test:0.017634 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011783, test:0.000631 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008790, test:0.002595 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007605, test:0.000369 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007908, test:0.001951 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010430, test:0.003619 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009315, test:0.001406 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008063, test:0.000707 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008564, test:0.000376 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013353, test:0.001149 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009220, test:0.000707 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007090, test:0.000537 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007598, test:0.003415 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008863, test:0.003103 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.527443137939799\n",
      "Root mean squared error:  9.367675109608795\n",
      "Epoch[1/100] | loss train:0.056776, test:0.001335 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012047, test:0.001288 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009211, test:0.001001 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008858, test:0.003274 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009952, test:0.001116 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008761, test:0.000358 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007514, test:0.000736 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009240, test:0.002368 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008886, test:0.004873 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008408, test:0.000402 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013359, test:0.000420 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008682, test:0.001815 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007275, test:0.002047 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007744, test:0.002255 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007656, test:0.001121 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008080, test:0.000794 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.9900823390247195\n",
      "Root mean squared error:  8.926337362770598\n",
      "Epoch[1/100] | loss train:0.068733, test:0.004622 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016015, test:0.000465 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008970, test:0.000646 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009121, test:0.001168 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010712, test:0.000711 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009162, test:0.000560 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007255, test:0.000981 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011178, test:0.000333 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010013, test:0.002060 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008390, test:0.004241 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008990, test:0.000396 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008441, test:0.002727 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007893, test:0.000436 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006893, test:0.002788 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008652, test:0.000355 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009199, test:0.000745 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007779, test:0.000581 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007725, test:0.003129 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.243005537351423\n",
      "Root mean squared error:  10.050830066943101\n",
      "Epoch[1/100] | loss train:0.067597, test:0.002101 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012061, test:0.000472 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009176, test:0.003251 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011732, test:0.000464 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010876, test:0.001222 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011621, test:0.002413 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010736, test:0.000374 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007789, test:0.000444 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007157, test:0.002162 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007345, test:0.000337 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015096, test:0.001334 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009594, test:0.000455 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007311, test:0.000608 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008096, test:0.000396 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.018075, test:0.000854 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008560, test:0.000400 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008289, test:0.000614 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006963, test:0.004807 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007943, test:0.000431 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009694, test:0.000378 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.6846088140068316\n",
      "Root mean squared error:  6.259368043817664\n",
      "Epoch[1/100] | loss train:0.069440, test:0.001694 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014498, test:0.000808 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011467, test:0.000546 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009560, test:0.004821 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009756, test:0.000824 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012100, test:0.006535 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009793, test:0.000594 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009232, test:0.002221 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014369, test:0.002592 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012029, test:0.003318 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012602, test:0.011926 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011039, test:0.000408 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008744, test:0.003550 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008156, test:0.000898 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010115, test:0.000657 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008904, test:0.001628 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007591, test:0.001763 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.022921, test:0.000446 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008751, test:0.000691 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008405, test:0.001954 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008032, test:0.003360 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007993, test:0.002587 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.708826931892892\n",
      "Root mean squared error:  9.306330482845805\n",
      "Epoch[1/100] | loss train:0.051458, test:0.001582 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009877, test:0.000416 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009943, test:0.013338 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010830, test:0.001975 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008160, test:0.001002 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008590, test:0.000900 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.009332, test:0.000882 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008318, test:0.000377 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009039, test:0.007277 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008269, test:0.000764 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008926, test:0.000558 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008304, test:0.002184 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007392, test:0.000707 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009348, test:0.000808 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008366, test:0.000499 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008684, test:0.000432 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008594, test:0.001337 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007999, test:0.000294 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007867, test:0.000350 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007538, test:0.000816 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006846, test:0.000858 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007631, test:0.000504 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007214, test:0.000907 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006599, test:0.000948 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006451, test:0.001298 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008034, test:0.004045 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006593, test:0.000472 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008077, test:0.000435 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.934214948349681\n",
      "Root mean squared error:  6.309072875239471\n",
      "Epoch[1/100] | loss train:0.050336, test:0.004633 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013271, test:0.001157 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011124, test:0.006271 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008847, test:0.006545 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011959, test:0.000354 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009669, test:0.000321 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007847, test:0.000325 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008595, test:0.021982 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010701, test:0.005029 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010791, test:0.003354 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007626, test:0.000478 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008532, test:0.001941 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010152, test:0.004150 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009341, test:0.000436 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007735, test:0.000342 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008028, test:0.001072 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.6642723208455696\n",
      "Root mean squared error:  6.879118771378111\n",
      "Epoch[1/100] | loss train:0.058428, test:0.000384 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011535, test:0.003097 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011737, test:0.000402 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010887, test:0.000332 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009253, test:0.001728 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007980, test:0.001082 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009154, test:0.008506 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007995, test:0.000947 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008472, test:0.000411 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008311, test:0.000353 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007919, test:0.006118 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008544, test:0.000331 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009542, test:0.000708 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008038, test:0.001094 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009128, test:0.001098 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009810, test:0.001176 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007965, test:0.000372 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007924, test:0.000441 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007705, test:0.000598 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007138, test:0.000356 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007214, test:0.000766 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007460, test:0.004593 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.738526524169828\n",
      "Root mean squared error:  10.300055436710634\n",
      "Epoch[1/100] | loss train:0.066772, test:0.002153 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011655, test:0.001788 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010772, test:0.000321 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010042, test:0.000328 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009660, test:0.001144 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007649, test:0.001208 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010349, test:0.001029 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009019, test:0.000345 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009112, test:0.001589 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008739, test:0.000421 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008609, test:0.000478 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007888, test:0.000789 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008356, test:0.001107 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.723675678071716\n",
      "Root mean squared error:  7.73871268240147\n",
      "Epoch[1/100] | loss train:0.062465, test:0.000395 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010550, test:0.000567 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012015, test:0.000355 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010786, test:0.001779 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009161, test:0.000517 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009584, test:0.001438 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011345, test:0.001163 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012791, test:0.001732 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008867, test:0.001686 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007447, test:0.001954 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011315, test:0.000738 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008046, test:0.000415 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007026, test:0.002231 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.8722604427597975\n",
      "Root mean squared error:  8.00901278511961\n",
      "Epoch[1/100] | loss train:0.061073, test:0.002575 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011859, test:0.005077 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010661, test:0.001331 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008294, test:0.002860 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008601, test:0.003259 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008164, test:0.002403 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010782, test:0.006974 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009888, test:0.000735 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008766, test:0.001459 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007780, test:0.000962 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008106, test:0.001526 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007392, test:0.001164 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007522, test:0.000559 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013954, test:0.002732 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012724, test:0.001643 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008806, test:0.001945 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008541, test:0.000380 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009711, test:0.003647 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012924, test:0.001784 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007890, test:0.000793 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007259, test:0.001042 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007873, test:0.001398 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008629, test:0.001469 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008425, test:0.000694 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006981, test:0.000397 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006630, test:0.000331 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010182, test:0.003475 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010627, test:0.000829 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011192, test:0.005881 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007858, test:0.000585 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007290, test:0.000364 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007104, test:0.000879 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008379, test:0.001485 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006713, test:0.002643 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/100] | loss train:0.007479, test:0.000518 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007059, test:0.000716 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.2028520155041305\n",
      "Root mean squared error:  6.660167347743126\n",
      "Epoch[1/100] | loss train:0.056468, test:0.000383 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010668, test:0.000608 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010334, test:0.000437 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007939, test:0.000357 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008201, test:0.001915 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008950, test:0.000381 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008552, test:0.003287 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009384, test:0.010486 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009458, test:0.000311 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007121, test:0.004388 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008251, test:0.000641 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007562, test:0.004246 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008469, test:0.000543 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007665, test:0.000997 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008324, test:0.001546 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013917, test:0.000476 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007828, test:0.000604 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008141, test:0.003895 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006997, test:0.000617 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.320652895297523\n",
      "Root mean squared error:  7.138858546104151\n",
      "Epoch[1/100] | loss train:0.046324, test:0.005828 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010472, test:0.000749 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009316, test:0.001049 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008864, test:0.000805 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008932, test:0.000777 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009055, test:0.000664 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009231, test:0.000709 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009017, test:0.000433 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008939, test:0.000847 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011954, test:0.002741 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008340, test:0.000496 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007703, test:0.000915 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013716, test:0.003702 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008617, test:0.000452 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007619, test:0.000409 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008321, test:0.000462 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010868, test:0.004488 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007457, test:0.000611 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006741, test:0.002196 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008340, test:0.000532 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007959, test:0.000524 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007065, test:0.003175 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007866, test:0.000746 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007872, test:0.001782 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007684, test:0.001565 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.9999330066600702\n",
      "Root mean squared error:  7.154629983321642\n",
      "Epoch[1/100] | loss train:0.043624, test:0.001691 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014643, test:0.000303 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008897, test:0.003786 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009714, test:0.000298 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008399, test:0.000420 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008084, test:0.001813 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008453, test:0.000820 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009401, test:0.003121 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011131, test:0.001491 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009036, test:0.000828 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007871, test:0.000407 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007808, test:0.000403 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008558, test:0.002579 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007882, test:0.000770 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.335690644612327\n",
      "Root mean squared error:  8.179247988174703\n",
      "Epoch[1/100] | loss train:0.063094, test:0.000540 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012536, test:0.000446 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019751, test:0.002086 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016445, test:0.001302 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011273, test:0.000791 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009276, test:0.000453 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008235, test:0.000319 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009182, test:0.001592 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007747, test:0.000292 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007912, test:0.000447 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009384, test:0.003494 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007127, test:0.001057 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013804, test:0.000789 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008420, test:0.001237 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008357, test:0.004743 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007668, test:0.004905 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008885, test:0.000447 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007286, test:0.001577 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006922, test:0.000332 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.9016878750736974\n",
      "Root mean squared error:  6.855055658383245\n",
      "Epoch[1/100] | loss train:0.061099, test:0.000987 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012707, test:0.002629 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009982, test:0.000350 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008768, test:0.001217 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008320, test:0.000301 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008286, test:0.003243 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007710, test:0.000333 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008194, test:0.000358 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009458, test:0.000532 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016939, test:0.003529 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012113, test:0.000564 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007226, test:0.000426 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009578, test:0.000363 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008134, test:0.000680 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008753, test:0.000520 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.5508097927654796\n",
      "Root mean squared error:  6.922284984181126\n",
      "Epoch[1/100] | loss train:0.063163, test:0.002584 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012165, test:0.000432 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014241, test:0.001124 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010127, test:0.000942 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009251, test:0.000306 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007398, test:0.000362 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010006, test:0.000341 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007967, test:0.000987 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010328, test:0.000347 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010290, test:0.001322 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008291, test:0.000726 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008418, test:0.000737 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008470, test:0.001382 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009294, test:0.001451 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010411, test:0.007374 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  7.852639027164414\n",
      "Root mean squared error:  13.6816896035583\n",
      "Epoch[1/100] | loss train:0.072988, test:0.001496 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013781, test:0.000355 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010338, test:0.000960 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010675, test:0.000367 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011335, test:0.001280 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008289, test:0.005624 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010983, test:0.009207 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008374, test:0.001047 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/100] | loss train:0.008102, test:0.000899 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007982, test:0.000340 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008513, test:0.000400 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008538, test:0.000940 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006985, test:0.002248 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008904, test:0.000543 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007811, test:0.002969 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007207, test:0.001647 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007149, test:0.001775 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007820, test:0.000381 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007812, test:0.000986 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007247, test:0.001404 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.54675181292271\n",
      "Root mean squared error:  7.125554466399126\n",
      "Epoch[1/100] | loss train:0.049426, test:0.005013 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011024, test:0.000349 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009346, test:0.000905 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008146, test:0.000491 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009173, test:0.001833 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008446, test:0.002634 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008737, test:0.001041 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008058, test:0.000333 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007794, test:0.000367 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007977, test:0.000470 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008615, test:0.000380 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007953, test:0.001193 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007260, test:0.005297 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007836, test:0.000448 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008259, test:0.003807 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006634, test:0.000725 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014984, test:0.000567 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007221, test:0.000555 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.503260519583694\n",
      "Root mean squared error:  6.370142593024625\n",
      "Epoch[1/100] | loss train:0.065062, test:0.002943 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012799, test:0.002677 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011557, test:0.000374 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010184, test:0.002856 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010618, test:0.001293 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012015, test:0.001049 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007911, test:0.001035 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010779, test:0.003567 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007867, test:0.002175 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010491, test:0.008428 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009706, test:0.001409 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007441, test:0.009259 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008105, test:0.000820 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.4447919234227986\n",
      "Root mean squared error:  6.404015457821231\n",
      "Epoch[1/100] | loss train:0.049425, test:0.002440 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011229, test:0.002770 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012343, test:0.001700 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011719, test:0.005680 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011938, test:0.005810 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008456, test:0.003090 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009015, test:0.002386 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008445, test:0.000366 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012091, test:0.001015 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008033, test:0.000841 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007905, test:0.000744 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008762, test:0.000446 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009820, test:0.002085 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009437, test:0.000771 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008440, test:0.001238 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006901, test:0.000577 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008022, test:0.000533 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008260, test:0.001437 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.6800474249425816\n",
      "Root mean squared error:  7.2719775803316224\n",
      "Epoch[1/100] | loss train:0.055019, test:0.001969 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012775, test:0.000522 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010478, test:0.000422 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008525, test:0.000321 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009424, test:0.000565 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010678, test:0.003146 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008349, test:0.001083 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008428, test:0.000812 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011955, test:0.001189 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008821, test:0.000387 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007843, test:0.000647 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010078, test:0.004060 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009156, test:0.000580 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006602, test:0.001028 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.0975447036732486\n",
      "Root mean squared error:  6.761653025608369\n",
      "Epoch[1/100] | loss train:0.080734, test:0.000847 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013378, test:0.006031 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012550, test:0.000724 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009551, test:0.000454 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008561, test:0.000597 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009005, test:0.001408 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010601, test:0.000389 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009041, test:0.004141 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008048, test:0.000328 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008013, test:0.003771 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009322, test:0.004280 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008935, test:0.003596 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009352, test:0.000442 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007377, test:0.002154 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009020, test:0.004284 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010543, test:0.000408 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007563, test:0.001350 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006842, test:0.001137 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009102, test:0.001097 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  7.297961442523868\n",
      "Root mean squared error:  9.48221383269633\n",
      "Epoch[1/100] | loss train:0.062560, test:0.008433 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017769, test:0.000747 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012265, test:0.001568 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009111, test:0.000733 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014209, test:0.009499 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010088, test:0.001644 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009389, test:0.000964 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007777, test:0.000345 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014803, test:0.002986 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010565, test:0.001512 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015698, test:0.000431 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009884, test:0.002233 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007995, test:0.002367 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008433, test:0.001065 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006767, test:0.001365 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008815, test:0.000496 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008427, test:0.000996 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007624, test:0.000841 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.7243567448650765\n",
      "Root mean squared error:  7.1023012590502566\n",
      "Epoch[1/100] | loss train:0.060257, test:0.001277 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013622, test:0.005160 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012311, test:0.007937 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010228, test:0.011586 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008844, test:0.000411 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6/100] | loss train:0.009005, test:0.004960 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008565, test:0.000695 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009307, test:0.000329 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015181, test:0.002075 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008791, test:0.000289 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007759, test:0.000538 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010949, test:0.000614 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007425, test:0.002161 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007372, test:0.000893 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008523, test:0.000330 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007971, test:0.000722 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007705, test:0.001831 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007426, test:0.000407 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008911, test:0.000505 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008418, test:0.001684 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.171883474644875\n",
      "Root mean squared error:  8.64358541857353\n",
      "Epoch[1/100] | loss train:0.068245, test:0.000518 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013357, test:0.005972 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010640, test:0.007214 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012324, test:0.001092 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010580, test:0.000441 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008948, test:0.002392 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007969, test:0.000400 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010944, test:0.004227 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010362, test:0.000386 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012073, test:0.000375 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010320, test:0.001396 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008998, test:0.011260 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009964, test:0.000317 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009030, test:0.000432 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007419, test:0.000469 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007185, test:0.001518 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007574, test:0.001735 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008135, test:0.001481 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008626, test:0.003780 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006911, test:0.000771 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008855, test:0.000383 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008598, test:0.001066 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009321, test:0.002727 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.725776509590751\n",
      "Root mean squared error:  9.722654145869878\n",
      "Epoch[1/100] | loss train:0.065433, test:0.000414 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012353, test:0.003159 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008758, test:0.000562 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009439, test:0.002097 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009389, test:0.000344 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011277, test:0.000948 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008610, test:0.000493 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009648, test:0.000471 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009289, test:0.000924 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009604, test:0.000493 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009742, test:0.003334 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009540, test:0.000924 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008673, test:0.000380 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009044, test:0.000489 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007726, test:0.000531 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.683043603636471\n",
      "Root mean squared error:  6.438790035076008\n",
      "Epoch[1/100] | loss train:0.069159, test:0.002066 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012420, test:0.000353 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009972, test:0.002978 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010118, test:0.000523 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009120, test:0.000870 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009425, test:0.001228 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009311, test:0.002197 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010664, test:0.000324 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007883, test:0.001809 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008429, test:0.001385 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007852, test:0.000487 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007616, test:0.000593 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010526, test:0.002616 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009462, test:0.002668 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011806, test:0.000487 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008578, test:0.000443 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007527, test:0.001633 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008270, test:0.000585 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.448915942295178\n",
      "Root mean squared error:  6.629509461005403\n",
      "Epoch[1/100] | loss train:0.059345, test:0.000487 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012483, test:0.002288 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012770, test:0.000451 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008079, test:0.000539 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008828, test:0.002997 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008253, test:0.004389 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009556, test:0.007323 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009330, test:0.000330 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008664, test:0.000313 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011685, test:0.000873 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008351, test:0.000414 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007468, test:0.001087 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007670, test:0.005281 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010982, test:0.000742 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007630, test:0.001347 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009822, test:0.002782 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008328, test:0.000358 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009257, test:0.001143 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009598, test:0.005099 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  8.161309577348757\n",
      "Root mean squared error:  11.755103710396712\n",
      "Epoch[1/100] | loss train:0.060119, test:0.001240 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011672, test:0.000455 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012816, test:0.000913 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011002, test:0.000464 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010778, test:0.001400 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009994, test:0.001152 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011367, test:0.000708 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010614, test:0.001691 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007749, test:0.000731 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009175, test:0.003468 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.016326, test:0.000390 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009006, test:0.005155 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008555, test:0.000379 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008673, test:0.001466 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007669, test:0.000714 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008199, test:0.000962 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007647, test:0.000555 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006865, test:0.000371 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008238, test:0.001973 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008881, test:0.000416 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007862, test:0.002475 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008923, test:0.001238 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007294, test:0.000381 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007412, test:0.001063 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007696, test:0.001179 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007131, test:0.001109 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007366, test:0.000787 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007062, test:0.000635 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.335153699250517\n",
      "Root mean squared error:  6.312479167770844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.048372, test:0.006026 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012297, test:0.000444 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009132, test:0.000318 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009713, test:0.000883 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009435, test:0.000579 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008607, test:0.002797 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008713, test:0.001337 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009525, test:0.003690 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008561, test:0.000359 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009068, test:0.000723 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006838, test:0.002656 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008767, test:0.001936 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009074, test:0.002715 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.204365324612926\n",
      "Root mean squared error:  9.017772499779358\n",
      "Epoch[1/100] | loss train:0.072146, test:0.001602 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013900, test:0.001982 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010169, test:0.003279 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010886, test:0.001467 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012845, test:0.000463 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009782, test:0.002264 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009168, test:0.003014 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008923, test:0.001285 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011812, test:0.001486 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008534, test:0.000425 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008619, test:0.000378 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008034, test:0.000833 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008128, test:0.000491 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008050, test:0.001453 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008731, test:0.002226 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008356, test:0.001396 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008098, test:0.001189 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006860, test:0.000599 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007752, test:0.001886 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006886, test:0.004153 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007820, test:0.001464 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.0963111960189105\n",
      "Root mean squared error:  8.149069146121278\n",
      "Epoch[1/100] | loss train:0.050402, test:0.001015 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016174, test:0.004107 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011225, test:0.004269 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011140, test:0.000789 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009711, test:0.000829 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009944, test:0.001135 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.019341, test:0.001848 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009875, test:0.000479 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008988, test:0.000398 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009227, test:0.001938 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009068, test:0.001476 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008516, test:0.000295 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008027, test:0.002646 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008434, test:0.002944 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008812, test:0.000396 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008954, test:0.000599 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008561, test:0.000415 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007781, test:0.002938 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013187, test:0.003738 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009936, test:0.000637 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007899, test:0.002130 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009008, test:0.001019 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.28981360498739\n",
      "Root mean squared error:  7.878198980594729\n",
      "Epoch[1/100] | loss train:0.063931, test:0.000954 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010640, test:0.005376 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010979, test:0.000317 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010208, test:0.005255 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008353, test:0.000442 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007958, test:0.000352 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008398, test:0.001000 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009546, test:0.000405 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010406, test:0.000752 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009515, test:0.000631 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008103, test:0.000350 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007774, test:0.001048 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010219, test:0.000753 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.9519004471609063\n",
      "Root mean squared error:  7.3092681404607305\n",
      "Epoch[1/100] | loss train:0.055544, test:0.002189 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011769, test:0.000398 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009964, test:0.009717 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010709, test:0.000606 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008567, test:0.000956 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009074, test:0.001659 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009871, test:0.000436 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011745, test:0.001257 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009238, test:0.000316 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008457, test:0.000313 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007367, test:0.000371 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011812, test:0.000425 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008876, test:0.000979 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009066, test:0.000361 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006871, test:0.000532 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007777, test:0.001625 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009076, test:0.005175 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007866, test:0.000359 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009072, test:0.000761 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007299, test:0.000375 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.011525192674036\n",
      "Root mean squared error:  6.289059749817215\n",
      "Epoch[1/100] | loss train:0.061455, test:0.002138 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013322, test:0.004769 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011522, test:0.000390 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011941, test:0.000402 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010229, test:0.000293 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009750, test:0.000541 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009281, test:0.000402 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008325, test:0.000487 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008184, test:0.000372 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009324, test:0.004997 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009972, test:0.000801 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007951, test:0.000476 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009231, test:0.000857 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009062, test:0.001230 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008888, test:0.000387 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.400427074794024\n",
      "Root mean squared error:  6.479570689680242\n",
      "Epoch[1/100] | loss train:0.050178, test:0.003503 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010430, test:0.000972 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010196, test:0.008868 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010595, test:0.000417 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008307, test:0.000365 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009725, test:0.000588 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010095, test:0.001698 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007655, test:0.000442 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008643, test:0.001022 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008551, test:0.003712 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007586, test:0.000382 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007653, test:0.002548 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009751, test:0.002433 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009523, test:0.001818 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/100] | loss train:0.008227, test:0.000585 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.00712350533681\n",
      "Root mean squared error:  7.826636152706649\n",
      "Epoch[1/100] | loss train:0.050338, test:0.004590 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012749, test:0.000780 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012256, test:0.006298 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011328, test:0.001244 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008965, test:0.002496 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008638, test:0.000728 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009853, test:0.000876 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009399, test:0.002471 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009073, test:0.001268 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008743, test:0.004855 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009001, test:0.002155 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008473, test:0.000513 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008355, test:0.000971 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007415, test:0.001103 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008367, test:0.001279 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007409, test:0.000386 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006946, test:0.004472 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010265, test:0.000740 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007045, test:0.000965 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008254, test:0.000701 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014144, test:0.000911 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009304, test:0.001256 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007757, test:0.000703 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008155, test:0.000379 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008070, test:0.001803 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007391, test:0.000518 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007724, test:0.000382 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007547, test:0.001265 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006928, test:0.002613 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007729, test:0.000386 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007128, test:0.004746 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009614, test:0.002310 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007424, test:0.000367 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006865, test:0.000470 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006864, test:0.001105 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009302, test:0.000458 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006446, test:0.000348 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006462, test:0.000325 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007970, test:0.000686 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007344, test:0.000542 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005447, test:0.000401 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005291, test:0.000392 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004872, test:0.000353 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005292, test:0.000322 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004690, test:0.000325 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005022, test:0.000474 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005024, test:0.000472 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005086, test:0.000394 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004846, test:0.000605 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005122, test:0.000361 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004817, test:0.000328 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004995, test:0.000410 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005626, test:0.000384 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.011370, test:0.000422 | lr:0.001000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.032282725338248\n",
      "Root mean squared error:  5.95853213071525\n",
      "Epoch[1/100] | loss train:0.051320, test:0.005254 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011058, test:0.010191 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010620, test:0.000461 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009526, test:0.000446 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008152, test:0.000940 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008778, test:0.001046 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008466, test:0.004991 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007547, test:0.000526 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008161, test:0.000889 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015498, test:0.000836 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009900, test:0.000375 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007501, test:0.000576 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007788, test:0.001547 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007770, test:0.001128 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008453, test:0.000382 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008603, test:0.001560 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008865, test:0.001088 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007803, test:0.001204 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009716, test:0.000490 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008771, test:0.000994 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006770, test:0.000623 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.5163785813056214\n",
      "Root mean squared error:  6.743649699872256\n",
      "Epoch[1/100] | loss train:0.058585, test:0.000749 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014053, test:0.001169 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010302, test:0.000335 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008347, test:0.000941 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010023, test:0.002988 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008662, test:0.002387 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009147, test:0.005914 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008544, test:0.001069 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009652, test:0.000336 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016233, test:0.008805 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011492, test:0.000512 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008002, test:0.002455 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010057, test:0.003096 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.880884842135513\n",
      "Root mean squared error:  9.715262384883959\n",
      "Epoch[1/100] | loss train:0.058522, test:0.000363 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011025, test:0.006116 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011069, test:0.000475 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012686, test:0.000344 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007789, test:0.001240 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010260, test:0.002411 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008975, test:0.000739 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007793, test:0.000584 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008301, test:0.002553 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007682, test:0.000401 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009180, test:0.001085 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009515, test:0.001876 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008613, test:0.000703 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007857, test:0.000305 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007352, test:0.000446 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008263, test:0.003736 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008535, test:0.000723 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008154, test:0.001183 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008383, test:0.000496 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006861, test:0.001448 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008164, test:0.000883 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008658, test:0.000391 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009023, test:0.002931 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007699, test:0.000532 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.2819392843096056\n",
      "Root mean squared error:  6.232254587367951\n",
      "Epoch[1/100] | loss train:0.065337, test:0.003525 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010830, test:0.000359 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010288, test:0.002898 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008794, test:0.000557 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012296, test:0.002158 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.019986, test:0.002789 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.010621, test:0.002744 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008034, test:0.000308 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008678, test:0.000907 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007928, test:0.000675 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007372, test:0.001103 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008667, test:0.000867 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008318, test:0.000392 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014989, test:0.008713 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009888, test:0.000339 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007610, test:0.009546 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008523, test:0.000381 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008242, test:0.002290 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.338335770486318\n",
      "Root mean squared error:  9.566661718217027\n",
      "Epoch[1/100] | loss train:0.054318, test:0.004593 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012104, test:0.002436 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013324, test:0.003948 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010118, test:0.000502 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011824, test:0.000779 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009052, test:0.003418 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008940, test:0.000574 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008170, test:0.001963 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009055, test:0.001482 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009379, test:0.000375 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009232, test:0.002631 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009166, test:0.001590 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008149, test:0.001129 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007662, test:0.007641 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011419, test:0.002121 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008547, test:0.000639 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010848, test:0.000345 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009219, test:0.001319 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008914, test:0.001815 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010304, test:0.002552 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006426, test:0.000328 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007331, test:0.000393 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009651, test:0.006438 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008822, test:0.001104 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009153, test:0.000357 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007344, test:0.002387 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010810, test:0.000602 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007048, test:0.000393 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006440, test:0.000464 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007337, test:0.000527 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006056, test:0.000778 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.411066303647748\n",
      "Root mean squared error:  6.344610497262951\n",
      "Epoch[1/100] | loss train:0.048892, test:0.000448 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011238, test:0.000526 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010837, test:0.001442 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011372, test:0.001070 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009817, test:0.013568 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009884, test:0.000358 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008013, test:0.000532 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008659, test:0.000610 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009130, test:0.001964 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007686, test:0.000620 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009827, test:0.001285 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008070, test:0.003340 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015319, test:0.000650 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011601, test:0.001972 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008588, test:0.001466 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008601, test:0.000356 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007675, test:0.000671 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007085, test:0.000790 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009686, test:0.000398 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007094, test:0.000815 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006980, test:0.001296 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007250, test:0.000386 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006136, test:0.001863 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007230, test:0.001562 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007846, test:0.000486 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006906, test:0.000333 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006536, test:0.000494 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007526, test:0.000465 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007524, test:0.000396 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006909, test:0.000482 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007122, test:0.000798 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007176, test:0.000312 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006396, test:0.001573 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007605, test:0.000569 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007265, test:0.000359 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007171, test:0.000571 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006215, test:0.001073 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006617, test:0.000314 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006778, test:0.000550 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007511, test:0.000949 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006069, test:0.000312 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005641, test:0.000440 | lr:0.001000\n",
      "Early stopping.\n",
      "Mean absolute error:  1.9497552380083585\n",
      "Root mean squared error:  5.97115005375976\n",
      "Epoch[1/100] | loss train:0.048785, test:0.001843 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015231, test:0.002003 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013326, test:0.001457 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009985, test:0.006370 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009565, test:0.001254 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008281, test:0.000557 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007687, test:0.000263 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008780, test:0.000616 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009540, test:0.009074 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010643, test:0.000522 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008654, test:0.003657 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007791, test:0.001195 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008574, test:0.010675 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009305, test:0.001055 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007169, test:0.000855 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007062, test:0.002273 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008830, test:0.000851 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.303231851741109\n",
      "Root mean squared error:  6.480279781073512\n",
      "Epoch[1/100] | loss train:0.054793, test:0.000555 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011820, test:0.000374 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011343, test:0.001427 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010014, test:0.000343 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008517, test:0.000643 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008459, test:0.002498 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008171, test:0.002819 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012651, test:0.003064 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009782, test:0.000323 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008951, test:0.002576 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009128, test:0.000502 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007366, test:0.000647 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010181, test:0.000901 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008202, test:0.000720 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010281, test:0.002446 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010487, test:0.000397 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007634, test:0.003603 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014172, test:0.002549 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010965, test:0.001205 | lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  3.376360942686083\n",
      "Root mean squared error:  7.325483819571186\n",
      "Epoch[1/100] | loss train:0.063001, test:0.000815 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013595, test:0.001907 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011681, test:0.000330 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009300, test:0.000989 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008817, test:0.002610 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008956, test:0.015670 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010120, test:0.005774 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009088, test:0.000333 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010055, test:0.000836 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006785, test:0.001111 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008888, test:0.002374 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007364, test:0.002031 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009505, test:0.000424 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.129300225859234\n",
      "Root mean squared error:  6.870962539365407\n",
      "Epoch[1/100] | loss train:0.048351, test:0.003136 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011493, test:0.010056 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011037, test:0.001619 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012304, test:0.000462 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011044, test:0.000606 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010493, test:0.002874 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009872, test:0.002099 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007110, test:0.000448 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007733, test:0.000436 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009279, test:0.000388 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007457, test:0.002869 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009523, test:0.000771 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007513, test:0.001835 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007718, test:0.002041 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007565, test:0.001250 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006308, test:0.000993 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007124, test:0.004861 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008093, test:0.000570 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007351, test:0.000368 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006913, test:0.000779 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008478, test:0.000419 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007331, test:0.000642 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006107, test:0.000545 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007659, test:0.000367 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006692, test:0.006947 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007344, test:0.001617 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006859, test:0.000529 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008124, test:0.000702 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009171, test:0.000471 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010222, test:0.000466 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007140, test:0.001703 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007343, test:0.002253 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007494, test:0.000507 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006753, test:0.004771 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.184999951686626\n",
      "Root mean squared error:  10.455528699695412\n",
      "Epoch[1/100] | loss train:0.072269, test:0.003294 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013749, test:0.002357 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011670, test:0.002650 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011271, test:0.000480 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009721, test:0.004676 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009078, test:0.000721 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007579, test:0.000382 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009859, test:0.001578 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008000, test:0.000498 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008077, test:0.000702 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007930, test:0.003049 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008067, test:0.001539 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009968, test:0.000980 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008898, test:0.001541 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006851, test:0.001977 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007547, test:0.004812 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009046, test:0.005240 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.7307459999878265\n",
      "Root mean squared error:  10.818714825592863\n",
      "Epoch[1/100] | loss train:0.042062, test:0.000713 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009457, test:0.010593 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009447, test:0.000687 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010656, test:0.000405 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008976, test:0.001350 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010927, test:0.000410 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008691, test:0.001719 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009448, test:0.000381 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008599, test:0.000513 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007256, test:0.000310 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007217, test:0.001951 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007459, test:0.000418 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009493, test:0.000745 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007298, test:0.000892 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008922, test:0.001920 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009839, test:0.001557 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007314, test:0.000341 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007073, test:0.000420 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006550, test:0.001279 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008757, test:0.001798 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.193540503404701\n",
      "Root mean squared error:  7.516076047106427\n",
      "Epoch[1/100] | loss train:0.056965, test:0.000548 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014388, test:0.001032 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014516, test:0.014191 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015926, test:0.003717 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009049, test:0.002107 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008746, test:0.002737 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008932, test:0.000405 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007660, test:0.001476 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008804, test:0.000492 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007801, test:0.001326 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009892, test:0.002576 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008404, test:0.002200 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008560, test:0.000833 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013257, test:0.002846 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008834, test:0.000417 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010130, test:0.000644 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007992, test:0.004815 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.252286275530912\n",
      "Root mean squared error:  10.644595024722493\n",
      "Epoch[1/100] | loss train:0.050385, test:0.001008 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011899, test:0.005176 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011208, test:0.000505 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011493, test:0.001570 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011124, test:0.001018 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010795, test:0.015456 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010072, test:0.000391 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007581, test:0.000375 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010681, test:0.006518 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016723, test:0.000687 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009702, test:0.000444 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007980, test:0.000900 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007718, test:0.001766 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008462, test:0.001275 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009671, test:0.005938 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009011, test:0.000972 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008117, test:0.000747 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[18/100] | loss train:0.013040, test:0.000758 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.961316518465653\n",
      "Root mean squared error:  7.589934472944089\n",
      "Epoch[1/100] | loss train:0.048784, test:0.001177 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011124, test:0.000356 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018742, test:0.001426 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011181, test:0.000300 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010200, test:0.005152 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008976, test:0.000937 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009927, test:0.006502 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010280, test:0.000387 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007976, test:0.006123 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007922, test:0.001636 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008883, test:0.000997 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009065, test:0.000879 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007799, test:0.001742 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007851, test:0.002652 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.334358915533293\n",
      "Root mean squared error:  8.235179274995794\n",
      "Epoch[1/100] | loss train:0.056082, test:0.000990 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013194, test:0.001035 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008732, test:0.000323 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011486, test:0.000349 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008278, test:0.001338 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009116, test:0.004606 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010086, test:0.000307 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007766, test:0.001850 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009456, test:0.005565 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011219, test:0.000348 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007809, test:0.001581 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008905, test:0.000463 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007281, test:0.000481 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007558, test:0.002661 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008227, test:0.000343 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007347, test:0.000868 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009668, test:0.000360 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.1817614188194057\n",
      "Root mean squared error:  6.016809408197667\n",
      "Epoch[1/100] | loss train:0.031642, test:0.002138 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019564, test:0.012361 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016333, test:0.003635 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016095, test:0.000751 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008644, test:0.001308 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009249, test:0.002707 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008473, test:0.005825 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011022, test:0.000417 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008527, test:0.000553 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007597, test:0.000457 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006924, test:0.009682 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008199, test:0.000432 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007152, test:0.000441 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009283, test:0.001517 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007438, test:0.001993 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007220, test:0.000557 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009199, test:0.000486 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007907, test:0.001381 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.661354786394184\n",
      "Root mean squared error:  7.497711616072018\n",
      "Epoch[1/100] | loss train:0.043329, test:0.000652 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010436, test:0.001005 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011677, test:0.000690 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008775, test:0.000532 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009572, test:0.001115 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009101, test:0.010742 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009780, test:0.000461 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009089, test:0.000530 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008844, test:0.000465 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016122, test:0.001979 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010189, test:0.001122 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009844, test:0.000834 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007765, test:0.005075 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009505, test:0.000974 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009007, test:0.002099 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008474, test:0.000907 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008173, test:0.000574 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.9488094630079233\n",
      "Root mean squared error:  7.1364529608783\n",
      "Epoch[1/100] | loss train:0.042792, test:0.002855 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010674, test:0.001299 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016723, test:0.007043 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008776, test:0.000434 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009617, test:0.001369 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009597, test:0.000934 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012802, test:0.003977 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008735, test:0.000564 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010020, test:0.007355 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008302, test:0.004586 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008834, test:0.000670 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007926, test:0.001234 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008344, test:0.001131 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007581, test:0.003205 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.813050342867532\n",
      "Root mean squared error:  10.059167366531492\n",
      "Epoch[1/100] | loss train:0.058725, test:0.002082 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010791, test:0.000412 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009504, test:0.000923 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010441, test:0.000359 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009349, test:0.001812 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008609, test:0.000386 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007352, test:0.010161 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009500, test:0.002048 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010620, test:0.000728 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008625, test:0.001193 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008122, test:0.004175 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007918, test:0.002957 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007277, test:0.001226 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007969, test:0.000838 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.388886925971136\n",
      "Root mean squared error:  7.278008177633415\n",
      "Epoch[1/100] | loss train:0.055830, test:0.000402 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015804, test:0.009680 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011248, test:0.005232 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010276, test:0.000272 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010432, test:0.000997 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009255, test:0.001883 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007615, test:0.000802 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009910, test:0.005054 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008681, test:0.005383 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007529, test:0.001563 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008303, test:0.000813 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008748, test:0.000405 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007490, test:0.001548 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007535, test:0.000873 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.59473551777762\n",
      "Root mean squared error:  6.549068282237122\n",
      "Epoch[1/100] | loss train:0.042184, test:0.000561 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010295, test:0.004716 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015242, test:0.000731 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009399, test:0.000370 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010089, test:0.001019 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008707, test:0.008305 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.011193, test:0.002723 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010242, test:0.000585 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008129, test:0.000374 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008099, test:0.001735 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014498, test:0.007593 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010090, test:0.000366 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006862, test:0.007795 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009093, test:0.002172 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008897, test:0.000432 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009873, test:0.000771 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008209, test:0.001488 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009072, test:0.000440 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007480, test:0.002444 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007308, test:0.000524 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007785, test:0.002214 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007853, test:0.005850 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  7.195628078165934\n",
      "Root mean squared error:  11.656761145834835\n",
      "Epoch[1/100] | loss train:0.051949, test:0.000834 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012693, test:0.008624 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009442, test:0.001588 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012942, test:0.001780 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008134, test:0.006240 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010247, test:0.004137 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012797, test:0.012910 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011124, test:0.001900 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006753, test:0.000869 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007385, test:0.000348 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007578, test:0.002821 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010998, test:0.003125 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009333, test:0.000598 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006671, test:0.000617 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008334, test:0.001057 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007331, test:0.000712 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010398, test:0.004428 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008786, test:0.003258 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010172, test:0.000464 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006473, test:0.000810 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.7022981043144196\n",
      "Root mean squared error:  6.4799451654059075\n",
      "Epoch[1/100] | loss train:0.043430, test:0.000908 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009584, test:0.005686 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010399, test:0.001108 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016835, test:0.015109 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013205, test:0.001246 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010281, test:0.001655 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013528, test:0.003814 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009072, test:0.000624 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007913, test:0.000407 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008159, test:0.012188 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008094, test:0.001720 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007231, test:0.001492 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007250, test:0.001048 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007927, test:0.000697 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007133, test:0.000667 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006580, test:0.000365 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006476, test:0.003633 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012457, test:0.003075 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007936, test:0.000429 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007527, test:0.000738 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007202, test:0.002433 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008913, test:0.004784 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007822, test:0.001068 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008914, test:0.000993 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007046, test:0.005472 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007297, test:0.003593 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.569015879877307\n",
      "Root mean squared error:  10.115842798582763\n",
      "Epoch[1/100] | loss train:0.067711, test:0.001352 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011261, test:0.000823 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011232, test:0.005294 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008683, test:0.000931 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007644, test:0.001819 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009089, test:0.003953 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011525, test:0.000532 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009139, test:0.002350 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010426, test:0.007471 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010811, test:0.000456 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013944, test:0.002268 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008571, test:0.002615 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007944, test:0.000687 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009570, test:0.000974 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008749, test:0.000445 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008996, test:0.002834 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007829, test:0.000523 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007643, test:0.000346 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007403, test:0.003856 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007268, test:0.000460 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008129, test:0.000407 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009188, test:0.000352 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008931, test:0.000822 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007501, test:0.005633 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009767, test:0.000362 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007460, test:0.001187 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007538, test:0.000475 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007785, test:0.000436 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.0890216165030493\n",
      "Root mean squared error:  6.1267942864270974\n",
      "Epoch[1/100] | loss train:0.049163, test:0.000604 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011145, test:0.001900 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011351, test:0.001063 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011350, test:0.000278 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008883, test:0.001341 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012158, test:0.000375 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011706, test:0.000379 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010333, test:0.001333 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007812, test:0.000621 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008628, test:0.003216 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010357, test:0.005797 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007286, test:0.002362 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007580, test:0.000302 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007197, test:0.001609 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  7.169368769402835\n",
      "Root mean squared error:  9.396487857205441\n",
      "Epoch[1/100] | loss train:0.057931, test:0.000560 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013290, test:0.006717 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011932, test:0.000408 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010456, test:0.000918 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008884, test:0.001865 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010244, test:0.001174 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009323, test:0.000374 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007769, test:0.000450 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010611, test:0.000820 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009570, test:0.002886 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009275, test:0.000455 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008765, test:0.001814 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007601, test:0.001064 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010050, test:0.009665 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008520, test:0.001256 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[16/100] | loss train:0.011897, test:0.002384 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008310, test:0.001702 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.7543330190200543\n",
      "Root mean squared error:  7.140016169065071\n",
      "Epoch[1/100] | loss train:0.054338, test:0.006403 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011881, test:0.000409 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009606, test:0.000318 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009737, test:0.002235 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009185, test:0.000458 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007237, test:0.003824 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008823, test:0.001516 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008638, test:0.001539 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009782, test:0.001685 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009829, test:0.004714 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007914, test:0.000563 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006784, test:0.000375 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008555, test:0.000551 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.372859531112909\n",
      "Root mean squared error:  6.908974907746407\n",
      "Epoch[1/100] | loss train:0.054472, test:0.002658 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012033, test:0.000399 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015463, test:0.006147 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011257, test:0.005622 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008700, test:0.000300 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008856, test:0.001673 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009452, test:0.003885 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010908, test:0.000375 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008612, test:0.000464 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009392, test:0.000297 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007967, test:0.000745 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007286, test:0.001313 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008390, test:0.000802 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007995, test:0.001143 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008971, test:0.000436 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007308, test:0.000868 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008083, test:0.000317 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007620, test:0.000949 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008076, test:0.002709 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008929, test:0.000394 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.3927650864545655\n",
      "Root mean squared error:  6.088922442917328\n",
      "Epoch[1/100] | loss train:0.047034, test:0.000321 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011047, test:0.000291 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010210, test:0.002760 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010620, test:0.001708 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011873, test:0.000361 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009103, test:0.001418 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008372, test:0.001014 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010334, test:0.000565 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008662, test:0.000570 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008323, test:0.000839 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008508, test:0.000458 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009387, test:0.005237 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.437161905352877\n",
      "Root mean squared error:  10.778262370256664\n",
      "Epoch[1/100] | loss train:0.052683, test:0.000776 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011960, test:0.001448 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013292, test:0.001148 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011632, test:0.000312 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009244, test:0.002424 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008576, test:0.001384 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010584, test:0.002802 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008453, test:0.000367 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007884, test:0.001212 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007889, test:0.000669 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009539, test:0.001688 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008206, test:0.000626 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013969, test:0.001768 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011235, test:0.001517 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.005057083311724\n",
      "Root mean squared error:  7.852533201933735\n",
      "Epoch[1/100] | loss train:0.068866, test:0.001896 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013424, test:0.001091 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011681, test:0.007275 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010043, test:0.001370 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008797, test:0.000296 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008486, test:0.006728 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008435, test:0.000453 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008680, test:0.001454 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009852, test:0.000847 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010471, test:0.000322 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008405, test:0.000832 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007109, test:0.000553 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007467, test:0.000420 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007815, test:0.001002 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009275, test:0.000564 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.0935616875398906\n",
      "Root mean squared error:  6.104264759404406\n",
      "Epoch[1/100] | loss train:0.055044, test:0.000501 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009016, test:0.000949 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010550, test:0.000982 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010731, test:0.013019 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010256, test:0.001730 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009178, test:0.001339 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009776, test:0.003651 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008060, test:0.000758 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010638, test:0.000547 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008984, test:0.000532 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007472, test:0.000315 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008584, test:0.000310 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009900, test:0.004638 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007875, test:0.000291 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007130, test:0.000734 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007096, test:0.003841 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008058, test:0.002056 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008469, test:0.000559 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008083, test:0.000338 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008328, test:0.001110 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006928, test:0.000390 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007694, test:0.000376 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008795, test:0.001516 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008269, test:0.003318 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  5.560899762367602\n",
      "Root mean squared error:  9.75259781895582\n",
      "Epoch[1/100] | loss train:0.057740, test:0.000384 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012368, test:0.000318 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011213, test:0.005053 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009168, test:0.000403 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007429, test:0.003200 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009765, test:0.001635 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008087, test:0.001559 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008997, test:0.000995 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007419, test:0.004196 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008523, test:0.000744 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013310, test:0.000415 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007612, test:0.001619 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.9865662740559484\n",
      "Root mean squared error:  7.61817286308679\n",
      "Epoch[1/100] | loss train:0.054642, test:0.000433 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010399, test:0.000572 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010748, test:0.000319 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/100] | loss train:0.010712, test:0.002338 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015604, test:0.010159 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011832, test:0.002650 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009487, test:0.001155 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008999, test:0.007791 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008659, test:0.008121 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008590, test:0.000641 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008585, test:0.000463 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007517, test:0.000845 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008467, test:0.000490 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.899202560464855\n",
      "Root mean squared error:  7.3428082100465195\n",
      "Epoch[1/100] | loss train:0.051191, test:0.007826 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015031, test:0.002786 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009183, test:0.001421 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009801, test:0.000824 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009659, test:0.001743 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014053, test:0.007347 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012485, test:0.000358 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009635, test:0.000968 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007765, test:0.004731 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009217, test:0.000606 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008625, test:0.001210 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009841, test:0.002656 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009539, test:0.001183 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008333, test:0.000723 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.020822, test:0.005598 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010827, test:0.000308 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006815, test:0.003694 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007131, test:0.000333 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007630, test:0.001308 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008288, test:0.000432 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007701, test:0.000426 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007155, test:0.000817 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007084, test:0.000648 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008032, test:0.000441 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013717, test:0.001129 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008716, test:0.001753 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.3587927521734553\n",
      "Root mean squared error:  7.365217556816463\n",
      "Epoch[1/100] | loss train:0.053188, test:0.000648 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016259, test:0.011334 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013296, test:0.003772 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008892, test:0.003075 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008364, test:0.003449 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008541, test:0.000575 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008084, test:0.000811 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008484, test:0.001658 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008188, test:0.000509 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008361, test:0.003124 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008358, test:0.000383 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008422, test:0.003701 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008976, test:0.000355 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006973, test:0.000390 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006655, test:0.001430 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008067, test:0.000408 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007047, test:0.003687 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007636, test:0.000555 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009285, test:0.001938 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009522, test:0.000435 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007483, test:0.000371 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006794, test:0.000470 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006667, test:0.001321 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.968335138196366\n",
      "Root mean squared error:  7.728974498362401\n",
      "Epoch[1/100] | loss train:0.039074, test:0.000825 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009590, test:0.003041 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008785, test:0.000727 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008624, test:0.000719 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008173, test:0.001281 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008672, test:0.000327 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009088, test:0.000546 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008273, test:0.000306 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008032, test:0.002029 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007784, test:0.001852 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007772, test:0.000843 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007302, test:0.001290 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008010, test:0.001272 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010845, test:0.002922 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008167, test:0.001062 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007573, test:0.000988 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.016456, test:0.001950 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009466, test:0.001004 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.959763461531338\n",
      "Root mean squared error:  6.812481966885435\n",
      "Epoch[1/100] | loss train:0.037842, test:0.001935 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.022034, test:0.003704 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010986, test:0.000553 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010824, test:0.003735 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009397, test:0.001200 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010633, test:0.000453 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008362, test:0.001765 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008355, test:0.004814 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007843, test:0.000472 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009206, test:0.000655 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009303, test:0.000329 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007696, test:0.000621 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007563, test:0.006919 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009904, test:0.001204 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008973, test:0.003580 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009658, test:0.000511 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007654, test:0.000432 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007626, test:0.001494 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006715, test:0.000378 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009304, test:0.001076 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007379, test:0.000547 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.4213407447176234\n",
      "Root mean squared error:  6.5992301210395174\n",
      "Epoch[1/100] | loss train:0.051966, test:0.000880 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010639, test:0.000356 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009904, test:0.000316 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009363, test:0.000339 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010004, test:0.000330 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008519, test:0.003570 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010909, test:0.002949 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009014, test:0.002285 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009844, test:0.000581 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012494, test:0.001456 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009938, test:0.000361 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008298, test:0.000596 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008481, test:0.002030 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  3.3469254373235984\n",
      "Root mean squared error:  7.857591122561177\n",
      "Epoch[1/100] | loss train:0.052709, test:0.001007 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013440, test:0.012439 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011526, test:0.002129 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010758, test:0.000389 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008435, test:0.000515 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008765, test:0.000533 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010593, test:0.000394 | lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/100] | loss train:0.008371, test:0.000325 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011128, test:0.000382 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008282, test:0.000411 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009044, test:0.002063 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009748, test:0.000835 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008992, test:0.004929 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008030, test:0.000778 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008855, test:0.000974 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007239, test:0.000469 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009177, test:0.006144 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008923, test:0.001534 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.746889461647017\n",
      "Root mean squared error:  8.050123274679635\n",
      "Epoch[1/100] | loss train:0.052535, test:0.000986 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011345, test:0.000400 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010839, test:0.002595 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015053, test:0.004644 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013366, test:0.002363 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013092, test:0.001787 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009396, test:0.000430 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007996, test:0.000647 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009522, test:0.000575 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009656, test:0.000925 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007739, test:0.000926 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007951, test:0.000946 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  6.370516367281627\n",
      "Root mean squared error:  9.810090172994936\n",
      "Epoch[1/100] | loss train:0.053827, test:0.000694 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016229, test:0.002068 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012256, test:0.003287 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011623, test:0.000893 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008170, test:0.000491 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008412, test:0.004679 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009388, test:0.000855 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009256, test:0.000421 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008229, test:0.002130 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008026, test:0.000771 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008926, test:0.001583 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007927, test:0.001989 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007636, test:0.006547 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008876, test:0.000460 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007529, test:0.001492 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008369, test:0.001770 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009638, test:0.000827 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007638, test:0.001708 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.581508017674111\n",
      "Root mean squared error:  8.441926257999397\n",
      "Epoch[1/100] | loss train:0.048905, test:0.000480 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008857, test:0.000947 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008644, test:0.000328 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009563, test:0.000301 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008798, test:0.000815 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009148, test:0.000361 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008545, test:0.001445 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009661, test:0.000540 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009293, test:0.002324 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012128, test:0.003959 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009397, test:0.002170 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007348, test:0.000959 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008949, test:0.001060 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007884, test:0.001592 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.374528097688631\n",
      "Root mean squared error:  7.754227554735915\n",
      "Epoch[1/100] | loss train:0.080661, test:0.000346 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013587, test:0.004610 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011258, test:0.000909 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009344, test:0.004047 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007900, test:0.000398 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009311, test:0.000326 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007224, test:0.001659 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008929, test:0.001978 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008602, test:0.000898 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007108, test:0.002621 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007866, test:0.000339 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007191, test:0.001939 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007944, test:0.001907 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009284, test:0.005113 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008093, test:0.000485 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008163, test:0.000540 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  4.75510514266918\n",
      "Root mean squared error:  7.410557916321396\n",
      "Epoch[1/100] | loss train:0.049898, test:0.001885 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012608, test:0.001274 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008400, test:0.000297 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009287, test:0.000776 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010561, test:0.003202 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009841, test:0.001013 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008642, test:0.000608 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009176, test:0.000945 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008687, test:0.000774 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007764, test:0.000304 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008011, test:0.003795 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009171, test:0.004336 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008159, test:0.000560 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  2.8581262615820346\n",
      "Root mean squared error:  6.314322176913235\n",
      "Epoch[1/100] | loss train:0.044192, test:0.002803 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011302, test:0.001995 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013623, test:0.000769 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009496, test:0.002713 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010407, test:0.001617 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012960, test:0.000370 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009422, test:0.000679 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008916, test:0.000759 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009998, test:0.000857 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010129, test:0.000385 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009867, test:0.002505 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010561, test:0.002564 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009005, test:0.001858 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009379, test:0.000772 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007633, test:0.000623 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008120, test:0.003669 | lr:0.010000\n",
      "Early stopping.\n",
      "Mean absolute error:  8.229584392668874\n",
      "Root mean squared error:  11.411803348177543\n"
     ]
    }
   ],
   "source": [
    "# Univariate Monte Carlo, early stopping\n",
    "\n",
    "uni_absolute2 = []\n",
    "uni_root2 = []\n",
    "\n",
    "for i in range(n):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "    model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    n_epochs_stop = stop\n",
    "    \n",
    "    for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "        loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "        loss_val, lr_val = run_epoch(val_dataloader)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch[{}/{}] | loss train:{:.6f}, test:{:.6f} | lr:{:.6f}'\n",
    "                  .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, loss_val, lr_train))\n",
    "        \n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "        \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predicted_train = np.array([])\n",
    "\n",
    "    for idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        out = model(x)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        predicted_train = np.concatenate((predicted_train, out))\n",
    "\n",
    "    predicted_val = np.array([])\n",
    "\n",
    "    for idx, (x, y) in enumerate(val_dataloader):\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        out = model(x)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        predicted_val = np.concatenate((predicted_val, out))\n",
    "\n",
    "    data_y_train_pred = np.zeros(num_data_points)\n",
    "    data_y_val_pred = np.zeros(num_data_points)\n",
    "\n",
    "    data_y_train_pred[config[\"data\"][\"window_size\"]:split_index+config[\"data\"][\"window_size\"]] = scaler.inverse_transform(predicted_train)\n",
    "    data_y_val_pred[split_index+config[\"data\"][\"window_size\"]:] = scaler.inverse_transform(predicted_val)\n",
    "\n",
    "    mae = mean_absolute_error(close_price_data, data_y_train_pred + data_y_val_pred)\n",
    "    print(\"Mean absolute error: \", mae)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(close_price_data, data_y_train_pred+data_y_val_pred))\n",
    "    print(\"Root mean squared error: \", rmse)\n",
    "    \n",
    "    uni_absolute2.append(mae)\n",
    "    uni_root2.append(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b2870b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 30.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mean absolute error, early stop\n",
    "\n",
    "mean = sum(uni_absolute2) / len(uni_absolute2)\n",
    "variance = sum([((x - mean) ** 2) for x in uni_absolute2]) / len(uni_absolute2)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=uni_absolute2, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Univariate Mean Absolute Error, Early Stop')\n",
    "maxfreq = m.max()\n",
    "plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}$'.format(mean,sd))\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bad592be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 40.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Root mean squared error, early stop\n",
    "\n",
    "mean = sum(uni_root2) / len(uni_root2)\n",
    "variance = sum([((x - mean) ** 2) for x in uni_root2]) / len(uni_root2)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=uni_root2, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Univariate Root Mean Squared Error, Early Stop')\n",
    "plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}'.format(mean,sd))\n",
    "maxfreq = m.max()\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b93a5",
   "metadata": {},
   "source": [
    "### Multivariate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cae444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with data preparation...\n",
      "Completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5573, 7), (314, 7))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prep the data\n",
    "\n",
    "train_df, test_df = preprocess.prep_data(df=data, split_index = split_index, plot_df=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ab7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "batch_size = config[\"training\"][\"batch_size\"]\n",
    "n_epochs = config[\"training\"][\"num_epoch\"]\n",
    "n_epochs_stop = stop\n",
    "label_name = 'Close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f742c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training...\n",
      "Epoch 1 train loss: 0.0073 test loss: 0.0052\n",
      "Epoch 2 train loss: 0.0679 test loss: 0.0019\n",
      "Epoch 3 train loss: 0.1062 test loss: 0.0108\n",
      "Epoch 4 train loss: 0.0377 test loss: 0.0021\n",
      "Epoch 5 train loss: 0.0109 test loss: 0.0017\n",
      "Epoch 6 train loss: 0.0059 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 8 train loss: 0.0027 test loss: 0.0004\n",
      "Epoch 9 train loss: 0.0029 test loss: 0.0004\n",
      "Epoch 10 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 11 train loss: 0.0042 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 23 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.002 test loss: 0.0012\n",
      "Epoch 26 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0026 test loss: 0.0006\n",
      "Epoch 28 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0017 test loss: 0.0007\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 39 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 43 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 51 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 55 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 58 train loss: 0.0017 test loss: 0.0011\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 60 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 67 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 68 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 69 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 72 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 80 train loss: 0.0011 test loss: 0.0013\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.0016\n",
      "Epoch 86 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 87 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0017 test loss: 0.0013\n",
      "Epoch 89 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 92 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0006\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0044 test loss: 0.0012\n",
      "Epoch 2 train loss: 0.0823 test loss: 0.0116\n",
      "Epoch 3 train loss: 0.0586 test loss: 0.0033\n",
      "Epoch 4 train loss: 0.0222 test loss: 0.0037\n",
      "Epoch 5 train loss: 0.0094 test loss: 0.0004\n",
      "Epoch 6 train loss: 0.0045 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0094 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0044 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0047 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0035 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0037 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0038 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0036 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0035 test loss: 0.0003\n",
      "Epoch 19 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0039 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0039 test loss: 0.0004\n",
      "Epoch 23 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0034 test loss: 0.0004\n",
      "Epoch 27 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0036 test loss: 0.0004\n",
      "Epoch 29 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 40 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 42 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0017 test loss: 0.0\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 60 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 62 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 63 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 65 train loss: 0.0019 test loss: 0.0005\n",
      "Epoch 66 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0019 test loss: 0.001\n",
      "Epoch 68 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 69 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 77 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 78 train loss: 0.0019 test loss: 0.0\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 86 train loss: 0.0017 test loss: 0.0007\n",
      "Epoch 87 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 89 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 90 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 94 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0013\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0081 test loss: 0.0062\n",
      "Epoch 2 train loss: 0.0617 test loss: 0.0005\n",
      "Epoch 3 train loss: 0.0627 test loss: 0.0002\n",
      "Epoch 4 train loss: 0.029 test loss: 0.0027\n",
      "Epoch 5 train loss: 0.0084 test loss: 0.0041\n",
      "Epoch 6 train loss: 0.0065 test loss: 0.0005\n",
      "Epoch 7 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0047 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0041 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0033 test loss: 0.0003\n",
      "Epoch 12 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0027 test loss: 0.0004\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 19 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 29 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0019 test loss: 0.0007\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0014\n",
      "Epoch 35 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0009\n",
      "Epoch 38 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 39 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 42 train loss: 0.0018 test loss: 0.001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 44 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.002\n",
      "Epoch 48 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.002 test loss: 0.0006\n",
      "Epoch 52 train loss: 0.0033 test loss: 0.0007\n",
      "Epoch 53 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0019 test loss: 0.0009\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0012\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0017 test loss: 0.0006\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0016 test loss: 0.0011\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 78 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 89 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0013\n",
      "Epoch 93 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0069 test loss: 0.0043\n",
      "Epoch 2 train loss: 0.07 test loss: 0.0044\n",
      "Epoch 3 train loss: 0.1061 test loss: 0.0148\n",
      "Epoch 4 train loss: 0.0568 test loss: 0.0032\n",
      "Epoch 5 train loss: 0.0191 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0034 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0035 test loss: 0.0007\n",
      "Epoch 8 train loss: 0.0033 test loss: 0.0024\n",
      "Epoch 9 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0056 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.003 test loss: 0.0005\n",
      "Epoch 18 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0037 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0032 test loss: 0.0005\n",
      "Epoch 22 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0033 test loss: 0.0004\n",
      "Epoch 26 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 29 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 30 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0012\n",
      "Epoch 53 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 56 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0016\n",
      "Epoch 66 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0019 test loss: 0.0006\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 79 train loss: 0.0011 test loss: 0.001\n",
      "Epoch 80 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0016 test loss: 0.001\n",
      "Epoch 84 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 95 train loss: 0.001 test loss: 0.0014\n",
      "Epoch 96 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 97 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.002 test loss: 0.0016\n",
      "Epoch 99 train loss: 0.0021 test loss: 0.0\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0006\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0062 test loss: 0.0036\n",
      "Epoch 2 train loss: 0.0652 test loss: 0.0063\n",
      "Epoch 3 train loss: 0.1036 test loss: 0.019\n",
      "Epoch 4 train loss: 0.0243 test loss: 0.0017\n",
      "Epoch 5 train loss: 0.0087 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0031 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 11 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 14 train loss: 0.0026 test loss: 0.0018\n",
      "Epoch 15 train loss: 0.003 test loss: 0.0005\n",
      "Epoch 16 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 17 train loss: 0.0052 test loss: 0.0003\n",
      "Epoch 18 train loss: 0.0045 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 20 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 22 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 24 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0023 test loss: 0.0006\n",
      "Epoch 35 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0011\n",
      "Epoch 42 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 46 train loss: 0.0017 test loss: 0.0009\n",
      "Epoch 47 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0019 test loss: 0.0005\n",
      "Epoch 49 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0022 test loss: 0.0015\n",
      "Epoch 51 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 52 train loss: 0.0035 test loss: 0.0012\n",
      "Epoch 53 train loss: 0.0039 test loss: 0.0006\n",
      "Epoch 54 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 55 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 75 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0017 test loss: 0.0006\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 85 train loss: 0.0014 test loss: 0.0012\n",
      "Epoch 86 train loss: 0.0018 test loss: 0.001\n",
      "Epoch 87 train loss: 0.0018 test loss: 0.0022\n",
      "Epoch 88 train loss: 0.0022 test loss: 0.0006\n",
      "Epoch 89 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 96 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 97 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 100 train loss: 0.0012 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0068 test loss: 0.0044\n",
      "Epoch 2 train loss: 0.0617 test loss: 0.001\n",
      "Epoch 3 train loss: 0.0919 test loss: 0.0103\n",
      "Epoch 4 train loss: 0.0258 test loss: 0.0023\n",
      "Epoch 5 train loss: 0.0126 test loss: 0.0011\n",
      "Epoch 6 train loss: 0.0073 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0036 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0116 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0035 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0041 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0042 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0036 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 29 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 56 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0019 test loss: 0.0008\n",
      "Epoch 60 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 66 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0016 test loss: 0.0012\n",
      "Epoch 73 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 74 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 76 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 78 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 83 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 88 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 89 train loss: 0.0009 test loss: 0.0012\n",
      "Epoch 90 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0006\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0058 test loss: 0.0035\n",
      "Epoch 2 train loss: 0.0767 test loss: 0.0004\n",
      "Epoch 3 train loss: 0.0938 test loss: 0.0142\n",
      "Epoch 4 train loss: 0.0225 test loss: 0.0019\n",
      "Epoch 5 train loss: 0.0085 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0035 test loss: 0.0007\n",
      "Epoch 8 train loss: 0.0089 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0035 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0022 test loss: 0.0004\n",
      "Epoch 12 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0027 test loss: 0.0004\n",
      "Epoch 16 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0032 test loss: 0.0004\n",
      "Epoch 18 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0029 test loss: 0.0004\n",
      "Epoch 22 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0033 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 29 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0013\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0018 test loss: 0.0007\n",
      "Epoch 56 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0014\n",
      "Epoch 63 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.002 test loss: 0.0009\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 75 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0017 test loss: 0.0013\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0022 test loss: 0.0005\n",
      "Epoch 81 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0026 test loss: 0.0013\n",
      "Epoch 85 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 89 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 97 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0014 test loss: 0.0005\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0077 test loss: 0.0058\n",
      "Epoch 2 train loss: 0.0714 test loss: 0.0018\n",
      "Epoch 3 train loss: 0.0869 test loss: 0.0079\n",
      "Epoch 4 train loss: 0.0431 test loss: 0.0037\n",
      "Epoch 5 train loss: 0.0148 test loss: 0.0036\n",
      "Epoch 6 train loss: 0.006 test loss: 0.0008\n",
      "Epoch 7 train loss: 0.0044 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.0137 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0037 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0023 test loss: 0.0005\n",
      "Epoch 18 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0032 test loss: 0.0004\n",
      "Epoch 20 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 34 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0017 test loss: 0.0006\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0013\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 43 train loss: 0.0018 test loss: 0.0012\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0012\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0021 test loss: 0.0006\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 55 train loss: 0.002 test loss: 0.0017\n",
      "Epoch 56 train loss: 0.0037 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0024 test loss: 0.0008\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0026 test loss: 0.0005\n",
      "Epoch 61 train loss: 0.0023 test loss: 0.0007\n",
      "Epoch 62 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 63 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0032 test loss: 0.0013\n",
      "Epoch 65 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 66 train loss: 0.0036 test loss: 0.0011\n",
      "Epoch 67 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0026 test loss: 0.0005\n",
      "Epoch 69 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 80 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 84 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 85 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 97 train loss: 0.0015 test loss: 0.0012\n",
      "Epoch 98 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 99 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0014 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0054 test loss: 0.0057\n",
      "Epoch 2 train loss: 0.1083 test loss: 0.0128\n",
      "Epoch 3 train loss: 0.0751 test loss: 0.0144\n",
      "Epoch 4 train loss: 0.0195 test loss: 0.0011\n",
      "Epoch 5 train loss: 0.0039 test loss: 0.0023\n",
      "Epoch 6 train loss: 0.0054 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0048 test loss: 0.0035\n",
      "Epoch 8 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 18 train loss: 0.0032 test loss: 0.0004\n",
      "Epoch 19 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.004 test loss: 0.0004\n",
      "Epoch 21 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 28 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0022 test loss: 0.0004\n",
      "Epoch 30 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 60 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 66 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.001\n",
      "Epoch 69 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0019 test loss: 0.0006\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 74 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.002 test loss: 0.0006\n",
      "Epoch 77 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 84 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 85 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 88 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.001\n",
      "Epoch 93 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0009\n",
      "Epoch 97 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0088 test loss: 0.0055\n",
      "Epoch 2 train loss: 0.062 test loss: 0.0006\n",
      "Epoch 3 train loss: 0.0804 test loss: 0.0025\n",
      "Epoch 4 train loss: 0.0447 test loss: 0.005\n",
      "Epoch 5 train loss: 0.0131 test loss: 0.0004\n",
      "Epoch 6 train loss: 0.0029 test loss: 0.0007\n",
      "Epoch 7 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 9 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 10 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 11 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 23 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 24 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 26 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 28 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 35 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0017 test loss: 0.0012\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 38 train loss: 0.0029 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 43 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 45 train loss: 0.0012 test loss: 0.0009\n",
      "Epoch 46 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0015\n",
      "Epoch 55 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 58 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 60 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 61 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 65 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 69 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 70 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 73 train loss: 0.0011 test loss: 0.0007\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 75 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 76 train loss: 0.0029 test loss: 0.0006\n",
      "Epoch 77 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 82 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0011 test loss: 0.0007\n",
      "Epoch 85 train loss: 0.001 test loss: 0.0003\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0012\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0012 test loss: 0.0005\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0094 test loss: 0.0059\n",
      "Epoch 2 train loss: 0.0567 test loss: 0.0006\n",
      "Epoch 3 train loss: 0.0726 test loss: 0.0004\n",
      "Epoch 4 train loss: 0.0386 test loss: 0.0007\n",
      "Epoch 5 train loss: 0.008 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0035 test loss: 0.0009\n",
      "Epoch 7 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0044 test loss: 0.0019\n",
      "Epoch 10 train loss: 0.0034 test loss: 0.0015\n",
      "Epoch 11 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0063 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0033 test loss: 0.0003\n",
      "Epoch 18 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0029 test loss: 0.0004\n",
      "Epoch 20 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 23 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0025 test loss: 0.0004\n",
      "Epoch 25 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0022 test loss: 0.0004\n",
      "Epoch 27 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 37 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0018 test loss: 0.0008\n",
      "Epoch 39 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 45 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0009\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 60 train loss: 0.0019 test loss: 0.0012\n",
      "Epoch 61 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 63 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.002 test loss: 0.0009\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 71 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 77 train loss: 0.001 test loss: 0.0006\n",
      "Epoch 78 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 83 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 84 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 85 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 99 train loss: 0.0009 test loss: 0.0014\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0003\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0066 test loss: 0.002\n",
      "Epoch 2 train loss: 0.0556 test loss: 0.0046\n",
      "Epoch 3 train loss: 0.1069 test loss: 0.0192\n",
      "Epoch 4 train loss: 0.0323 test loss: 0.005\n",
      "Epoch 5 train loss: 0.056 test loss: 0.0114\n",
      "Epoch 6 train loss: 0.0109 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0101 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0052 test loss: 0.0006\n",
      "Epoch 10 train loss: 0.0141 test loss: 0.0004\n",
      "Epoch 11 train loss: 0.0083 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.007 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0052 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.004 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0038 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0035 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 39 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 40 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 42 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 43 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 45 train loss: 0.0044 test loss: 0.0003\n",
      "Epoch 46 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 47 train loss: 0.0043 test loss: 0.0003\n",
      "Epoch 48 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.0037 test loss: 0.0004\n",
      "Epoch 50 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0037 test loss: 0.0004\n",
      "Epoch 52 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 55 train loss: 0.0034 test loss: 0.0003\n",
      "Epoch 56 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0036 test loss: 0.0004\n",
      "Epoch 58 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0035 test loss: 0.0004\n",
      "Epoch 60 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0037 test loss: 0.0004\n",
      "Epoch 62 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 63 train loss: 0.0035 test loss: 0.0004\n",
      "Epoch 64 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 65 train loss: 0.0039 test loss: 0.0003\n",
      "Epoch 66 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 67 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 69 train loss: 0.0032 test loss: 0.0003\n",
      "Epoch 70 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 73 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 74 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 77 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 81 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 87 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 93 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 94 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0007\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0075 test loss: 0.0057\n",
      "Epoch 2 train loss: 0.058 test loss: 0.0003\n",
      "Epoch 3 train loss: 0.082 test loss: 0.0006\n",
      "Epoch 4 train loss: 0.0573 test loss: 0.0001\n",
      "Epoch 5 train loss: 0.0172 test loss: 0.0023\n",
      "Epoch 6 train loss: 0.0048 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0046 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0034 test loss: 0.0003\n",
      "Epoch 11 train loss: 0.0024 test loss: 0.0006\n",
      "Epoch 12 train loss: 0.0055 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 16 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0029 test loss: 0.0004\n",
      "Epoch 18 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 19 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0025 test loss: 0.0004\n",
      "Epoch 23 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0027 test loss: 0.0004\n",
      "Epoch 25 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 27 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 39 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 45 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 46 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0016 test loss: 0.0019\n",
      "Epoch 50 train loss: 0.0027 test loss: 0.0007\n",
      "Epoch 51 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 60 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 67 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 76 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 77 train loss: 0.0011 test loss: 0.0009\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0009\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 85 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 89 train loss: 0.0009 test loss: 0.0013\n",
      "Epoch 90 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 97 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0013 test loss: 0.0004\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0048 test loss: 0.0008\n",
      "Epoch 2 train loss: 0.0695 test loss: 0.0011\n",
      "Epoch 3 train loss: 0.1021 test loss: 0.0213\n",
      "Epoch 4 train loss: 0.0221 test loss: 0.0022\n",
      "Epoch 5 train loss: 0.0086 test loss: 0.0024\n",
      "Epoch 6 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0027 test loss: 0.0007\n",
      "Epoch 9 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0043 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0025 test loss: 0.0005\n",
      "Epoch 13 train loss: 0.0028 test loss: 0.0006\n",
      "Epoch 14 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 17 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 18 train loss: 0.0034 test loss: 0.0003\n",
      "Epoch 19 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 23 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0022 test loss: 0.0004\n",
      "Epoch 27 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0025 test loss: 0.0004\n",
      "Epoch 29 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 51 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0008\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 54 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0014\n",
      "Epoch 68 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0022 test loss: 0.0006\n",
      "Epoch 72 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0019 test loss: 0.0007\n",
      "Epoch 76 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 87 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 88 train loss: 0.0016 test loss: 0.0013\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0009\n",
      "Epoch 90 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0017 test loss: 0.0\n",
      "Epoch 92 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 93 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 96 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.006 test loss: 0.0014\n",
      "Epoch 2 train loss: 0.0717 test loss: 0.0004\n",
      "Epoch 3 train loss: 0.0998 test loss: 0.0172\n",
      "Epoch 4 train loss: 0.0279 test loss: 0.0009\n",
      "Epoch 5 train loss: 0.0091 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0026 test loss: 0.0008\n",
      "Epoch 12 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 13 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 14 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 17 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0019 test loss: 0.0011\n",
      "Epoch 20 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0032 test loss: 0.0005\n",
      "Epoch 22 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 23 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 39 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 42 train loss: 0.0019 test loss: 0.0018\n",
      "Epoch 43 train loss: 0.0027 test loss: 0.0004\n",
      "Epoch 44 train loss: 0.0022 test loss: 0.0005\n",
      "Epoch 45 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 46 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 47 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 60 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 65 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.001 test loss: 0.0007\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 73 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 80 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 83 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 86 train loss: 0.0008 test loss: 0.0003\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0006\n",
      "Epoch 90 train loss: 0.001 test loss: 0.0007\n",
      "Epoch 91 train loss: 0.001 test loss: 0.0006\n",
      "Epoch 92 train loss: 0.001 test loss: 0.0008\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 95 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 96 train loss: 0.0024 test loss: 0.0008\n",
      "Epoch 97 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 99 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0013 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0043 test loss: 0.0007\n",
      "Epoch 2 train loss: 0.0845 test loss: 0.0131\n",
      "Epoch 3 train loss: 0.0601 test loss: 0.0082\n",
      "Epoch 4 train loss: 0.0186 test loss: 0.0016\n",
      "Epoch 5 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0024 test loss: 0.0006\n",
      "Epoch 7 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0027 test loss: 0.0007\n",
      "Epoch 10 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0038 test loss: 0.0003\n",
      "Epoch 12 train loss: 0.0029 test loss: 0.0006\n",
      "Epoch 13 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0027 test loss: 0.0004\n",
      "Epoch 22 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0024 test loss: 0.0005\n",
      "Epoch 24 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0024 test loss: 0.0004\n",
      "Epoch 26 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 46 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.001\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 61 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 66 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0016 test loss: 0.0009\n",
      "Epoch 72 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 83 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0013\n",
      "Epoch 93 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0019 test loss: 0.0014\n",
      "Epoch 95 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 96 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0035 test loss: 0.002\n",
      "Epoch 99 train loss: 0.0035 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0016 test loss: 0.0005\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0078 test loss: 0.0049\n",
      "Epoch 2 train loss: 0.056 test loss: 0.0006\n",
      "Epoch 3 train loss: 0.071 test loss: 0.0022\n",
      "Epoch 4 train loss: 0.0687 test loss: 0.0008\n",
      "Epoch 5 train loss: 0.02 test loss: 0.0028\n",
      "Epoch 6 train loss: 0.0043 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 11 train loss: 0.0053 test loss: 0.0003\n",
      "Epoch 12 train loss: 0.0023 test loss: 0.0005\n",
      "Epoch 13 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0025 test loss: 0.0004\n",
      "Epoch 16 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 17 train loss: 0.0038 test loss: 0.0004\n",
      "Epoch 18 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 39 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0011\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0016 test loss: 0.0009\n",
      "Epoch 44 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 51 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 52 train loss: 0.0018 test loss: 0.0014\n",
      "Epoch 53 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0022 test loss: 0.0006\n",
      "Epoch 57 train loss: 0.0028 test loss: 0.0006\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0017 test loss: 0.0007\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.0012\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0011\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0015\n",
      "Epoch 82 train loss: 0.002 test loss: 0.0006\n",
      "Epoch 83 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 85 train loss: 0.0017 test loss: 0.0\n",
      "Epoch 86 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.001 test loss: 0.0003\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0065 test loss: 0.0011\n",
      "Epoch 2 train loss: 0.0847 test loss: 0.0075\n",
      "Epoch 3 train loss: 0.0683 test loss: 0.0098\n",
      "Epoch 4 train loss: 0.0197 test loss: 0.0002\n",
      "Epoch 5 train loss: 0.0033 test loss: 0.0005\n",
      "Epoch 6 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0027 test loss: 0.0008\n",
      "Epoch 9 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 11 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 14 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 19 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 28 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 31 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 35 train loss: 0.0013 test loss: 0.0011\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.001\n",
      "Epoch 40 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 43 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 44 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 45 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 51 train loss: 0.0016 test loss: 0.0013\n",
      "Epoch 52 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0012 test loss: 0.0011\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 66 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.001 test loss: 0.0012\n",
      "Epoch 71 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 74 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 75 train loss: 0.0009 test loss: 0.0016\n",
      "Epoch 76 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 80 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 83 train loss: 0.0008 test loss: 0.0005\n",
      "Epoch 84 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 87 train loss: 0.0009 test loss: 0.0005\n",
      "Epoch 88 train loss: 0.0009 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.001 test loss: 0.0003\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 92 train loss: 0.001 test loss: 0.0003\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0018\n",
      "Epoch 94 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0008 test loss: 0.0003\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0062 test loss: 0.004\n",
      "Epoch 2 train loss: 0.0692 test loss: 0.0056\n",
      "Epoch 3 train loss: 0.1107 test loss: 0.0181\n",
      "Epoch 4 train loss: 0.0306 test loss: 0.0019\n",
      "Epoch 5 train loss: 0.0145 test loss: 0.0033\n",
      "Epoch 6 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0036 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0035 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0037 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0031 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0033 test loss: 0.0004\n",
      "Epoch 26 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0034 test loss: 0.0004\n",
      "Epoch 32 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0038 test loss: 0.0004\n",
      "Epoch 34 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0035 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0031 test loss: 0.0004\n",
      "Epoch 40 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 42 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 43 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 44 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 45 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 49 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 78 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0009\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 89 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 96 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0013\n",
      "Epoch 100 train loss: 0.0018 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0114 test loss: 0.0046\n",
      "Epoch 2 train loss: 0.0648 test loss: 0.0004\n",
      "Epoch 3 train loss: 0.0791 test loss: 0.002\n",
      "Epoch 4 train loss: 0.0421 test loss: 0.0005\n",
      "Epoch 5 train loss: 0.0098 test loss: 0.0012\n",
      "Epoch 6 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0045 test loss: 0.003\n",
      "Epoch 9 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 11 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 12 train loss: 0.0074 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 16 train loss: 0.0043 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 19 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0035 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 37 train loss: 0.0013 test loss: 0.0014\n",
      "Epoch 38 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0019 test loss: 0.001\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0023 test loss: 0.0007\n",
      "Epoch 46 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 47 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 55 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 58 train loss: 0.0017 test loss: 0.0007\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0009\n",
      "Epoch 61 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 64 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 67 train loss: 0.0011 test loss: 0.0008\n",
      "Epoch 68 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.001\n",
      "Epoch 71 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 75 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 77 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 82 train loss: 0.0011 test loss: 0.0013\n",
      "Epoch 83 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 84 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.001 test loss: 0.0004\n",
      "Epoch 93 train loss: 0.0009 test loss: 0.0006\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 98 train loss: 0.001 test loss: 0.0009\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0003\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0079 test loss: 0.005\n",
      "Epoch 2 train loss: 0.0656 test loss: 0.0003\n",
      "Epoch 3 train loss: 0.0791 test loss: 0.0008\n",
      "Epoch 4 train loss: 0.0547 test loss: 0.0036\n",
      "Epoch 5 train loss: 0.0236 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0077 test loss: 0.0069\n",
      "Epoch 7 train loss: 0.0148 test loss: 0.0003\n",
      "Epoch 8 train loss: 0.0084 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.0055 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0041 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0037 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0043 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0038 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0022 test loss: 0.0004\n",
      "Epoch 35 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 37 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0019 test loss: 0.0007\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 70 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 72 train loss: 0.0013 test loss: 0.0013\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 77 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 87 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 96 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0079 test loss: 0.0053\n",
      "Epoch 2 train loss: 0.0601 test loss: 0.0006\n",
      "Epoch 3 train loss: 0.0746 test loss: 0.0006\n",
      "Epoch 4 train loss: 0.0398 test loss: 0.0004\n",
      "Epoch 5 train loss: 0.0083 test loss: 0.0003\n",
      "Epoch 6 train loss: 0.0047 test loss: 0.0007\n",
      "Epoch 7 train loss: 0.0034 test loss: 0.0006\n",
      "Epoch 8 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.005 test loss: 0.0005\n",
      "Epoch 10 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 13 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 20 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 23 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 27 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0014 test loss: 0.0011\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 35 train loss: 0.0013 test loss: 0.0018\n",
      "Epoch 36 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0018 test loss: 0.0008\n",
      "Epoch 40 train loss: 0.002 test loss: 0.0011\n",
      "Epoch 41 train loss: 0.0041 test loss: 0.0011\n",
      "Epoch 42 train loss: 0.0082 test loss: 0.0002\n",
      "Epoch 43 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0021 test loss: 0.0006\n",
      "Epoch 45 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0012 test loss: 0.0015\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 66 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0017\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 70 train loss: 0.0017 test loss: 0.0007\n",
      "Epoch 71 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 75 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 86 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0015 test loss: 0.0013\n",
      "Epoch 91 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 92 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.001 test loss: 0.0\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0063 test loss: 0.0022\n",
      "Epoch 2 train loss: 0.0595 test loss: 0.0057\n",
      "Epoch 3 train loss: 0.0998 test loss: 0.015\n",
      "Epoch 4 train loss: 0.0276 test loss: 0.0019\n",
      "Epoch 5 train loss: 0.0071 test loss: 0.0005\n",
      "Epoch 6 train loss: 0.0029 test loss: 0.0005\n",
      "Epoch 7 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 17 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0016 test loss: 0.0014\n",
      "Epoch 19 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0035 test loss: 0.0007\n",
      "Epoch 21 train loss: 0.0035 test loss: 0.001\n",
      "Epoch 22 train loss: 0.0051 test loss: 0.0003\n",
      "Epoch 23 train loss: 0.006 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 35 train loss: 0.0019 test loss: 0.0\n",
      "Epoch 36 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 66 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 70 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 75 train loss: 0.0011 test loss: 0.0009\n",
      "Epoch 76 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 77 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 83 train loss: 0.001 test loss: 0.0007\n",
      "Epoch 84 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 95 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0016\n",
      "Epoch 97 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.0015 test loss: 0.0016\n",
      "Epoch 100 train loss: 0.0019 test loss: 0.0002\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0064 test loss: 0.0014\n",
      "Epoch 2 train loss: 0.0881 test loss: 0.0146\n",
      "Epoch 3 train loss: 0.063 test loss: 0.0077\n",
      "Epoch 4 train loss: 0.0202 test loss: 0.0031\n",
      "Epoch 5 train loss: 0.0067 test loss: 0.0006\n",
      "Epoch 6 train loss: 0.0045 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0064 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0034 test loss: 0.0004\n",
      "Epoch 10 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 12 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 13 train loss: 0.0034 test loss: 0.0004\n",
      "Epoch 14 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 17 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 35 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 36 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0018 test loss: 0.0008\n",
      "Epoch 39 train loss: 0.0018 test loss: 0.0009\n",
      "Epoch 40 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0008\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 45 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.001\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 48 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 50 train loss: 0.0021 test loss: 0.0008\n",
      "Epoch 51 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 52 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0022 test loss: 0.0009\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 71 train loss: 0.0012 test loss: 0.0015\n",
      "Epoch 72 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0019 test loss: 0.0006\n",
      "Epoch 74 train loss: 0.002 test loss: 0.0007\n",
      "Epoch 75 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0019 test loss: 0.0005\n",
      "Epoch 78 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 84 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 89 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0014 test loss: 0.0015\n",
      "Epoch 92 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 93 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0004\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0065 test loss: 0.0032\n",
      "Epoch 2 train loss: 0.0596 test loss: 0.0016\n",
      "Epoch 3 train loss: 0.0894 test loss: 0.0178\n",
      "Epoch 4 train loss: 0.0214 test loss: 0.0021\n",
      "Epoch 5 train loss: 0.0071 test loss: 0.0005\n",
      "Epoch 6 train loss: 0.0043 test loss: 0.0005\n",
      "Epoch 7 train loss: 0.0029 test loss: 0.0007\n",
      "Epoch 8 train loss: 0.0059 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.0029 test loss: 0.0005\n",
      "Epoch 10 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 13 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0036 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 21 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.001\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0011\n",
      "Epoch 36 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 37 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 46 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 50 train loss: 0.0017 test loss: 0.0012\n",
      "Epoch 51 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 55 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0018\n",
      "Epoch 58 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 61 train loss: 0.0019 test loss: 0.0008\n",
      "Epoch 62 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 69 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0011\n",
      "Epoch 73 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 76 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 79 train loss: 0.0009 test loss: 0.0004\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 81 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0011\n",
      "Epoch 83 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0008 test loss: 0.0009\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 93 train loss: 0.001 test loss: 0.0005\n",
      "Epoch 94 train loss: 0.0009 test loss: 0.0011\n",
      "Epoch 95 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0009 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0009 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0009 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0064 test loss: 0.0033\n",
      "Epoch 2 train loss: 0.0611 test loss: 0.0044\n",
      "Epoch 3 train loss: 0.1085 test loss: 0.0161\n",
      "Epoch 4 train loss: 0.0404 test loss: 0.0007\n",
      "Epoch 5 train loss: 0.0158 test loss: 0.0055\n",
      "Epoch 6 train loss: 0.0069 test loss: 0.0004\n",
      "Epoch 7 train loss: 0.0055 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0084 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0038 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0038 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 28 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0032 test loss: 0.0003\n",
      "Epoch 30 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 33 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 37 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 59 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0017 test loss: 0.0009\n",
      "Epoch 71 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 73 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0026 test loss: 0.0011\n",
      "Epoch 75 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 85 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 96 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0013 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0056 test loss: 0.0019\n",
      "Epoch 2 train loss: 0.064 test loss: 0.0066\n",
      "Epoch 3 train loss: 0.1037 test loss: 0.0166\n",
      "Epoch 4 train loss: 0.0253 test loss: 0.0024\n",
      "Epoch 5 train loss: 0.0078 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0026 test loss: 0.0006\n",
      "Epoch 7 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 8 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0018 test loss: 0.0011\n",
      "Epoch 10 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0039 test loss: 0.0006\n",
      "Epoch 12 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 13 train loss: 0.0045 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0022 test loss: 0.0005\n",
      "Epoch 18 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 43 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0013 test loss: 0.0013\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0016\n",
      "Epoch 55 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 56 train loss: 0.0026 test loss: 0.0009\n",
      "Epoch 57 train loss: 0.0025 test loss: 0.0004\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 60 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 66 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 67 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 80 train loss: 0.0009 test loss: 0.0004\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0017\n",
      "Epoch 84 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0009\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0059 test loss: 0.0013\n",
      "Epoch 2 train loss: 0.0698 test loss: 0.0006\n",
      "Epoch 3 train loss: 0.0654 test loss: 0.0027\n",
      "Epoch 4 train loss: 0.0187 test loss: 0.0008\n",
      "Epoch 5 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 8 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0044 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 12 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0036 test loss: 0.0023\n",
      "Epoch 22 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.002 test loss: 0.0006\n",
      "Epoch 24 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 25 train loss: 0.0046 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 28 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 30 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 34 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0013\n",
      "Epoch 59 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0013\n",
      "Epoch 67 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 72 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0013 test loss: 0.0013\n",
      "Epoch 78 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 84 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 86 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 87 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 89 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 97 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0052 test loss: 0.0019\n",
      "Epoch 2 train loss: 0.075 test loss: 0.0013\n",
      "Epoch 3 train loss: 0.0841 test loss: 0.0144\n",
      "Epoch 4 train loss: 0.0181 test loss: 0.0023\n",
      "Epoch 5 train loss: 0.0036 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.0023 test loss: 0.0007\n",
      "Epoch 7 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0028 test loss: 0.0004\n",
      "Epoch 9 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0022 test loss: 0.0009\n",
      "Epoch 11 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 13 train loss: 0.0017 test loss: 0.001\n",
      "Epoch 14 train loss: 0.003 test loss: 0.001\n",
      "Epoch 15 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 16 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 22 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 26 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 28 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 39 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 40 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0019\n",
      "Epoch 44 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 46 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 47 train loss: 0.0045 test loss: 0.0014\n",
      "Epoch 48 train loss: 0.0038 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0032 test loss: 0.0011\n",
      "Epoch 51 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0034 test loss: 0.0004\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 56 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 74 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 82 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.001 test loss: 0.0017\n",
      "Epoch 97 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0005\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0044 test loss: 0.0018\n",
      "Epoch 2 train loss: 0.0931 test loss: 0.0167\n",
      "Epoch 3 train loss: 0.0467 test loss: 0.0013\n",
      "Epoch 4 train loss: 0.0396 test loss: 0.0132\n",
      "Epoch 5 train loss: 0.0118 test loss: 0.0003\n",
      "Epoch 6 train loss: 0.0061 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0069 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0037 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0029 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 28 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0038 test loss: 0.0003\n",
      "Epoch 30 train loss: 0.0042 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0035 test loss: 0.0003\n",
      "Epoch 35 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0052 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0029 test loss: 0.0004\n",
      "Epoch 38 train loss: 0.0039 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 40 train loss: 0.0055 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 42 train loss: 0.0036 test loss: 0.0004\n",
      "Epoch 43 train loss: 0.0043 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0058 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 46 train loss: 0.004 test loss: 0.0004\n",
      "Epoch 47 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 48 train loss: 0.0054 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0037 test loss: 0.0003\n",
      "Epoch 51 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0048 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 54 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 55 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 58 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 61 train loss: 0.0028 test loss: 0.0004\n",
      "Epoch 62 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 63 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 64 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 66 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 70 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0025 test loss: 0.0004\n",
      "Epoch 72 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 74 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 76 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 78 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 80 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 83 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 87 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 88 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 92 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 93 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0021 test loss: 0.0013\n",
      "Epoch 98 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0089 test loss: 0.0054\n",
      "Epoch 2 train loss: 0.0683 test loss: 0.0005\n",
      "Epoch 3 train loss: 0.0901 test loss: 0.0088\n",
      "Epoch 4 train loss: 0.0476 test loss: 0.0018\n",
      "Epoch 5 train loss: 0.0193 test loss: 0.0013\n",
      "Epoch 6 train loss: 0.0036 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.0031 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0039 test loss: 0.0003\n",
      "Epoch 11 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 12 train loss: 0.0064 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0042 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.0025 test loss: 0.0006\n",
      "Epoch 16 train loss: 0.0059 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 19 train loss: 0.0024 test loss: 0.0004\n",
      "Epoch 20 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 23 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0028 test loss: 0.0004\n",
      "Epoch 25 train loss: 0.0027 test loss: 0.0004\n",
      "Epoch 26 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 29 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 42 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 45 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 46 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 55 train loss: 0.0013 test loss: 0.0011\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0019 test loss: 0.0007\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 60 train loss: 0.0018 test loss: 0.0007\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 63 train loss: 0.0023 test loss: 0.0006\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 69 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 75 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 82 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.001\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 95 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0009\n",
      "Epoch 97 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0006\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0086 test loss: 0.0057\n",
      "Epoch 2 train loss: 0.0595 test loss: 0.0013\n",
      "Epoch 3 train loss: 0.0915 test loss: 0.0034\n",
      "Epoch 4 train loss: 0.0599 test loss: 0.0001\n",
      "Epoch 5 train loss: 0.0206 test loss: 0.0026\n",
      "Epoch 6 train loss: 0.0045 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0031 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0061 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0043 test loss: 0.0003\n",
      "Epoch 13 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0042 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 17 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 18 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 19 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 28 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 37 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0017\n",
      "Epoch 40 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 44 train loss: 0.002 test loss: 0.0012\n",
      "Epoch 45 train loss: 0.0044 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 51 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0013\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 67 train loss: 0.0016 test loss: 0.001\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 70 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0017 test loss: 0.0011\n",
      "Epoch 72 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0009\n",
      "Epoch 80 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 83 train loss: 0.002 test loss: 0.0005\n",
      "Epoch 84 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0009\n",
      "Epoch 100 train loss: 0.001 test loss: 0.0003\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0092 test loss: 0.0047\n",
      "Epoch 2 train loss: 0.0625 test loss: 0.0005\n",
      "Epoch 3 train loss: 0.0875 test loss: 0.0043\n",
      "Epoch 4 train loss: 0.0637 test loss: 0.0006\n",
      "Epoch 5 train loss: 0.0212 test loss: 0.0016\n",
      "Epoch 6 train loss: 0.0044 test loss: 0.0004\n",
      "Epoch 7 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0045 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.002 test loss: 0.0006\n",
      "Epoch 10 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0038 test loss: 0.0005\n",
      "Epoch 12 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 14 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 21 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 22 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0011\n",
      "Epoch 31 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 33 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 35 train loss: 0.0032 test loss: 0.0031\n",
      "Epoch 36 train loss: 0.0028 test loss: 0.0004\n",
      "Epoch 37 train loss: 0.0084 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0022 test loss: 0.0004\n",
      "Epoch 39 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 43 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 56 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 65 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0019 test loss: 0.0006\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 87 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0015 test loss: 0.0014\n",
      "Epoch 89 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 90 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.002 test loss: 0.0012\n",
      "Epoch 92 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0082 test loss: 0.0052\n",
      "Epoch 2 train loss: 0.0655 test loss: 0.0006\n",
      "Epoch 3 train loss: 0.0804 test loss: 0.0026\n",
      "Epoch 4 train loss: 0.0425 test loss: 0.0001\n",
      "Epoch 5 train loss: 0.0126 test loss: 0.0013\n",
      "Epoch 6 train loss: 0.0038 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 11 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 12 train loss: 0.0023 test loss: 0.0005\n",
      "Epoch 13 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 15 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 18 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 19 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0014 test loss: 0.0015\n",
      "Epoch 22 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 23 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0027 test loss: 0.0012\n",
      "Epoch 26 train loss: 0.0019 test loss: 0.0005\n",
      "Epoch 27 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0037 test loss: 0.0005\n",
      "Epoch 29 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 38 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 43 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 51 train loss: 0.0016 test loss: 0.0021\n",
      "Epoch 52 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 55 train loss: 0.0022 test loss: 0.0005\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 60 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 63 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0014\n",
      "Epoch 67 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 70 train loss: 0.0022 test loss: 0.0006\n",
      "Epoch 71 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 81 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0014\n",
      "Epoch 85 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 88 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 92 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 96 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 97 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0014 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0062 test loss: 0.0033\n",
      "Epoch 2 train loss: 0.0792 test loss: 0.0018\n",
      "Epoch 3 train loss: 0.0842 test loss: 0.0115\n",
      "Epoch 4 train loss: 0.0206 test loss: 0.002\n",
      "Epoch 5 train loss: 0.0032 test loss: 0.0005\n",
      "Epoch 6 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.0028 test loss: 0.0008\n",
      "Epoch 8 train loss: 0.0025 test loss: 0.0007\n",
      "Epoch 9 train loss: 0.003 test loss: 0.0005\n",
      "Epoch 10 train loss: 0.0055 test loss: 0.0004\n",
      "Epoch 11 train loss: 0.0037 test loss: 0.0005\n",
      "Epoch 12 train loss: 0.0047 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0033 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.003 test loss: 0.0005\n",
      "Epoch 16 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0032 test loss: 0.0003\n",
      "Epoch 18 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0032 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 23 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 29 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 49 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.001\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0009\n",
      "Epoch 65 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 67 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 76 train loss: 0.0014 test loss: 0.0016\n",
      "Epoch 77 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0017 test loss: 0.0\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 84 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0019 test loss: 0.001\n",
      "Epoch 88 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 94 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0015 test loss: 0.0012\n",
      "Epoch 97 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0012 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0046 test loss: 0.0045\n",
      "Epoch 2 train loss: 0.0997 test loss: 0.0132\n",
      "Epoch 3 train loss: 0.0411 test loss: 0.0008\n",
      "Epoch 4 train loss: 0.0427 test loss: 0.0035\n",
      "Epoch 5 train loss: 0.0099 test loss: 0.0005\n",
      "Epoch 6 train loss: 0.0086 test loss: 0.0009\n",
      "Epoch 7 train loss: 0.0038 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0064 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0037 test loss: 0.0003\n",
      "Epoch 29 train loss: 0.0032 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.005 test loss: 0.0005\n",
      "Epoch 31 train loss: 0.0045 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.007 test loss: 0.0007\n",
      "Epoch 33 train loss: 0.0072 test loss: 0.0006\n",
      "Epoch 34 train loss: 0.0046 test loss: 0.0008\n",
      "Epoch 35 train loss: 0.01 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 40 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0031 test loss: 0.0004\n",
      "Epoch 46 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0055 test loss: 0.0006\n",
      "Epoch 48 train loss: 0.0069 test loss: 0.0011\n",
      "Epoch 49 train loss: 0.0078 test loss: 0.0019\n",
      "Epoch 50 train loss: 0.0114 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 52 train loss: 0.0032 test loss: 0.0004\n",
      "Epoch 53 train loss: 0.0059 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0082 test loss: 0.0019\n",
      "Epoch 55 train loss: 0.0152 test loss: 0.001\n",
      "Epoch 56 train loss: 0.0169 test loss: 0.0013\n",
      "Epoch 57 train loss: 0.0082 test loss: 0.0003\n",
      "Epoch 58 train loss: 0.0113 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0078 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0095 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0056 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0096 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0054 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0088 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0043 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0069 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0046 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0029 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0039 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0041 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0066 test loss: 0.0004\n",
      "Epoch 79 train loss: 0.0062 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0105 test loss: 0.0007\n",
      "Epoch 81 train loss: 0.0057 test loss: 0.0003\n",
      "Epoch 82 train loss: 0.0098 test loss: 0.0005\n",
      "Epoch 83 train loss: 0.0044 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0078 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0049 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.008 test loss: 0.0003\n",
      "Epoch 87 train loss: 0.0057 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0093 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0049 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0077 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0053 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0086 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0043 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0063 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0045 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.007 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.004 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0057 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.0046 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.007 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0063 test loss: 0.0011\n",
      "Epoch 2 train loss: 0.0524 test loss: 0.0036\n",
      "Epoch 3 train loss: 0.1033 test loss: 0.019\n",
      "Epoch 4 train loss: 0.0495 test loss: 0.0013\n",
      "Epoch 5 train loss: 0.0159 test loss: 0.0024\n",
      "Epoch 6 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 13 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 19 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.0037 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 36 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 39 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 42 train loss: 0.0013 test loss: 0.001\n",
      "Epoch 43 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0018 test loss: 0.001\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.001\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 59 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.001\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.002 test loss: 0.0016\n",
      "Epoch 68 train loss: 0.002 test loss: 0.0\n",
      "Epoch 69 train loss: 0.0023 test loss: 0.0014\n",
      "Epoch 70 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0021 test loss: 0.0\n",
      "Epoch 72 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 74 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.002 test loss: 0.0007\n",
      "Epoch 76 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 88 train loss: 0.0011 test loss: 0.0007\n",
      "Epoch 89 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0009\n",
      "Epoch 94 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0005\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0085 test loss: 0.006\n",
      "Epoch 2 train loss: 0.0724 test loss: 0.0003\n",
      "Epoch 3 train loss: 0.0899 test loss: 0.01\n",
      "Epoch 4 train loss: 0.0401 test loss: 0.0048\n",
      "Epoch 5 train loss: 0.0141 test loss: 0.0004\n",
      "Epoch 6 train loss: 0.0034 test loss: 0.0009\n",
      "Epoch 7 train loss: 0.0032 test loss: 0.0005\n",
      "Epoch 8 train loss: 0.0037 test loss: 0.0006\n",
      "Epoch 9 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 12 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 14 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0049 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 17 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 20 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 32 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 33 train loss: 0.0018 test loss: 0.0012\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 35 train loss: 0.002 test loss: 0.0013\n",
      "Epoch 36 train loss: 0.0027 test loss: 0.0006\n",
      "Epoch 37 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0033 test loss: 0.0009\n",
      "Epoch 39 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 45 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 47 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 55 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 57 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0015 test loss: 0.0009\n",
      "Epoch 61 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 62 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 66 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0017 test loss: 0.0015\n",
      "Epoch 70 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 72 train loss: 0.0017 test loss: 0.001\n",
      "Epoch 73 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 74 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0028 test loss: 0.0009\n",
      "Epoch 76 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0019 test loss: 0.0\n",
      "Epoch 78 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 82 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 88 train loss: 0.001 test loss: 0.0006\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0012 test loss: 0.0006\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0068 test loss: 0.005\n",
      "Epoch 2 train loss: 0.0526 test loss: 0.001\n",
      "Epoch 3 train loss: 0.0783 test loss: 0.0002\n",
      "Epoch 4 train loss: 0.0399 test loss: 0.0002\n",
      "Epoch 5 train loss: 0.0083 test loss: 0.0015\n",
      "Epoch 6 train loss: 0.005 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 8 train loss: 0.0048 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0022 test loss: 0.0005\n",
      "Epoch 10 train loss: 0.0044 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 19 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 22 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 30 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0013 test loss: 0.0014\n",
      "Epoch 32 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 33 train loss: 0.0026 test loss: 0.0006\n",
      "Epoch 34 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 40 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0012 test loss: 0.0009\n",
      "Epoch 43 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 45 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 46 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0012\n",
      "Epoch 48 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0019 test loss: 0.0008\n",
      "Epoch 51 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 56 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 60 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 61 train loss: 0.001 test loss: 0.0017\n",
      "Epoch 62 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 63 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.002 test loss: 0.0006\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 66 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 70 train loss: 0.001 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0013\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 75 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 76 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 77 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 78 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 81 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 84 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.001 test loss: 0.0005\n",
      "Epoch 88 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0008 test loss: 0.0019\n",
      "Epoch 93 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 96 train loss: 0.0026 test loss: 0.0009\n",
      "Epoch 97 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0002\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0089 test loss: 0.0059\n",
      "Epoch 2 train loss: 0.0619 test loss: 0.0003\n",
      "Epoch 3 train loss: 0.0898 test loss: 0.0054\n",
      "Epoch 4 train loss: 0.05 test loss: 0.0044\n",
      "Epoch 5 train loss: 0.0158 test loss: 0.0005\n",
      "Epoch 6 train loss: 0.0035 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.003 test loss: 0.0007\n",
      "Epoch 8 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0037 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 11 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0026 test loss: 0.0015\n",
      "Epoch 15 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 17 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 18 train loss: 0.0043 test loss: 0.0003\n",
      "Epoch 19 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 21 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 36 train loss: 0.001 test loss: 0.0003\n",
      "Epoch 37 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0012 test loss: 0.0009\n",
      "Epoch 39 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 43 train loss: 0.0016 test loss: 0.0012\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 45 train loss: 0.0017 test loss: 0.0011\n",
      "Epoch 46 train loss: 0.0025 test loss: 0.0006\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 50 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0012 test loss: 0.0011\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 61 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 63 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 66 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0012\n",
      "Epoch 69 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 70 train loss: 0.0017 test loss: 0.0006\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 72 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 77 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0018 test loss: 0.0005\n",
      "Epoch 80 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0011\n",
      "Epoch 83 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 87 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 90 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 94 train loss: 0.002 test loss: 0.0\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 96 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 97 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.001 test loss: 0.0002\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0079 test loss: 0.0052\n",
      "Epoch 2 train loss: 0.0585 test loss: 0.0005\n",
      "Epoch 3 train loss: 0.0821 test loss: 0.0021\n",
      "Epoch 4 train loss: 0.0384 test loss: 0.0008\n",
      "Epoch 5 train loss: 0.0087 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.0045 test loss: 0.0006\n",
      "Epoch 7 train loss: 0.0037 test loss: 0.0004\n",
      "Epoch 8 train loss: 0.0025 test loss: 0.0004\n",
      "Epoch 9 train loss: 0.0072 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0023 test loss: 0.0005\n",
      "Epoch 12 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0033 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 20 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 21 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 29 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0014 test loss: 0.0017\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 36 train loss: 0.0018 test loss: 0.001\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 42 train loss: 0.0013 test loss: 0.0016\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 45 train loss: 0.0034 test loss: 0.0015\n",
      "Epoch 46 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0021 test loss: 0.0005\n",
      "Epoch 48 train loss: 0.0056 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 50 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 53 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 66 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0014\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 74 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.001 test loss: 0.0004\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0017\n",
      "Epoch 81 train loss: 0.002 test loss: 0.0005\n",
      "Epoch 82 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0017 test loss: 0.0007\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 95 train loss: 0.0008 test loss: 0.0006\n",
      "Epoch 96 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0015\n",
      "Epoch 100 train loss: 0.0018 test loss: 0.0002\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0075 test loss: 0.0052\n",
      "Epoch 2 train loss: 0.0596 test loss: 0.0003\n",
      "Epoch 3 train loss: 0.075 test loss: 0.0006\n",
      "Epoch 4 train loss: 0.0532 test loss: 0.0059\n",
      "Epoch 5 train loss: 0.0297 test loss: 0.0051\n",
      "Epoch 6 train loss: 0.007 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0057 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0059 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0029 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.004 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.003 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0037 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0037 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0031 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0029 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 40 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 42 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 45 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 63 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0018 test loss: 0.0008\n",
      "Epoch 67 train loss: 0.0019 test loss: 0.0012\n",
      "Epoch 68 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0012\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 85 train loss: 0.0017 test loss: 0.0012\n",
      "Epoch 86 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 90 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0012 test loss: 0.001\n",
      "Epoch 99 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0015 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0061 test loss: 0.002\n",
      "Epoch 2 train loss: 0.0667 test loss: 0.0037\n",
      "Epoch 3 train loss: 0.1039 test loss: 0.015\n",
      "Epoch 4 train loss: 0.0254 test loss: 0.0016\n",
      "Epoch 5 train loss: 0.01 test loss: 0.0021\n",
      "Epoch 6 train loss: 0.0093 test loss: 0.0006\n",
      "Epoch 7 train loss: 0.0042 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.017 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0043 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0038 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0031 test loss: 0.0004\n",
      "Epoch 16 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0036 test loss: 0.0004\n",
      "Epoch 18 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0033 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0029 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0032 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 27 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 29 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 49 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 51 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 53 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 55 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 57 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 59 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 62 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 64 train loss: 0.0018 test loss: 0.001\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0017 test loss: 0.0\n",
      "Epoch 67 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0019 test loss: 0.0007\n",
      "Epoch 69 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 82 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 84 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 86 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 88 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0012\n",
      "Epoch 93 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0018 test loss: 0.0007\n",
      "Epoch 97 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0061 test loss: 0.0026\n",
      "Epoch 2 train loss: 0.0832 test loss: 0.0079\n",
      "Epoch 3 train loss: 0.0523 test loss: 0.0009\n",
      "Epoch 4 train loss: 0.0162 test loss: 0.0012\n",
      "Epoch 5 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0059 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 11 train loss: 0.0039 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 17 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 20 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.0011\n",
      "Epoch 48 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.002 test loss: 0.0005\n",
      "Epoch 50 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 60 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0012\n",
      "Epoch 67 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 70 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0015 test loss: 0.0009\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 74 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 76 train loss: 0.0011 test loss: 0.0006\n",
      "Epoch 77 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 87 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0009 test loss: 0.0009\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0005\n",
      "Epoch 94 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0013\n",
      "Epoch 97 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0017 test loss: 0.0013\n",
      "Epoch 100 train loss: 0.0024 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0044 test loss: 0.0007\n",
      "Epoch 2 train loss: 0.0911 test loss: 0.0142\n",
      "Epoch 3 train loss: 0.0642 test loss: 0.0088\n",
      "Epoch 4 train loss: 0.0209 test loss: 0.0002\n",
      "Epoch 5 train loss: 0.0043 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.004 test loss: 0.0034\n",
      "Epoch 8 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 9 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 13 train loss: 0.0023 test loss: 0.0005\n",
      "Epoch 14 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0022 test loss: 0.0004\n",
      "Epoch 19 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 37 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0013\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0018 test loss: 0.0007\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0017 test loss: 0.0012\n",
      "Epoch 49 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0021 test loss: 0.0011\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0021 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.001\n",
      "Epoch 62 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 63 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0012 test loss: 0.0018\n",
      "Epoch 75 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 77 train loss: 0.0017 test loss: 0.0014\n",
      "Epoch 78 train loss: 0.002 test loss: 0.0\n",
      "Epoch 79 train loss: 0.0018 test loss: 0.0007\n",
      "Epoch 80 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 90 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0009 test loss: 0.0011\n",
      "Epoch 100 train loss: 0.0015 test loss: 0.0002\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0064 test loss: 0.003\n",
      "Epoch 2 train loss: 0.0585 test loss: 0.0019\n",
      "Epoch 3 train loss: 0.1003 test loss: 0.0169\n",
      "Epoch 4 train loss: 0.057 test loss: 0.0026\n",
      "Epoch 5 train loss: 0.0178 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.003 test loss: 0.0004\n",
      "Epoch 7 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 19 train loss: 0.0016 test loss: 0.0006\n",
      "Epoch 20 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 26 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 27 train loss: 0.0017 test loss: 0.0006\n",
      "Epoch 28 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 35 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 38 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 39 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0013\n",
      "Epoch 43 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0034 test loss: 0.0015\n",
      "Epoch 45 train loss: 0.004 test loss: 0.0007\n",
      "Epoch 46 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.003 test loss: 0.0007\n",
      "Epoch 48 train loss: 0.0019 test loss: 0.0\n",
      "Epoch 49 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 54 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 55 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 61 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 63 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 65 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0012 test loss: 0.0023\n",
      "Epoch 68 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0019 test loss: 0.0006\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0032 test loss: 0.0016\n",
      "Epoch 72 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 73 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 80 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 83 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0013 test loss: 0.001\n",
      "Epoch 86 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 87 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.002 test loss: 0.0011\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0009 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0059 test loss: 0.0011\n",
      "Epoch 2 train loss: 0.0588 test loss: 0.0049\n",
      "Epoch 3 train loss: 0.1104 test loss: 0.0226\n",
      "Epoch 4 train loss: 0.0464 test loss: 0.0014\n",
      "Epoch 5 train loss: 0.0169 test loss: 0.0019\n",
      "Epoch 6 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 8 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0049 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0036 test loss: 0.0004\n",
      "Epoch 18 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 23 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 29 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 58 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 61 train loss: 0.0019 test loss: 0.0007\n",
      "Epoch 62 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 67 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 71 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 77 train loss: 0.0011 test loss: 0.0007\n",
      "Epoch 78 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 80 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 81 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0006\n",
      "Epoch 86 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 91 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0\n",
      "Epoch 93 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 95 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.001\n",
      "Epoch 100 train loss: 0.0014 test loss: 0.001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0063 test loss: 0.0027\n",
      "Epoch 2 train loss: 0.0675 test loss: 0.0061\n",
      "Epoch 3 train loss: 0.0956 test loss: 0.0139\n",
      "Epoch 4 train loss: 0.0232 test loss: 0.0023\n",
      "Epoch 5 train loss: 0.0051 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0059 test loss: 0.0058\n",
      "Epoch 7 train loss: 0.0047 test loss: 0.0007\n",
      "Epoch 8 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.0067 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.0034 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0052 test loss: 0.0002\n",
      "Epoch 14 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0028 test loss: 0.0003\n",
      "Epoch 17 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0033 test loss: 0.0003\n",
      "Epoch 19 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0033 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 23 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 28 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0024 test loss: 0.0004\n",
      "Epoch 30 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 36 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 45 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 47 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 53 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0011\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 57 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0018 test loss: 0.0008\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 61 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 62 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 66 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0012\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 81 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 92 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 93 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 95 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 98 train loss: 0.0009 test loss: 0.0003\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0062 test loss: 0.0017\n",
      "Epoch 2 train loss: 0.0956 test loss: 0.0186\n",
      "Epoch 3 train loss: 0.062 test loss: 0.0083\n",
      "Epoch 4 train loss: 0.0175 test loss: 0.0008\n",
      "Epoch 5 train loss: 0.0036 test loss: 0.0003\n",
      "Epoch 6 train loss: 0.0021 test loss: 0.0006\n",
      "Epoch 7 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 9 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0021 test loss: 0.0003\n",
      "Epoch 15 train loss: 0.0019 test loss: 0.0002\n",
      "Epoch 16 train loss: 0.0019 test loss: 0.0006\n",
      "Epoch 17 train loss: 0.0022 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0022 test loss: 0.0011\n",
      "Epoch 19 train loss: 0.0031 test loss: 0.0006\n",
      "Epoch 20 train loss: 0.0061 test loss: 0.0005\n",
      "Epoch 21 train loss: 0.0044 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0026 test loss: 0.0004\n",
      "Epoch 23 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 25 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0017 test loss: 0.0005\n",
      "Epoch 27 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 44 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 51 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 54 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0014\n",
      "Epoch 56 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0023 test loss: 0.0012\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 64 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 70 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 76 train loss: 0.0012 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 78 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0008\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 87 train loss: 0.0011 test loss: 0.0008\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 92 train loss: 0.0011 test loss: 0.0002\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 95 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0011 test loss: 0.0009\n",
      "Epoch 97 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 98 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0006\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0063 test loss: 0.0028\n",
      "Epoch 2 train loss: 0.0667 test loss: 0.0071\n",
      "Epoch 3 train loss: 0.0967 test loss: 0.0169\n",
      "Epoch 4 train loss: 0.0262 test loss: 0.0021\n",
      "Epoch 5 train loss: 0.0076 test loss: 0.0001\n",
      "Epoch 6 train loss: 0.0025 test loss: 0.0006\n",
      "Epoch 7 train loss: 0.0026 test loss: 0.0005\n",
      "Epoch 8 train loss: 0.0026 test loss: 0.0005\n",
      "Epoch 9 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 10 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 11 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 12 train loss: 0.0016 test loss: 0.0011\n",
      "Epoch 13 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0017 test loss: 0.0011\n",
      "Epoch 16 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 17 train loss: 0.0028 test loss: 0.0007\n",
      "Epoch 18 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 24 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0013\n",
      "Epoch 34 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 37 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0016 test loss: 0.0009\n",
      "Epoch 39 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0024 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0012 test loss: 0.0007\n",
      "Epoch 45 train loss: 0.0012 test loss: 0.0008\n",
      "Epoch 46 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 47 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 48 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0015\n",
      "Epoch 50 train loss: 0.002 test loss: 0.0004\n",
      "Epoch 51 train loss: 0.0028 test loss: 0.0011\n",
      "Epoch 52 train loss: 0.0024 test loss: 0.0002\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 59 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.002 test loss: 0.0002\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0011 test loss: 0.0006\n",
      "Epoch 67 train loss: 0.001 test loss: 0.0008\n",
      "Epoch 68 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0009\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 71 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 75 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 76 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.0012 test loss: 0.0004\n",
      "Epoch 78 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 80 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 81 train loss: 0.0012 test loss: 0.0012\n",
      "Epoch 82 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 83 train loss: 0.0019 test loss: 0.0003\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.001 test loss: 0.0002\n",
      "Epoch 91 train loss: 0.0011 test loss: 0.0015\n",
      "Epoch 92 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 93 train loss: 0.0015 test loss: 0.0012\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0009 test loss: 0.0001\n",
      "Epoch 100 train loss: 0.0011 test loss: 0.0002\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0062 test loss: 0.0014\n",
      "Epoch 2 train loss: 0.0696 test loss: 0.0019\n",
      "Epoch 3 train loss: 0.0959 test loss: 0.016\n",
      "Epoch 4 train loss: 0.0201 test loss: 0.0018\n",
      "Epoch 5 train loss: 0.0073 test loss: 0.0011\n",
      "Epoch 6 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 7 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.006 test loss: 0.0003\n",
      "Epoch 9 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.006 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0038 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0028 test loss: 0.0004\n",
      "Epoch 14 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.004 test loss: 0.0004\n",
      "Epoch 16 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 17 train loss: 0.0047 test loss: 0.0003\n",
      "Epoch 18 train loss: 0.0036 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0042 test loss: 0.0004\n",
      "Epoch 20 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 21 train loss: 0.0045 test loss: 0.0003\n",
      "Epoch 22 train loss: 0.0034 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0041 test loss: 0.0003\n",
      "Epoch 24 train loss: 0.0032 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0042 test loss: 0.0003\n",
      "Epoch 26 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.004 test loss: 0.0003\n",
      "Epoch 28 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0038 test loss: 0.0004\n",
      "Epoch 30 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0039 test loss: 0.0004\n",
      "Epoch 32 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0035 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 35 train loss: 0.0032 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 37 train loss: 0.0031 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.003 test loss: 0.0002\n",
      "Epoch 40 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 41 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 42 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 43 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 44 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 46 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 51 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0\n",
      "Epoch 59 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 63 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 68 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 70 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 72 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 74 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 76 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 77 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 82 train loss: 0.0016 test loss: 0.0009\n",
      "Epoch 83 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 84 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 86 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 87 train loss: 0.0014 test loss: 0.0\n",
      "Epoch 88 train loss: 0.0013 test loss: 0.0\n",
      "Epoch 89 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 96 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 97 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 99 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 100 train loss: 0.0013 test loss: 0.0012\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0061 test loss: 0.0018\n",
      "Epoch 2 train loss: 0.0508 test loss: 0.0025\n",
      "Epoch 3 train loss: 0.0935 test loss: 0.016\n",
      "Epoch 4 train loss: 0.0258 test loss: 0.002\n",
      "Epoch 5 train loss: 0.0082 test loss: 0.0006\n",
      "Epoch 6 train loss: 0.0053 test loss: 0.0004\n",
      "Epoch 7 train loss: 0.0038 test loss: 0.0003\n",
      "Epoch 8 train loss: 0.0028 test loss: 0.0004\n",
      "Epoch 9 train loss: 0.0109 test loss: 0.0003\n",
      "Epoch 10 train loss: 0.0037 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.002 test loss: 0.0003\n",
      "Epoch 13 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.002 test loss: 0.0006\n",
      "Epoch 15 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.003 test loss: 0.0005\n",
      "Epoch 17 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0022 test loss: 0.0003\n",
      "Epoch 19 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 21 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 31 train loss: 0.0017 test loss: 0.0004\n",
      "Epoch 32 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 36 train loss: 0.0014 test loss: 0.0011\n",
      "Epoch 37 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 39 train loss: 0.0023 test loss: 0.0008\n",
      "Epoch 40 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0022 test loss: 0.0011\n",
      "Epoch 42 train loss: 0.0032 test loss: 0.0006\n",
      "Epoch 43 train loss: 0.0027 test loss: 0.0009\n",
      "Epoch 44 train loss: 0.004 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.002 test loss: 0.0005\n",
      "Epoch 46 train loss: 0.0026 test loss: 0.0003\n",
      "Epoch 47 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 51 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 52 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 55 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 56 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 58 train loss: 0.002 test loss: 0.0014\n",
      "Epoch 59 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0021 test loss: 0.0004\n",
      "Epoch 61 train loss: 0.0023 test loss: 0.0006\n",
      "Epoch 62 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 66 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 72 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 73 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 74 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 75 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 76 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 77 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0014 test loss: 0.0011\n",
      "Epoch 80 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 84 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0017 test loss: 0.0006\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 88 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0011\n",
      "Epoch 92 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 95 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 96 train loss: 0.0022 test loss: 0.0019\n",
      "Epoch 97 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 98 train loss: 0.0016 test loss: 0.0005\n",
      "Epoch 99 train loss: 0.0016 test loss: 0.0\n",
      "Epoch 100 train loss: 0.0017 test loss: 0.0001\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0056 test loss: 0.0021\n",
      "Epoch 2 train loss: 0.0954 test loss: 0.0156\n",
      "Epoch 3 train loss: 0.059 test loss: 0.0039\n",
      "Epoch 4 train loss: 0.0196 test loss: 0.0014\n",
      "Epoch 5 train loss: 0.0041 test loss: 0.0002\n",
      "Epoch 6 train loss: 0.0032 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0026 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0061 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 11 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0032 test loss: 0.0004\n",
      "Epoch 13 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 15 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 17 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 18 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 22 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 24 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 25 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 26 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 28 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 33 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 36 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 37 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0015 test loss: 0.0008\n",
      "Epoch 39 train loss: 0.0011 test loss: 0.0013\n",
      "Epoch 40 train loss: 0.0021 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 43 train loss: 0.0018 test loss: 0.0009\n",
      "Epoch 44 train loss: 0.0021 test loss: 0.0007\n",
      "Epoch 45 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0006\n",
      "Epoch 48 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0015 test loss: 0.0004\n",
      "Epoch 50 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0013 test loss: 0.0007\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0017 test loss: 0.0008\n",
      "Epoch 56 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 59 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 61 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 64 train loss: 0.0016 test loss: 0.0007\n",
      "Epoch 65 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 67 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 68 train loss: 0.0014 test loss: 0.0008\n",
      "Epoch 69 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 71 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 72 train loss: 0.0015 test loss: 0.0006\n",
      "Epoch 73 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 74 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 75 train loss: 0.0013 test loss: 0.001\n",
      "Epoch 76 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 77 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 78 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 79 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 82 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 85 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0014\n",
      "Epoch 87 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 89 train loss: 0.0011 test loss: 0.0003\n",
      "Epoch 90 train loss: 0.001 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 93 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 94 train loss: 0.001 test loss: 0.0008\n",
      "Epoch 95 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 97 train loss: 0.0014 test loss: 0.0005\n",
      "Epoch 98 train loss: 0.0009 test loss: 0.0001\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.001 test loss: 0.0004\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0085 test loss: 0.006\n",
      "Epoch 2 train loss: 0.0583 test loss: 0.0004\n",
      "Epoch 3 train loss: 0.0655 test loss: 0.0004\n",
      "Epoch 4 train loss: 0.0486 test loss: 0.0002\n",
      "Epoch 5 train loss: 0.0134 test loss: 0.0014\n",
      "Epoch 6 train loss: 0.0057 test loss: 0.0001\n",
      "Epoch 7 train loss: 0.0034 test loss: 0.0007\n",
      "Epoch 8 train loss: 0.0047 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0026 test loss: 0.0001\n",
      "Epoch 10 train loss: 0.0041 test loss: 0.0006\n",
      "Epoch 11 train loss: 0.0029 test loss: 0.0005\n",
      "Epoch 12 train loss: 0.0067 test loss: 0.0002\n",
      "Epoch 13 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 14 train loss: 0.0025 test loss: 0.0008\n",
      "Epoch 15 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 16 train loss: 0.004 test loss: 0.0001\n",
      "Epoch 17 train loss: 0.0015 test loss: 0.0005\n",
      "Epoch 18 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 19 train loss: 0.0027 test loss: 0.0001\n",
      "Epoch 20 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 21 train loss: 0.0022 test loss: 0.0007\n",
      "Epoch 22 train loss: 0.0019 test loss: 0.0004\n",
      "Epoch 23 train loss: 0.0019 test loss: 0.0008\n",
      "Epoch 24 train loss: 0.0018 test loss: 0.0002\n",
      "Epoch 25 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 26 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 27 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 29 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 30 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 31 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 32 train loss: 0.0013 test loss: 0.0004\n",
      "Epoch 33 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 35 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 36 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 38 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 40 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 41 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 42 train loss: 0.0013 test loss: 0.0005\n",
      "Epoch 43 train loss: 0.0014 test loss: 0.0024\n",
      "Epoch 44 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 45 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 46 train loss: 0.0018 test loss: 0.0004\n",
      "Epoch 47 train loss: 0.0018 test loss: 0.0006\n",
      "Epoch 48 train loss: 0.0017 test loss: 0.0011\n",
      "Epoch 49 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0026 test loss: 0.001\n",
      "Epoch 51 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 55 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 56 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 57 train loss: 0.0012 test loss: 0.0002\n",
      "Epoch 58 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0012\n",
      "Epoch 60 train loss: 0.0015 test loss: 0.0003\n",
      "Epoch 61 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 62 train loss: 0.0018 test loss: 0.0007\n",
      "Epoch 63 train loss: 0.0014 test loss: 0.0006\n",
      "Epoch 64 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 65 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 66 train loss: 0.0015 test loss: 0.001\n",
      "Epoch 67 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 68 train loss: 0.0016 test loss: 0.0003\n",
      "Epoch 69 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 70 train loss: 0.0013 test loss: 0.0012\n",
      "Epoch 71 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 72 train loss: 0.0016 test loss: 0.0004\n",
      "Epoch 73 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 74 train loss: 0.0014 test loss: 0.0015\n",
      "Epoch 75 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 76 train loss: 0.0018 test loss: 0.0003\n",
      "Epoch 77 train loss: 0.0018 test loss: 0.0011\n",
      "Epoch 78 train loss: 0.0019 test loss: 0.0001\n",
      "Epoch 79 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 80 train loss: 0.0013 test loss: 0.0001\n",
      "Epoch 81 train loss: 0.0014 test loss: 0.0003\n",
      "Epoch 82 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 83 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 84 train loss: 0.0011 test loss: 0.0001\n",
      "Epoch 85 train loss: 0.0011 test loss: 0.0006\n",
      "Epoch 86 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 87 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 88 train loss: 0.0012 test loss: 0.0003\n",
      "Epoch 89 train loss: 0.0014 test loss: 0.0007\n",
      "Epoch 90 train loss: 0.0012 test loss: 0.0001\n",
      "Epoch 91 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 92 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 93 train loss: 0.0013 test loss: 0.0003\n",
      "Epoch 94 train loss: 0.0013 test loss: 0.0015\n",
      "Epoch 95 train loss: 0.0017 test loss: 0.0002\n",
      "Epoch 96 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 97 train loss: 0.0015 test loss: 0.0007\n",
      "Epoch 98 train loss: 0.0014 test loss: 0.0004\n",
      "Epoch 99 train loss: 0.0013 test loss: 0.0002\n",
      "Epoch 100 train loss: 0.0013 test loss: 0.0005\n",
      "Completed.\n",
      "[[0.90625679]\n",
      " [0.90810657]\n",
      " [0.90807772]\n",
      " [0.91206157]\n",
      " [0.91195774]\n",
      " [0.90776008]\n",
      " [0.90753472]\n",
      " [0.9094786 ]\n",
      " [0.90997452]\n",
      " [0.90555608]\n",
      " [0.90135157]\n",
      " [0.8971976 ]\n",
      " [0.89205199]\n",
      " [0.88376194]\n",
      " [0.87847847]\n",
      " [0.87993002]\n",
      " [0.87849379]\n",
      " [0.87723577]\n",
      " [0.87625152]\n",
      " [0.8745845 ]\n",
      " [0.87159556]\n",
      " [0.86529154]\n",
      " [0.86340499]\n",
      " [0.85996538]\n",
      " [0.85316682]\n",
      " [0.84902644]\n",
      " [0.84601402]\n",
      " [0.84603536]\n",
      " [0.84802788]\n",
      " [0.85066372]\n",
      " [0.8579101 ]\n",
      " [0.86631256]\n",
      " [0.87223369]\n",
      " [0.87651598]\n",
      " [0.88059324]\n",
      " [0.88751203]\n",
      " [0.89608508]\n",
      " [0.90327764]\n",
      " [0.90722883]\n",
      " [0.90986294]\n",
      " [0.91645706]\n",
      " [0.92052698]\n",
      " [0.92377174]\n",
      " [0.92440939]\n",
      " [0.92441285]\n",
      " [0.92620331]\n",
      " [0.92509931]\n",
      " [0.92372948]\n",
      " [0.92113495]\n",
      " [0.92192644]\n",
      " [0.9214254 ]\n",
      " [0.92079985]\n",
      " [0.91540915]\n",
      " [0.90757221]\n",
      " [0.90511274]\n",
      " [0.89889938]\n",
      " [0.89207131]\n",
      " [0.8854394 ]\n",
      " [0.87462497]\n",
      " [0.87007886]\n",
      " [0.86736584]\n",
      " [0.86380047]\n",
      " [0.85797966]\n",
      " [0.85409504]\n",
      " [0.8499397 ]\n",
      " [0.84415358]\n",
      " [0.83629793]\n",
      " [0.82575613]\n",
      " [0.81828344]\n",
      " [0.81415802]\n",
      " [0.81426167]\n",
      " [0.80895388]\n",
      " [0.80253989]\n",
      " [0.79040748]\n",
      " [0.78766239]\n",
      " [0.78560579]\n",
      " [0.78417963]\n",
      " [0.78276026]\n",
      " [0.78425843]\n",
      " [0.78998429]\n",
      " [0.79323572]\n",
      " [0.79606771]\n",
      " [0.79779065]\n",
      " [0.80350792]\n",
      " [0.80730534]\n",
      " [0.81082702]\n",
      " [0.81041837]\n",
      " [0.8108235 ]\n",
      " [0.81008857]\n",
      " [0.80451512]\n",
      " [0.79799002]\n",
      " [0.79024869]\n",
      " [0.7794829 ]\n",
      " [0.77583474]\n",
      " [0.76973957]\n",
      " [0.76438528]\n",
      " [0.76075083]\n",
      " [0.76358944]\n",
      " [0.76071692]\n",
      " [0.76133478]\n",
      " [0.76019722]\n",
      " [0.75832105]\n",
      " [0.7581268 ]\n",
      " [0.75856501]\n",
      " [0.75994027]\n",
      " [0.76260799]\n",
      " [0.7688843 ]\n",
      " [0.77267247]\n",
      " [0.77481461]\n",
      " [0.77269942]\n",
      " [0.77172291]\n",
      " [0.77112091]\n",
      " [0.77294326]\n",
      " [0.7767275 ]\n",
      " [0.77956468]\n",
      " [0.78049982]\n",
      " [0.78710884]\n",
      " [0.79106128]\n",
      " [0.7937746 ]\n",
      " [0.7956295 ]\n",
      " [0.80016011]\n",
      " [0.80906385]\n",
      " [0.81518978]\n",
      " [0.82113934]\n",
      " [0.8276155 ]\n",
      " [0.83093065]\n",
      " [0.83671767]\n",
      " [0.8404845 ]\n",
      " [0.8453517 ]\n",
      " [0.84806782]\n",
      " [0.85093343]\n",
      " [0.85844296]\n",
      " [0.86616927]\n",
      " [0.87153476]\n",
      " [0.87618446]\n",
      " [0.87893885]\n",
      " [0.88276958]\n",
      " [0.88414752]\n",
      " [0.88330579]\n",
      " [0.88131851]\n",
      " [0.87300777]\n",
      " [0.86989462]\n",
      " [0.86447906]\n",
      " [0.85887772]\n",
      " [0.85131752]\n",
      " [0.84086394]\n",
      " [0.8354156 ]\n",
      " [0.83039683]\n",
      " [0.82605535]\n",
      " [0.82368964]\n",
      " [0.83033472]\n",
      " [0.83027422]\n",
      " [0.83037949]\n",
      " [0.82850444]\n",
      " [0.82690269]\n",
      " [0.82781279]\n",
      " [0.82664824]\n",
      " [0.82013303]\n",
      " [0.81535733]\n",
      " [0.80841029]\n",
      " [0.80224574]\n",
      " [0.79408461]\n",
      " [0.78729236]\n",
      " [0.78046918]\n",
      " [0.7719664 ]\n",
      " [0.76543069]\n",
      " [0.76234788]\n",
      " [0.76101393]\n",
      " [0.76161546]\n",
      " [0.75592768]\n",
      " [0.75500971]\n",
      " [0.75317806]\n",
      " [0.75432336]\n",
      " [0.74668491]\n",
      " [0.73660344]\n",
      " [0.73142624]\n",
      " [0.73225462]\n",
      " [0.73454893]\n",
      " [0.73549777]\n",
      " [0.73603493]\n",
      " [0.74257076]\n",
      " [0.75128877]\n",
      " [0.75938922]\n",
      " [0.76547968]\n",
      " [0.76974112]\n",
      " [0.77908242]\n",
      " [0.78560728]\n",
      " [0.7859028 ]\n",
      " [0.78393054]\n",
      " [0.78072661]\n",
      " [0.78290761]\n",
      " [0.78550339]\n",
      " [0.78195471]\n",
      " [0.78379303]\n",
      " [0.78855091]\n",
      " [0.7952649 ]\n",
      " [0.80222064]\n",
      " [0.81012583]\n",
      " [0.81431288]\n",
      " [0.81825411]\n",
      " [0.82407695]\n",
      " [0.82911837]\n",
      " [0.83422011]\n",
      " [0.83772027]\n",
      " [0.84181064]\n",
      " [0.84428221]\n",
      " [0.84457392]\n",
      " [0.84708828]\n",
      " [0.84894758]\n",
      " [0.85205424]\n",
      " [0.85166574]\n",
      " [0.85059917]\n",
      " [0.84937429]\n",
      " [0.84702402]\n",
      " [0.84657693]\n",
      " [0.84630334]\n",
      " [0.84650087]\n",
      " [0.84578264]\n",
      " [0.84381819]\n",
      " [0.84378308]\n",
      " [0.84222704]\n",
      " [0.8416388 ]\n",
      " [0.83608347]\n",
      " [0.83070606]\n",
      " [0.82913303]\n",
      " [0.82460839]\n",
      " [0.82186723]\n",
      " [0.81933355]\n",
      " [0.81824386]\n",
      " [0.81714743]\n",
      " [0.81378216]\n",
      " [0.80703741]\n",
      " [0.80519462]\n",
      " [0.80385703]\n",
      " [0.80345237]\n",
      " [0.80370963]\n",
      " [0.80220598]\n",
      " [0.80443186]\n",
      " [0.8037293 ]\n",
      " [0.80255681]\n",
      " [0.79960477]\n",
      " [0.80198067]\n",
      " [0.80600852]\n",
      " [0.80702895]\n",
      " [0.80882728]\n",
      " [0.81032115]\n",
      " [0.81433737]\n",
      " [0.81864935]\n",
      " [0.81891322]\n",
      " [0.82353693]\n",
      " [0.82665902]\n",
      " [0.8328886 ]\n",
      " [0.83697844]\n",
      " [0.83917195]\n",
      " [0.83764166]\n",
      " [0.83639508]\n",
      " [0.83987951]\n",
      " [0.84303302]\n",
      " [0.84631348]\n",
      " [0.84737206]\n",
      " [0.848171  ]\n",
      " [0.84843087]\n",
      " [0.84663904]\n",
      " [0.84478313]\n",
      " [0.84128004]\n",
      " [0.84129977]\n",
      " [0.8418566 ]\n",
      " [0.84193772]\n",
      " [0.83857244]\n",
      " [0.83534604]\n",
      " [0.83732736]\n",
      " [0.83719629]\n",
      " [0.83680606]\n",
      " [0.8311013 ]\n",
      " [0.82687211]\n",
      " [0.82364833]\n",
      " [0.8245219 ]\n",
      " [0.82627368]\n",
      " [0.82537746]\n",
      " [0.82381272]\n",
      " [0.82561934]\n",
      " [0.82844979]\n",
      " [0.82562917]\n",
      " [0.82247144]]\n",
      "[[0.94760268]\n",
      " [0.95644212]\n",
      " [0.97229548]\n",
      " [0.95255084]\n",
      " [0.93126891]\n",
      " [0.92781   ]\n",
      " [0.94481633]\n",
      " [0.94601734]\n",
      " [0.92310203]\n",
      " [0.9163043 ]\n",
      " [0.90511087]\n",
      " [0.88680745]\n",
      " [0.9020603 ]\n",
      " [0.92475943]\n",
      " [0.92206916]\n",
      " [0.9060957 ]\n",
      " [0.92509571]\n",
      " [0.9198593 ]\n",
      " [0.91135613]\n",
      " [0.88075435]\n",
      " [0.87311591]\n",
      " [0.8999225 ]\n",
      " [0.89528659]\n",
      " [0.88229164]\n",
      " [0.87491743]\n",
      " [0.89694399]\n",
      " [0.91964312]\n",
      " [0.93273415]\n",
      " [0.94430232]\n",
      " [0.94398909]\n",
      " [0.956518  ]\n",
      " [0.94256754]\n",
      " [0.95868647]\n",
      " [0.96398716]\n",
      " [0.97174544]\n",
      " [0.98533449]\n",
      " [0.97846768]\n",
      " [0.96145728]\n",
      " [0.96454132]\n",
      " [0.97388981]\n",
      " [0.95998755]\n",
      " [0.94912113]\n",
      " [0.95454229]\n",
      " [0.951651  ]\n",
      " [0.93321905]\n",
      " [0.92929172]\n",
      " [0.94138694]\n",
      " [0.92808702]\n",
      " [0.92852071]\n",
      " [0.94555521]\n",
      " [0.9447601 ]\n",
      " [0.92873756]\n",
      " [0.8997765 ]\n",
      " [0.90572774]\n",
      " [0.87582701]\n",
      " [0.87864602]\n",
      " [0.90404115]\n",
      " [0.86594845]\n",
      " [0.87192378]\n",
      " [0.87650165]\n",
      " [0.90705291]\n",
      " [0.87030948]\n",
      " [0.86435824]\n",
      " [0.83262638]\n",
      " [0.83484303]\n",
      " [0.8195674 ]\n",
      " [0.81857954]\n",
      " [0.84117976]\n",
      " [0.83725243]\n",
      " [0.85708184]\n",
      " [0.81742303]\n",
      " [0.81164045]\n",
      " [0.81205005]\n",
      " [0.82961462]\n",
      " [0.82231412]\n",
      " [0.83069885]\n",
      " [0.84982953]\n",
      " [0.87380311]\n",
      " [0.8681892 ]\n",
      " [0.86014179]\n",
      " [0.87893515]\n",
      " [0.86243072]\n",
      " [0.86544248]\n",
      " [0.87495963]\n",
      " [0.86406912]\n",
      " [0.84050513]\n",
      " [0.81245965]\n",
      " [0.77680045]\n",
      " [0.77407782]\n",
      " [0.78691996]\n",
      " [0.75668192]\n",
      " [0.75857781]\n",
      " [0.78086407]\n",
      " [0.77921862]\n",
      " [0.78809925]\n",
      " [0.81718512]\n",
      " [0.81357964]\n",
      " [0.7943665 ]\n",
      " [0.79361637]\n",
      " [0.78613922]\n",
      " [0.79579418]\n",
      " [0.79753643]\n",
      " [0.80065796]\n",
      " [0.81454755]\n",
      " [0.81377322]\n",
      " [0.80302935]\n",
      " [0.79480207]\n",
      " [0.78996249]\n",
      " [0.78773628]\n",
      " [0.80520716]\n",
      " [0.79751223]\n",
      " [0.82248446]\n",
      " [0.82853394]\n",
      " [0.83826149]\n",
      " [0.82930827]\n",
      " [0.83046977]\n",
      " [0.81914515]\n",
      " [0.84370602]\n",
      " [0.85587757]\n",
      " [0.87020272]\n",
      " [0.86725058]\n",
      " [0.86069295]\n",
      " [0.87615541]\n",
      " [0.87547787]\n",
      " [0.87378401]\n",
      " [0.87262251]\n",
      " [0.86865406]\n",
      " [0.88956104]\n",
      " [0.88956104]\n",
      " [0.90676575]\n",
      " [0.91102458]\n",
      " [0.9130572 ]\n",
      " [0.90567684]\n",
      " [0.90867738]\n",
      " [0.89476359]\n",
      " [0.87349364]\n",
      " [0.87107385]\n",
      " [0.87426797]\n",
      " [0.88839954]\n",
      " [0.85403853]\n",
      " [0.84755349]\n",
      " [0.83685802]\n",
      " [0.82952605]\n",
      " [0.83252659]\n",
      " [0.82241187]\n",
      " [0.81883058]\n",
      " [0.83581751]\n",
      " [0.84210896]\n",
      " [0.85716005]\n",
      " [0.86773454]\n",
      " [0.82449289]\n",
      " [0.82812257]\n",
      " [0.81728191]\n",
      " [0.81011062]\n",
      " [0.81737575]\n",
      " [0.8065388 ]\n",
      " [0.79025908]\n",
      " [0.78255659]\n",
      " [0.76732168]\n",
      " [0.75847718]\n",
      " [0.75621745]\n",
      " [0.77359059]\n",
      " [0.75478387]\n",
      " [0.74115264]\n",
      " [0.76406574]\n",
      " [0.79166837]\n",
      " [0.78953014]\n",
      " [0.78007818]\n",
      " [0.75478387]\n",
      " [0.7480533 ]\n",
      " [0.74251333]\n",
      " [0.73964616]\n",
      " [0.76251066]\n",
      " [0.74224605]\n",
      " [0.764576  ]\n",
      " [0.77504848]\n",
      " [0.76865808]\n",
      " [0.76114997]\n",
      " [0.78272667]\n",
      " [0.7938552 ]\n",
      " [0.80855554]\n",
      " [0.8015091 ]\n",
      " [0.79655229]\n",
      " [0.81851776]\n",
      " [0.81169   ]\n",
      " [0.80758362]\n",
      " [0.78413596]\n",
      " [0.7747569 ]\n",
      " [0.78773208]\n",
      " [0.79647939]\n",
      " [0.8014605 ]\n",
      " [0.7823379 ]\n",
      " [0.83229478]\n",
      " [0.84157665]\n",
      " [0.8333396 ]\n",
      " [0.84152805]\n",
      " [0.83414143]\n",
      " [0.83120136]\n",
      " [0.83555072]\n",
      " [0.8320518 ]\n",
      " [0.84495408]\n",
      " [0.8510772 ]\n",
      " [0.85085852]\n",
      " [0.83525914]\n",
      " [0.83360687]\n",
      " [0.863858  ]\n",
      " [0.86312905]\n",
      " [0.86198704]\n",
      " [0.84420084]\n",
      " [0.83020514]\n",
      " [0.82857717]\n",
      " [0.83606098]\n",
      " [0.82886875]\n",
      " [0.84264576]\n",
      " [0.84998379]\n",
      " [0.84373918]\n",
      " [0.81999994]\n",
      " [0.80887482]\n",
      " [0.80094124]\n",
      " [0.80221061]\n",
      " [0.81610048]\n",
      " [0.80265001]\n",
      " [0.80799602]\n",
      " [0.80430996]\n",
      " [0.79273913]\n",
      " [0.80928981]\n",
      " [0.80682429]\n",
      " [0.80289412]\n",
      " [0.81007096]\n",
      " [0.79937893]\n",
      " [0.82061652]\n",
      " [0.82007948]\n",
      " [0.82671928]\n",
      " [0.83877833]\n",
      " [0.84229351]\n",
      " [0.84605281]\n",
      " [0.84427081]\n",
      " [0.82894068]\n",
      " [0.82198354]\n",
      " [0.83965712]\n",
      " [0.85125236]\n",
      " [0.85020269]\n",
      " [0.85056885]\n",
      " [0.8613097 ]\n",
      " [0.86357993]\n",
      " [0.85115472]\n",
      " [0.86553281]\n",
      " [0.87607837]\n",
      " [0.89067617]\n",
      " [0.87986208]\n",
      " [0.8737105 ]\n",
      " [0.88679481]\n",
      " [0.87571221]\n",
      " [0.86702188]\n",
      " [0.86934093]\n",
      " [0.88103381]\n",
      " [0.88057   ]\n",
      " [0.88384108]\n",
      " [0.8699268 ]\n",
      " [0.86743687]\n",
      " [0.84749306]\n",
      " [0.84615046]\n",
      " [0.85132559]\n",
      " [0.84087767]\n",
      " [0.84417316]\n",
      " [0.84058474]\n",
      " [0.83687427]\n",
      " [0.84436845]\n",
      " [0.85994269]\n",
      " [0.86062619]\n",
      " [0.84549136]\n",
      " [0.84707808]\n",
      " [0.82911156]\n",
      " [0.81531933]\n",
      " [0.81397672]\n",
      " [0.82952655]\n",
      " [0.82354585]\n",
      " [0.84021858]\n",
      " [0.82895583]\n",
      " [0.83814532]\n",
      " [0.85081456]\n",
      " [0.83415095]\n",
      " [0.83674851]\n",
      " [0.84307088]]\n",
      "Starting with model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0065 test loss: 0.0052\n",
      "Epoch 2 train loss: 0.0779 test loss: 0.0057\n",
      "Epoch 3 train loss: 0.1087 test loss: 0.0125\n",
      "Epoch 4 train loss: 0.044 test loss: 0.0002\n",
      "Epoch 5 train loss: 0.018 test loss: 0.0006\n",
      "Epoch 6 train loss: 0.0055 test loss: 0.0003\n",
      "Epoch 7 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 8 train loss: 0.0059 test loss: 0.0001\n",
      "Epoch 9 train loss: 0.0022 test loss: 0.0002\n",
      "Epoch 10 train loss: 0.0023 test loss: 0.0002\n",
      "Epoch 11 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 12 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 13 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 14 train loss: 0.0018 test loss: 0.0001\n",
      "Epoch 15 train loss: 0.0027 test loss: 0.0003\n",
      "Epoch 16 train loss: 0.0023 test loss: 0.0004\n",
      "Epoch 17 train loss: 0.0043 test loss: 0.0002\n",
      "Epoch 18 train loss: 0.0028 test loss: 0.0001\n",
      "Epoch 19 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 20 train loss: 0.0031 test loss: 0.0004\n",
      "Epoch 21 train loss: 0.0043 test loss: 0.0002\n",
      "Epoch 22 train loss: 0.0037 test loss: 0.0001\n",
      "Epoch 23 train loss: 0.0025 test loss: 0.0002\n",
      "Epoch 24 train loss: 0.0028 test loss: 0.0004\n",
      "Epoch 25 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 26 train loss: 0.004 test loss: 0.0002\n",
      "Epoch 27 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 28 train loss: 0.0031 test loss: 0.0002\n",
      "Epoch 29 train loss: 0.0033 test loss: 0.0002\n",
      "Epoch 30 train loss: 0.0035 test loss: 0.0002\n",
      "Epoch 31 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 32 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 33 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 34 train loss: 0.003 test loss: 0.0003\n",
      "Epoch 35 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 36 train loss: 0.0028 test loss: 0.0002\n",
      "Epoch 37 train loss: 0.0024 test loss: 0.0003\n",
      "Epoch 38 train loss: 0.0025 test loss: 0.0003\n",
      "Epoch 39 train loss: 0.0027 test loss: 0.0002\n",
      "Epoch 40 train loss: 0.0029 test loss: 0.0002\n",
      "Epoch 41 train loss: 0.0023 test loss: 0.0003\n",
      "Epoch 42 train loss: 0.0023 test loss: 0.0001\n",
      "Epoch 43 train loss: 0.0019 test loss: 0.0005\n",
      "Epoch 44 train loss: 0.002 test loss: 0.0001\n",
      "Epoch 45 train loss: 0.0024 test loss: 0.0004\n",
      "Epoch 46 train loss: 0.0025 test loss: 0.0001\n",
      "Epoch 47 train loss: 0.0017 test loss: 0.0003\n",
      "Epoch 48 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 49 train loss: 0.0017 test loss: 0.0001\n",
      "Epoch 50 train loss: 0.0016 test loss: 0.0002\n",
      "Epoch 51 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 52 train loss: 0.0016 test loss: 0.0001\n",
      "Epoch 53 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 54 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 55 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 56 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 57 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 58 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 59 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 60 train loss: 0.0015 test loss: 0.0001\n",
      "Epoch 61 train loss: 0.0014 test loss: 0.0001\n",
      "Epoch 62 train loss: 0.0014 test loss: 0.0002\n",
      "Epoch 63 train loss: 0.0015 test loss: 0.0002\n",
      "Epoch 64 train loss: 0.0015 test loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# Multivariate Monte Carlo, no early stopping\n",
    "\n",
    "multi_absolute1 = []\n",
    "multi_root1 = []\n",
    "for i in range(n):\n",
    "    \n",
    "    hist = train.train_model_no_early(train_df, test_df, label_name, sequence_length, batch_size, n_epochs)\n",
    "    predictions_descaled, labels_descaled = inference.predict(df=test_df,\n",
    "                                                          label_name='Close',\n",
    "                                                          sequence_length=sequence_length)\n",
    "    mae, rmse = inference.return_loss_metrics(labels_descaled, predictions_descaled)\n",
    "    multi_absolute1.append(mae)\n",
    "    multi_root1.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error, no early stop\n",
    "\n",
    "mean = sum(multi_absolute1) / len(multi_absolute1)\n",
    "variance = sum([((x - mean) ** 2) for x in multi_absolute1]) / len(multi_absolute1)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=multi_absolute1, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Multivariate Mean Absolute Error, No Early Stop')\n",
    "maxfreq = m.max()\n",
    "plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}$'.format(mean,sd))\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a546a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean squared error, no early stop\n",
    "\n",
    "mean = sum(multi_root1) / len(multi_root1)\n",
    "variance = sum([((x - mean) ** 2) for x in multi_root1]) / len(multi_root1)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=multi_root1, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Univariate Root Mean Squared Error, No Early Stop')\n",
    "# plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}'.format(mean,sd))\n",
    "maxfreq = m.max()\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00449e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Monte Carlo, early stopping\n",
    "\n",
    "multi_absolute2 = []\n",
    "multi_root2 = []\n",
    "for i in range(n):\n",
    "    \n",
    "    hist = train.train_model(train_df, test_df, label_name, sequence_length, batch_size, n_epochs, n_epochs_stop)\n",
    "    predictions_descaled, labels_descaled = inference.predict(df=test_df,\n",
    "                                                          label_name='Close',\n",
    "                                                          sequence_length=sequence_length)\n",
    "    mae, rmse = inference.return_loss_metrics(labels_descaled, predictions_descaled)\n",
    "    multi_absolute2.append(mae)\n",
    "    multi_root2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22beb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error, early stop\n",
    "\n",
    "mean = sum(multi_absolute2) / len(multi_absolute2)\n",
    "variance = sum([((x - mean) ** 2) for x in multi_absolute2]) / len(multi_absolute2)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=multi_absolute2, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Multivariate Mean Absolute Error, Early Stop')\n",
    "maxfreq = m.max()\n",
    "plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}$'.format(mean,sd))\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean squared error, early stop\n",
    "\n",
    "mean = sum(multi_root2) / len(multi_root2)\n",
    "variance = sum([((x - mean) ** 2) for x in multi_root2]) / len(multi_root2)\n",
    "sd = variance ** 0.5\n",
    "\n",
    "m, bins, patches = plt.hist(x=multi_root2, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Univariate Root Mean Squared Error, Early Stop')\n",
    "plt.text(1.75, 4.5, '\\u03BC={},\\n\\n\\u03C3={}'.format(mean,sd))\n",
    "maxfreq = m.max()\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
