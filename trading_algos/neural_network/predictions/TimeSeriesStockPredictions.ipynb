{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37e13e9",
   "metadata": {},
   "source": [
    "This notebook explores a machine learning algorithm to predict the stock prices of SPY, the S&P 500 ETF, and is intended to utilize functions that can be easily translated to a python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01bcfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.9.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: alpha_vantage in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alpha_vantage) (3.8.4)\n",
      "Requirement already satisfied: requests in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alpha_vantage) (2.28.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->alpha_vantage) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->alpha_vantage) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->alpha_vantage) (2022.12.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: pandas_market_calendars in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.1.4)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas_market_calendars) (1.5.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas_market_calendars) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from pandas_market_calendars) (2.8.2)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas_market_calendars) (4.2.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.24.2)\n",
      "Requirement already satisfied: pyluach in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
      "Requirement already satisfied: korean-lunar-calendar in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install alpha_vantage\n",
    "!pip install scikit-learn\n",
    "!pip install pandas_market_calendars\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26129c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas_market_calendars as mcal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb7b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file (placing here for now, some fields will change on later impplementations)\n",
    "\n",
    "config = {\n",
    "    \"alpha_vantage\": {\n",
    "        \"key\": \"2JMCN347HZ3BU9RC\", \n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"key_adjusted_close\": \"5. adjusted close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 30,\n",
    "        \"train_split_size\": 1,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"xticks_interval\": 90, # show a date every 90 days\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since for now we are only using close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\",\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epoch\": 100,\n",
    "        \"epoch_stop\": 10,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c35333f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "# tickers = np.array(sp500[0]['Symbol'])\n",
    "tickers = ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ATVI', 'ADM', 'ADBE', 'ADP',\n",
    "       'AAP', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE',\n",
    "       'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN',\n",
    "       'AMCR', 'AMD', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK',\n",
    "       'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA',\n",
    "       'AAPL', 'AMAT', 'APTV', 'ACGL', 'ANET', 'AJG', 'AIZ', 'T', 'ATO',\n",
    "       'ADSK', 'AZO', 'AVB', 'AVY', 'BKR', 'BALL', 'BAC', 'BBWI', 'BAX',\n",
    "       'BDX', 'WRB', 'BRK.B', 'BBY', 'BIO', 'TECH', 'BIIB', 'BLK', 'BK',\n",
    "       'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO',\n",
    "       'BF-B', 'BG', 'CHRW', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH',\n",
    "       'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE',\n",
    "       'CNC', 'CNP', 'CDAY', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG',\n",
    "       'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX',\n",
    "       'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP',\n",
    "       'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CTVA', 'CSGP', 'COST',\n",
    "       'CTRA', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA',\n",
    "       'DE', 'DAL', 'XRAY', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DISH',\n",
    "       'DIS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DTE', 'DUK', 'DD',\n",
    "       'DXC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV',\n",
    "       'LLY', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EQT', 'EFX', 'EQIX',\n",
    "       'EQR', 'ESS', 'EL', 'ETSY', 'RE', 'EVRG', 'ES', 'EXC', 'EXPE',\n",
    "       'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FICO', 'FAST', 'FRT', 'FDX',\n",
    "       'FITB', 'FRC', 'FSLR', 'FE', 'FIS', 'FISV', 'FLT', 'FMC', 'F',\n",
    "       'FTNT', 'FTV', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GEHC',\n",
    "       'GEN', 'GNRC', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GL', 'GPN',\n",
    "       'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'PEAK', 'HSIC', 'HSY', 'HES',\n",
    "       'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ',\n",
    "       'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'ILMN', 'INCY',\n",
    "       'IR', 'PODD', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG',\n",
    "       'IVZ', 'INVH', 'IQV', 'IRM', 'JBHT', 'JKHY', 'J', 'JNJ', 'JCI',\n",
    "       'JPM', 'JNPR', 'K', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI',\n",
    "       'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS',\n",
    "       'LEN', 'LNC', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB',\n",
    "       'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH',\n",
    "       'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM',\n",
    "       'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ',\n",
    "       'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP',\n",
    "       'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'NDSN',\n",
    "       'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI',\n",
    "       'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OGN', 'OTIS',\n",
    "       'PCAR', 'PKG', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PNR', 'PEP',\n",
    "       'PKI', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'POOL',\n",
    "       'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC',\n",
    "       'PSA', 'PHM', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX',\n",
    "       'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP',\n",
    "       'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE',\n",
    "       'NOW', 'SHW', 'SPG', 'SWKS', 'SJM', 'SNA', 'SEDG', 'SO', 'LUV',\n",
    "       'SWK', 'SBUX', 'STT', 'STLD', 'STE', 'SYK', 'SYF', 'SNPS', 'SYY',\n",
    "       'TMUS', 'TROW', 'TTWO', 'TPR', 'TRGP', 'TGT', 'TEL', 'TDY', 'TFX',\n",
    "       'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG',\n",
    "       'TRV', 'TRMB', 'TFC', 'TYL', 'TSN', 'USB', 'UDR', 'ULTA', 'UNP',\n",
    "       'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VRSN', 'VRSK',\n",
    "       'VZ', 'VRTX', 'VFC', 'VTRS', 'VICI', 'V', 'VMC', 'WAB', 'WBA',\n",
    "       'WMT', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC',\n",
    "       'WRK', 'WY', 'WHR', 'WMB', 'WTW', 'GWW', 'WYNN', 'XEL', 'XYL',\n",
    "       'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec895396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to account for API limitations (on the weekend)\n",
    "\n",
    "# index = tickers.index('REG')\n",
    "# tickers = tickers[(index+1):]\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "694a8aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-28'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "next_weeks = mcal.date_range(mcal.get_calendar('NYSE').schedule(start_date=today, end_date=(today+relativedelta(months=1))), frequency='1D')\n",
    "next_weeks = [date.strftime('%Y-%m-%d') for date in next_weeks]\n",
    "next_weeks = next_weeks[1:]\n",
    "next_day = next_weeks[0]\n",
    "next_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69976f90",
   "metadata": {},
   "source": [
    "# Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12afaca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from the configuration file\n",
    "def get_data(config, ticker):\n",
    "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"]) \n",
    "    data, meta_data = ts.get_daily_adjusted(ticker, outputsize=config[\"alpha_vantage\"][\"outputsize\"])\n",
    "\n",
    "    date_data = [date for date in data.keys()]\n",
    "    date_data.reverse()\n",
    "\n",
    "    close_price_data = [float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]]) for date in data.keys()]\n",
    "    close_price_data.reverse()\n",
    "    close_price_data = np.array(close_price_data)\n",
    "\n",
    "    num_data_points = len(date_data)\n",
    "    display_date_range = \"from \" + date_data[0] + \" to \" + date_data[num_data_points-1]\n",
    "    print(\"Number data points\", num_data_points, display_date_range)\n",
    "\n",
    "    return date_data, close_price_data, num_data_points, display_date_range\n",
    "\n",
    "class Normalization():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "    \n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    # use the next day as label\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "# Class to prepare data for training and LSTM model\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # right now we have only 1 feature, so we need to convert `x` into [batch, sequence, features]\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "# neural network model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        \n",
    "        # layer 2\n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "    \n",
    "# function for training LSTM model\n",
    "def run_epoch(dataloader, is_training=False):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        y = y.to(config[\"training\"][\"device\"])\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += (loss.detach().item() / batchsize)\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    return epoch_loss, lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab50488a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "REGN training\n",
      "Epoch[1/100] | loss train:0.073186| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014840| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016740| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013506| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013067| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011673| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011244| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010540| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011676| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010077| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011022| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011473| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009717| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009644| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010538| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010059| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010067| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009821| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010772| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010908| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009340| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009940| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010334| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010657| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008967| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009403| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009059| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009544| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009995| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009547| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010628| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008914| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009584| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009144| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009062| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010643| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009247| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007894| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007820| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008022| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008116| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007894| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007875| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007498| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007586| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007951| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007358| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007770| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007710| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007669| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007652| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007759| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007695| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007625| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007515| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007734| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007695| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "RF training\n",
      "Epoch[1/100] | loss train:0.060682| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016709| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014843| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014711| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012688| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013029| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012580| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012571| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013229| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011929| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011831| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012146| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011599| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011474| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012668| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011962| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011508| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011848| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011896| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010852| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011544| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011224| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011981| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011085| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011409| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011064| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010941| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011561| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011058| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010741| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011047| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011300| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010958| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011033| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010898| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011394| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009921| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011157| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010630| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011521| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009869| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009668| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009302| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009211| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009332| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009649| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009024| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009537| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008959| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009048| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009266| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009127| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009001| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009483| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009051| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009168| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009341| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009236| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009528| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "RSG training\n",
      "Epoch[1/100] | loss train:0.082752| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011948| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011442| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011325| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011044| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010326| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011264| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010649| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010356| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010967| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010371| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009306| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009555| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009381| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009751| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010977| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010306| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008986| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010131| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009255| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009062| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009246| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008089| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009082| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008206| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009197| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009280| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008772| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008716| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008438| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008539| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008846| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "RMD training\n",
      "Epoch[1/100] | loss train:0.062191| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015614| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011142| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010989| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011556| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011962| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011004| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009718| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011391| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011030| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/100] | loss train:0.010068| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011614| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011024| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010224| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010324| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010720| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008943| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010069| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009047| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009554| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010395| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009567| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008927| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009454| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008863| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009419| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009698| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009714| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009274| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008385| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008729| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009591| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008818| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009466| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008682| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008650| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008886| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009214| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006958| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007471| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006932| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007551| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007264| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007149| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006555| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007031| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007679| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007238| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006828| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007353| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006840| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007324| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007065| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007063| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006948| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5887 from 1999-11-01 to 2023-03-27\n",
      "RHI training\n",
      "Epoch[1/100] | loss train:0.068567| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017573| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016775| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013926| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013743| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014217| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011867| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012519| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012736| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013595| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011411| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012571| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011717| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011612| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011565| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013580| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010959| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012435| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010992| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012316| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011546| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011398| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010990| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011278| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012838| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011918| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012340| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "ROK training\n",
      "Epoch[1/100] | loss train:0.068980| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014310| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014304| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012597| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010404| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011630| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010972| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011150| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009739| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010749| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009805| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010005| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010384| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009437| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010981| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010363| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010018| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010063| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009922| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010227| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009636| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008784| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011087| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009345| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009765| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009520| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008810| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009096| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010044| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009696| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "ROL training\n",
      "Epoch[1/100] | loss train:0.047315| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012028| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011215| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010600| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010623| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009946| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011089| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009224| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010230| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008897| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009410| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010289| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010195| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009494| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010222| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009214| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009246| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "ROP training\n",
      "Epoch[1/100] | loss train:0.062541| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014077| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013401| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012799| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012853| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010879| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012106| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010528| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009690| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008852| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010420| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009249| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010137| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009317| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009625| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008769| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009218| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010526| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009830| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009377| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009137| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009296| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008895| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008948| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008683| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008269| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008185| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008590| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009662| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008875| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007985| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009879| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008332| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[36/100] | loss train:0.009492| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008906| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008459| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008391| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007679| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006935| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007075| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007075| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007136| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007440| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006904| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006544| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007280| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006779| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007230| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006919| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006901| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006874| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006674| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007118| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007131| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007094| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "ROST training\n",
      "Epoch[1/100] | loss train:0.068611| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012088| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011326| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009678| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010408| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009687| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011026| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009037| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010164| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009948| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009204| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009420| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009205| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008584| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008629| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008957| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008833| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009083| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009817| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009079| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008869| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008928| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009702| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "RCL training\n",
      "Epoch[1/100] | loss train:0.071883| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017321| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016236| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013722| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015063| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013420| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011924| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011636| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012403| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011985| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012092| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012221| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012324| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012577| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011622| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011898| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012239| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010751| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011671| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011402| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012559| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011136| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011824| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011889| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010907| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012180| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010713| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011359| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011879| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011064| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011355| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011124| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011169| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011798| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011871| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010977| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010942| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5587 from 2001-01-02 to 2023-03-27\n",
      "SPGI training\n",
      "Epoch[1/100] | loss train:0.060169| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015747| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013902| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013011| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011134| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009998| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009910| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010421| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011391| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009810| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009815| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009150| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009305| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009599| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009523| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008839| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009507| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010207| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009107| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008827| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010061| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008772| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009075| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009278| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008653| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009428| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008468| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008576| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008730| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009038| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008688| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009076| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009806| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010304| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009700| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009358| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008839| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4723 from 2004-06-23 to 2023-03-27\n",
      "CRM training\n",
      "Epoch[1/100] | loss train:0.065241| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013171| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012035| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010560| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009933| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009982| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008215| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010063| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009345| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009903| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008468| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010356| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008391| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009151| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009167| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008478| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SBAC training\n",
      "Epoch[1/100] | loss train:0.059082| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016373| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013226| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011233| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010602| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011101| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009753| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011405| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011407| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009938| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010857| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012617| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009400| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010480| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008802| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010344| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[18/100] | loss train:0.009156| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010342| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010464| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008320| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008737| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009043| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008781| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010070| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008800| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008554| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008585| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009091| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SLB training\n",
      "Epoch[1/100] | loss train:0.070566| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015466| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013260| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013277| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013501| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013012| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013268| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012085| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012768| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012690| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012456| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011867| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011735| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012319| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012618| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012095| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012069| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012374| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012728| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012333| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011388| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012330| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010883| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011354| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012275| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011625| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011214| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011213| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012125| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011559| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011164| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011057| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011842| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5107 from 2002-12-11 to 2023-03-27\n",
      "STX training\n",
      "Epoch[1/100] | loss train:0.060420| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018142| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013839| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013205| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013196| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012905| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013901| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013029| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012141| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009355| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011651| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009563| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013508| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011861| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010736| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011411| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011185| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011042| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010284| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010561| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SEE training\n",
      "Epoch[1/100] | loss train:0.068783| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015550| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014902| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012493| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013372| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012508| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011087| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012476| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013338| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011858| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012387| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012578| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011679| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011108| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011350| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011535| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010860| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010881| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011222| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011845| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010856| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011342| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011375| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010656| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011913| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010837| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012167| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010316| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010562| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010222| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012616| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010454| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011096| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010213| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010517| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009861| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010945| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010857| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010629| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008818| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008692| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008468| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008688| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008189| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008606| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008665| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008313| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008496| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008596| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008205| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008220| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008055| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008093| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008619| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008188| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008496| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008366| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008503| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008509| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008513| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008639| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008264| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SRE training\n",
      "Epoch[1/100] | loss train:0.077950| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012762| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015293| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012373| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009928| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010676| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010381| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009540| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009539| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010339| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009288| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009591| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009336| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009531| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008965| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009482| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009167| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009066| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009622| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009846| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008972| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009449| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010040| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008608| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008845| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008946| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008251| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009220| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008952| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008507| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[33/100] | loss train:0.008652| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008460| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008250| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008891| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008631| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009127| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008133| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009354| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007010| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007057| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007123| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007089| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006798| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006893| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007300| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006657| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006841| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007172| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007530| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006910| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006596| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006733| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006729| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006726| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006861| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007009| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006767| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006767| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006958| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006510| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006698| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006927| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007013| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006783| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007210| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006812| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006944| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007024| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007031| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006716| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2702 from 2012-06-29 to 2023-03-27\n",
      "NOW training\n",
      "Epoch[1/100] | loss train:0.063089| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009742| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007579| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006812| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005992| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006370| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006609| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005645| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005823| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006115| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005711| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005883| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006703| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005910| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005851| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005234| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005809| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005514| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005230| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005096| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005452| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005095| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006778| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005241| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005459| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004945| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005340| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005373| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005761| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005113| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005215| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005292| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004899| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005298| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004670| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.004567| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005046| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005817| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004307| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005221| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.004461| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.004500| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004067| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004080| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004188| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003793| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004126| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004202| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004388| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004252| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004270| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004083| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004269| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004031| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004230| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.003929| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SHW training\n",
      "Epoch[1/100] | loss train:0.071505| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015849| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012620| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012703| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012594| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010786| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012749| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011912| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011173| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010422| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009399| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010418| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011939| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009600| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009801| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009069| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009031| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009097| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011889| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011319| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009684| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010757| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009637| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009010| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009773| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010604| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009182| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009283| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009750| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008539| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009075| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008187| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008245| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009022| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008356| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009107| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007536| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007907| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007504| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007368| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007254| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007118| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006781| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007042| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007102| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006722| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007043| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007068| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006953| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007158| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007085| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007202| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006749| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007173| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006867| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006760| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SPG training\n",
      "Epoch[1/100] | loss train:0.060514| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014003| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012226| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010372| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009690| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009566| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010180| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009743| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009422| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009270| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009166| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008551| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009320| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009793| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009246| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008768| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008983| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009279| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009428| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009349| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008453| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008860| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008528| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008694| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008791| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008652| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008510| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008794| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008353| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008717| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009004| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008782| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008545| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008734| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008823| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008255| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008961| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008735| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008486| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007635| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007381| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007273| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007247| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007390| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007144| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007136| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007200| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007200| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007422| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006959| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007351| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007310| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006900| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007065| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007471| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007290| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007289| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007333| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007327| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007021| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007365| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007257| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007346| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SWKS training\n",
      "Epoch[1/100] | loss train:0.057414| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016212| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013452| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014108| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012707| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013146| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013013| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012274| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013102| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011829| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011381| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010055| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012346| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011310| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012369| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012132| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012104| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011531| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010936| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011951| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011073| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011774| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SJM training\n",
      "Epoch[1/100] | loss train:0.065330| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012535| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010958| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011747| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008992| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010748| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009376| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009198| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010481| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009937| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008921| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008704| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009567| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008741| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009256| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009894| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009577| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009742| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008877| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008574| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008776| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009334| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008870| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009221| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009731| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008985| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009221| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008328| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007982| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008392| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008399| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009118| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008918| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008251| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008635| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009388| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007282| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007302| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007022| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007369| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006910| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007093| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006976| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007212| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006939| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007046| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006823| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007154| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007380| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007042| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006875| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006899| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007030| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006894| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006997| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006969| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006859| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SNA training\n",
      "Epoch[1/100] | loss train:0.057792| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013832| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013506| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010833| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012428| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010896| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009391| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009245| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010079| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011747| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009617| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010163| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009875| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009889| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009276| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010173| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009121| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008899| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009071| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009513| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010418| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009212| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008794| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[24/100] | loss train:0.009337| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009674| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008645| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009414| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008727| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009526| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008432| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008480| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008963| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008750| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008461| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008706| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009066| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009477| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008451| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2015 from 2015-03-26 to 2023-03-27\n",
      "SEDG training\n",
      "Epoch[1/100] | loss train:0.081202| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010716| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016370| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007355| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005713| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.045687| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.023271| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012866| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013070| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006796| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005612| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.124647| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012640| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005479| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012677| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005822| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.030392| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.018291| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.023999| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010015| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006039| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005334| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005447| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005355| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.018055| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006952| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005328| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.025002| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010052| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.026766| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011575| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007297| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005505| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005517| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.021404| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011726| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.079352| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SO training\n",
      "Epoch[1/100] | loss train:0.070073| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017786| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012887| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011898| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011957| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010714| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013921| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012016| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010191| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010508| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010147| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011341| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012039| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009488| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009729| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010120| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010986| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009684| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010610| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009664| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010078| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009863| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008876| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009403| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010407| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009512| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008270| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008914| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009066| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008371| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010285| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009454| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008957| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009870| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008506| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009285| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "LUV training\n",
      "Epoch[1/100] | loss train:0.050679| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013561| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012506| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012432| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010268| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011298| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011795| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011973| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011072| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011404| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010313| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010408| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010417| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010690| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010535| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SWK training\n",
      "Epoch[1/100] | loss train:0.074593| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015185| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014292| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014427| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011475| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012272| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012279| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013403| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011861| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011880| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010429| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011108| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009924| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011460| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010314| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011068| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010031| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011108| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009996| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009653| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010192| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010425| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009199| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010392| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009361| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009550| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010763| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010982| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009090| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009973| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010088| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009739| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011250| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009562| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009529| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009916| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009207| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009813| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SBUX training\n",
      "Epoch[1/100] | loss train:0.066991| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013060| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012065| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012292| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010661| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010629| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010542| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010468| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010323| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009310| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009546| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010042| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009703| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009232| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009634| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010508| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010445| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009576| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008810| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010240| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008536| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009428| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008279| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009023| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009412| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009614| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008708| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008836| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008862| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008927| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008698| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008621| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "STT training\n",
      "Epoch[1/100] | loss train:0.069587| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018405| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018953| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.018025| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015206| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015321| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015444| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015619| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015702| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014275| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015698| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015626| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015349| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014593| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015598| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.015771| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013952| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.015564| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014078| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.015423| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014175| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014206| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014182| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013687| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014546| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014858| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013991| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013872| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014514| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.014037| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.014800| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.014300| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013889| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.014631| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "STLD training\n",
      "Epoch[1/100] | loss train:0.098112| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021812| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015600| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.018342| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.017701| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015987| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015688| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.017873| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015539| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016755| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.017750| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014855| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015024| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.017416| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013117| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014643| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012435| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014764| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012284| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013533| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013042| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014137| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014198| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013816| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014224| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012878| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013341| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012833| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014038| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "STE training\n",
      "Epoch[1/100] | loss train:0.068735| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018592| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014854| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013458| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011827| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010972| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010866| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012385| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011776| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010913| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011020| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010971| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009965| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010600| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009919| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011311| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010081| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010315| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010649| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010542| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009987| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010198| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010343| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008958| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010774| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010379| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010465| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009683| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009473| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009727| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009116| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009115| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009821| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009763| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SYK training\n",
      "Epoch[1/100] | loss train:0.053255| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013743| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012349| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011033| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009780| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009177| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009425| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009547| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009848| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008953| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009162| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010215| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009837| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009600| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008777| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009781| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009114| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010027| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008564| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009285| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009391| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008514| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008642| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009720| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007946| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008389| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008636| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008909| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008332| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008668| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008737| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008727| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008488| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008317| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2179 from 2014-07-31 to 2023-03-27\n",
      "SYF training\n",
      "Epoch[1/100] | loss train:0.076281| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015227| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011952| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010748| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/100] | loss train:0.010790| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010273| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009671| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009438| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010180| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011021| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009299| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009516| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010759| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009245| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008376| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010297| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010556| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008679| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009115| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009396| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010147| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008482| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010207| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009165| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008583| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SNPS training\n",
      "Epoch[1/100] | loss train:0.059759| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014913| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013040| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012634| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013912| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012774| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012930| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011590| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011966| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011930| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011079| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010153| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012885| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011025| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010831| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010242| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011584| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010268| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009990| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009713| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011038| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011743| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010429| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010596| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012543| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010269| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009064| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010756| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009467| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009887| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010267| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010939| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010483| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010219| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010119| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009827| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009606| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "SYY training\n",
      "Epoch[1/100] | loss train:0.056668| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015789| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012718| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010411| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011491| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010758| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011258| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010331| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010307| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010603| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010186| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010401| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009968| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010624| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010545| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009838| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009483| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009711| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009743| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009833| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009982| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009831| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009720| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010266| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010741| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4013 from 2007-04-19 to 2023-03-27\n",
      "TMUS training\n",
      "Epoch[1/100] | loss train:0.057421| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011851| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010600| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010590| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007780| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008895| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009498| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008258| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007836| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007885| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009074| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008169| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007805| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006455| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007006| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007085| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006683| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007649| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007518| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007398| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008250| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007919| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007262| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007260| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TROW training\n",
      "Epoch[1/100] | loss train:0.053520| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015780| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014179| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013403| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011617| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010946| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012206| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011378| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011098| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012039| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010164| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010658| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011249| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011317| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011413| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011055| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011100| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010958| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010171| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011157| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010922| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TTWO training\n",
      "Epoch[1/100] | loss train:0.090460| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016792| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013262| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012421| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016086| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011293| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010968| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011677| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011613| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011640| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010108| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009718| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010873| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010903| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010751| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010588| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009840| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010379| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009502| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009672| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009466| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009466| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009533| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009933| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009753| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008916| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[28/100] | loss train:0.009930| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008907| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009566| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011200| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010029| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009993| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008953| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009537| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008861| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009052| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008858| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010091| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008956| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007793| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007588| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007843| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007828| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007953| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007608| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007533| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007922| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007907| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008065| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007970| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007389| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007488| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007543| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007348| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007538| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007783| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007148| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007576| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007965| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007384| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007677| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007221| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007238| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007600| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008046| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007019| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007575| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008094| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007602| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007937| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007447| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007771| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.007750| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007014| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007487| lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007569| lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007328| lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007400| lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007497| lr:0.001000\n",
      "Epoch[81/100] | loss train:0.007157| lr:0.000100\n",
      "Epoch[82/100] | loss train:0.007649| lr:0.000100\n",
      "Epoch[83/100] | loss train:0.007404| lr:0.000100\n",
      "Epoch[84/100] | loss train:0.007493| lr:0.000100\n",
      "Epoch[85/100] | loss train:0.007066| lr:0.000100\n",
      "Early stopping.\n",
      "Number data points 5652 from 2000-10-06 to 2023-03-27\n",
      "TPR training\n",
      "Epoch[1/100] | loss train:0.057493| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015321| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015036| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013979| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015303| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014000| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012965| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012137| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011971| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013822| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011925| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012148| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011825| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011710| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012417| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012638| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011644| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011909| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011628| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011151| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011688| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012077| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011843| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011249| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011321| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011694| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012328| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011650| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012250| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011869| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3096 from 2010-12-07 to 2023-03-27\n",
      "TRGP training\n",
      "Epoch[1/100] | loss train:0.067752| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013545| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010820| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009703| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009392| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009116| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007923| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008129| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009056| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007855| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007853| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008135| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007362| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009107| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008089| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007568| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007461| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007345| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007706| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007662| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007620| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006608| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007452| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007130| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008085| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006931| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007282| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007737| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007333| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007536| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008012| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006727| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TGT training\n",
      "Epoch[1/100] | loss train:0.059685| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016878| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015088| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014530| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014721| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014097| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013049| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013210| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011566| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014630| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012060| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013035| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010977| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013460| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011784| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011913| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011894| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012302| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012040| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011800| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011305| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011682| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010561| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010555| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010349| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010627| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012223| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011554| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011495| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009826| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012225| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010281| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011605| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010154| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011006| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009579| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012300| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010472| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008822| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008343| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008066| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[44/100] | loss train:0.007616| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008458| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008626| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008118| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008110| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007897| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008073| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007627| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008075| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008275| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007840| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3974 from 2007-06-14 to 2023-03-27\n",
      "TEL training\n",
      "Epoch[1/100] | loss train:0.054823| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011189| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009663| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009160| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007850| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007807| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006712| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009627| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008111| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006378| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008498| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007205| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007763| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007129| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006698| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007269| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006492| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007361| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006898| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006600| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5872 from 1999-11-23 to 2023-03-27\n",
      "TDY training\n",
      "Epoch[1/100] | loss train:0.068329| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014976| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015319| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013187| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011330| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011420| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011863| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010387| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010812| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011882| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010283| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010184| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009998| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009565| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008100| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008459| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010130| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009009| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009834| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010059| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009388| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009006| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008486| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TFX training\n",
      "Epoch[1/100] | loss train:0.074551| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015037| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013855| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011331| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011389| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010615| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012532| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010711| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011249| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011264| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011199| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010610| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011030| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009947| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009628| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010015| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010969| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009907| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010635| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010951| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010419| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010577| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010465| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009973| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009571| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009701| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009352| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008891| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010613| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009873| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008983| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010970| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009388| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008940| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010872| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009086| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009379| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TER training\n",
      "Epoch[1/100] | loss train:0.071505| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018156| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016787| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017490| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014280| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015228| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014750| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011654| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014564| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013450| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015179| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013566| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013136| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015818| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014111| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013552| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012268| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013122| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3208 from 2010-06-29 to 2023-03-27\n",
      "TSLA training\n",
      "Epoch[1/100] | loss train:0.051440| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011301| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010202| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009140| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008825| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007192| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009130| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007647| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009781| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007472| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007379| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011295| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008011| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006585| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008510| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007557| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007251| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007106| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006793| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007995| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007470| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006916| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007139| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007841| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TXN training\n",
      "Epoch[1/100] | loss train:0.073897| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015833| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014204| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011400| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010869| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011868| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010987| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011558| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010812| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010435| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010812| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008461| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010550| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009407| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009459| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010140| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010025| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009461| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010683| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009684| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010138| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TXT training\n",
      "Epoch[1/100] | loss train:0.062604| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015016| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013658| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013579| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013321| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013065| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011530| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012707| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013729| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011680| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013460| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011783| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012471| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012344| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011740| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011213| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013092| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011100| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012427| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011241| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011865| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011515| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011864| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011394| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010948| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010951| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011451| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011928| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010932| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011273| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010710| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011257| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011382| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011796| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010971| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011637| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011136| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010661| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011614| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009590| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009238| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009735| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008891| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009299| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009203| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009187| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008998| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009056| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008672| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009505| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009288| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009094| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008959| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008853| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009142| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008913| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008888| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008635| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009213| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008953| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009474| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.009069| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.009434| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.009533| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.009168| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.009555| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.009485| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008876| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TMO training\n",
      "Epoch[1/100] | loss train:0.059553| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014336| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012680| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011892| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011192| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010349| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011820| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010209| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009826| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009730| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009856| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009278| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010430| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011023| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009732| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009369| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009903| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008786| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009087| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009221| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010371| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009079| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009181| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009097| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008936| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009307| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010126| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TJX training\n",
      "Epoch[1/100] | loss train:0.071774| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014995| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012248| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012325| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010599| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009649| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010875| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010923| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009676| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011440| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009604| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009749| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009366| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010194| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009815| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008928| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008588| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008709| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009389| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008896| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008542| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009431| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009760| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008768| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010191| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008947| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008660| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008838| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009039| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008904| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008274| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008496| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008116| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008487| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008291| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008432| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008788| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008242| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007173| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007201| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007076| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007147| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006983| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006640| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006750| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007284| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007154| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006564| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006842| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006973| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007219| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007241| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006395| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006911| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006768| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006541| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006990| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006980| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007171| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007195| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006863| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006684| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[66/100] | loss train:0.006895| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TSCO training\n",
      "Epoch[1/100] | loss train:0.085479| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015371| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012344| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013347| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012169| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014212| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010826| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010833| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010406| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011444| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011133| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011123| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011671| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010385| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009592| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010922| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010328| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010372| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009467| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011250| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009576| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010292| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010594| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009633| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009993| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008824| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010813| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009719| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010051| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009335| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009497| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009755| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008821| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010637| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008480| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009542| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009384| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009320| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009191| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009129| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007702| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007691| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007660| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007263| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007181| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007462| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007035| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007176| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007273| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006678| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007116| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007681| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007280| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007175| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007278| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006998| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007450| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007379| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006751| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007069| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TT training\n",
      "Epoch[1/100] | loss train:0.084604| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015573| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013488| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012269| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011480| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011432| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011893| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010955| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010620| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010323| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010967| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011405| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010509| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011129| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009294| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012326| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009923| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009336| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010807| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009126| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010645| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010296| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009316| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009086| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010201| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009030| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009298| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010829| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008690| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010105| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009863| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010896| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010633| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010045| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008684| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009070| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009451| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008923| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008323| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007242| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007084| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007563| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007114| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007345| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007233| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007292| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007398| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007414| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007301| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006698| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007316| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007384| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007665| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007104| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007307| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006817| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007411| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006806| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007131| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007277| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4288 from 2006-03-15 to 2023-03-27\n",
      "TDG training\n",
      "Epoch[1/100] | loss train:0.073957| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013005| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010566| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008800| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008059| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007662| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008959| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007957| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007768| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007846| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008048| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008051| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007368| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008269| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007287| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007799| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007362| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007377| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006850| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008134| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007278| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006566| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007248| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007126| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007636| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006570| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006714| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007087| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007541| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007148| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006920| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007306| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TRV training\n",
      "Epoch[1/100] | loss train:0.064766| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013027| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012312| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012265| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011272| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011083| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010742| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/100] | loss train:0.010545| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010093| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008854| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009954| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010557| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009261| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009653| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009503| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009490| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009667| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009683| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009045| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009862| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TRMB training\n",
      "Epoch[1/100] | loss train:0.064871| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017057| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016908| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015709| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012350| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013323| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012327| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011694| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010997| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011818| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011925| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011781| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011113| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011520| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010752| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009855| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012580| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010830| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011853| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012064| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010089| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011584| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011091| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011221| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011047| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TFC training\n",
      "Epoch[1/100] | loss train:0.073953| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018126| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014565| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013128| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014989| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014274| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014043| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013237| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013074| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013318| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012702| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013398| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013422| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012122| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014240| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012203| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012397| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012784| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013456| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012987| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012961| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011531| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012525| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012428| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012226| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013652| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012844| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011450| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012288| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012339| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012323| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013657| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013677| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011673| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011834| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.013444| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012765| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012449| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TYL training\n",
      "Epoch[1/100] | loss train:0.073839| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014438| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011653| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014297| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011703| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012306| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010867| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010690| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012119| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010308| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010960| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010788| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009407| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011223| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010678| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011153| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011503| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010100| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011920| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009184| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009481| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010126| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009785| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009582| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009412| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010045| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010209| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009480| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009649| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010967| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "TSN training\n",
      "Epoch[1/100] | loss train:0.070068| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017011| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013060| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010804| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012486| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011309| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009709| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010881| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010510| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011167| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009762| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011295| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009585| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009471| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009357| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009164| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009851| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010008| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008757| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009345| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010276| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008875| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009945| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008851| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008817| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008786| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009118| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009657| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009310| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "USB training\n",
      "Epoch[1/100] | loss train:0.054337| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014209| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013901| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011319| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011681| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012489| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011378| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010859| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010484| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011148| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010869| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010926| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011095| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010626| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011453| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009676| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010463| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010919| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010095| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011381| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009969| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[22/100] | loss train:0.010920| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010503| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009924| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010435| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010203| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "UDR training\n",
      "Epoch[1/100] | loss train:0.088115| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015012| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014781| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012609| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013877| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012603| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010966| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011559| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013463| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010101| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010128| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011027| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010181| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010742| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011848| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010425| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010645| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010404| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009955| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009610| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009134| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010270| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009770| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010052| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009023| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010522| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009566| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010298| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008776| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009091| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011036| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009523| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009406| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008405| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007893| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007257| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007473| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007737| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007511| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007387| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007344| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007255| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007253| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007463| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007225| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007679| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007411| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007633| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007485| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007520| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007651| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007342| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007473| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007541| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007195| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007528| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007438| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007332| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006947| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007688| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007356| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007539| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007386| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007227| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007682| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007252| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.007427| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007286| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007336| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3881 from 2007-10-25 to 2023-03-27\n",
      "ULTA training\n",
      "Epoch[1/100] | loss train:0.054963| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010845| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009839| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009395| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008743| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009900| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009253| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009771| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008047| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008262| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007465| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008503| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008508| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007761| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008297| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007593| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007279| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009764| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008501| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007936| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007461| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008131| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009457| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006940| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006760| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007429| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007287| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007533| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008019| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007439| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007379| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007154| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008091| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007488| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007508| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "UNP training\n",
      "Epoch[1/100] | loss train:0.078545| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014546| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011800| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011405| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010990| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013351| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010740| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010935| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010385| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010876| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009643| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008728| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010288| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009893| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008801| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009315| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011152| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009603| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009455| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008433| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009412| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009225| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009798| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009109| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008859| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008876| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008823| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4314 from 2006-02-06 to 2023-03-27\n",
      "UAL training\n",
      "Epoch[1/100] | loss train:0.064007| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014358| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012312| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010993| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010300| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010853| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013260| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009970| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011209| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011412| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010553| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009854| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010451| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008903| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[16/100] | loss train:0.009459| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011395| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009774| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010482| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009026| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009363| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009362| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009397| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009468| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009924| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5881 from 1999-11-10 to 2023-03-27\n",
      "UPS training\n",
      "Epoch[1/100] | loss train:0.066667| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018389| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013217| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016775| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013841| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012169| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011517| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011556| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013004| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011205| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012567| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010430| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011528| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011126| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011137| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010224| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009533| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010439| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012556| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009459| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011099| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009547| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009749| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009721| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010146| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009948| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010211| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009542| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012697| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "URI training\n",
      "Epoch[1/100] | loss train:0.068458| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016763| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016128| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017360| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015005| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013614| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013681| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013706| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011309| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011106| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012172| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013905| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013918| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011415| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010550| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013199| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012694| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010934| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009757| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011835| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011789| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010195| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011133| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011039| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011906| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010182| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010518| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011293| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009834| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "UNH training\n",
      "Epoch[1/100] | loss train:0.047992| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013909| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011110| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012320| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012100| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012675| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011420| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010063| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011268| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009536| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011391| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011012| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009813| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010519| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010305| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009083| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010151| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009016| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010488| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010239| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010375| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009685| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009837| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009470| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009588| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009501| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010847| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "UHS training\n",
      "Epoch[1/100] | loss train:0.073757| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014757| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013233| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011470| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010234| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009862| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010188| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009563| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010139| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009587| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009057| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010245| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009496| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009632| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009191| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009867| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009874| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009321| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009527| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009282| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009928| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "VLO training\n",
      "Epoch[1/100] | loss train:0.065319| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016986| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017356| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014723| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013369| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014303| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012545| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012736| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011661| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011901| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013041| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013345| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011372| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012041| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012330| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011404| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012233| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011803| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011103| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012262| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011724| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011509| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012171| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011394| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013578| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010599| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013393| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012836| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010768| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010161| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011473| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012340| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010086| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011763| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011656| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012239| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011232| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010535| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012497| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.012228| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/100] | loss train:0.009856| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009049| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008750| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008498| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008813| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009130| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008923| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008895| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009195| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008538| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009559| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008820| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008769| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008682| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "VTR training\n",
      "Epoch[1/100] | loss train:0.052192| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013506| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012925| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011148| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011538| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010306| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010703| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010701| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009934| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010182| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010015| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010089| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009662| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009677| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009375| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010008| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009416| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009409| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009107| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009978| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008729| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008915| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009057| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009857| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008950| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009524| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009630| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008747| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008958| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008888| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008913| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "VRSN training\n",
      "Epoch[1/100] | loss train:0.066432| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014507| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013056| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013539| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012410| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012059| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011875| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011120| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011371| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010874| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011439| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010583| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010611| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010640| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011408| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010585| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010088| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010604| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011500| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009925| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009823| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010679| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009630| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010153| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010082| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009161| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011064| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009869| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009643| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009400| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009454| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009899| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009709| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010715| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009672| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3390 from 2009-10-07 to 2023-03-27\n",
      "VRSK training\n",
      "Epoch[1/100] | loss train:0.055573| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010415| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007969| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007852| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006966| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006556| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006466| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005639| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005608| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007050| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005528| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006523| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006026| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006016| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006006| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005643| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005844| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006010| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007191| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005783| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007085| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "VZ training\n",
      "Epoch[1/100] | loss train:0.071156| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012565| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012494| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011012| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011337| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010588| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010721| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011897| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009813| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009969| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010082| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010263| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009508| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009338| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010561| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009014| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009289| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009424| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009404| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009140| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008747| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009022| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009256| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009223| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009236| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009096| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009866| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008663| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009102| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008901| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008674| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008656| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008643| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008750| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008376| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009086| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008485| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007697| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007834| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007748| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007576| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007547| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007723| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007632| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007519| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007550| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007231| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007191| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007514| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007357| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007376| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007093| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007209| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007402| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007675| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007376| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007198| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[61/100] | loss train:0.007575| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007260| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007574| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007514| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007225| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "VRTX training\n",
      "Epoch[1/100] | loss train:0.073569| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017626| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015510| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011560| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012827| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013522| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012187| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011598| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011710| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011070| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011531| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011434| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012595| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010841| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010248| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012630| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010519| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011024| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011048| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010667| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010477| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010863| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011111| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "VFC training\n",
      "Epoch[1/100] | loss train:0.071699| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014848| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011790| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011991| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010763| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011257| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011964| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011216| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010957| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009980| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010711| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009562| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009604| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010456| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009550| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009767| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009840| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009739| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009523| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009990| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009429| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009960| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009807| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009923| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009682| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009327| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010298| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009216| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009276| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009828| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008758| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010015| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009156| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009720| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009235| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009736| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010172| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009551| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008200| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007988| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007782| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008184| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007840| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007741| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007972| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007827| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007807| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007525| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007638| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007694| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008029| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007748| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007635| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007674| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007629| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007876| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007580| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007955| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 593 from 2020-11-16 to 2023-03-27\n",
      "VTRS training\n",
      "Epoch[1/100] | loss train:0.043771| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008530| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005783| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005501| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005019| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004118| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004285| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004545| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004527| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004212| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.003772| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.003894| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.003788| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.003319| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.003968| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.003657| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003886| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003849| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003583| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003420| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003454| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.003950| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004493| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.003284| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003980| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.003747| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.003830| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.003748| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003289| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003985| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.003057| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003371| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003402| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.003472| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003705| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.003849| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.003777| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.003799| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.003526| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003619| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003231| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 1369 from 2017-10-17 to 2023-03-27\n",
      "VICI training\n",
      "Epoch[1/100] | loss train:0.048907| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008329| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.005756| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005375| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005181| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004197| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004469| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004564| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004486| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004001| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004352| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.003995| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.003812| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.003549| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.003820| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.003844| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.003717| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004103| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003807| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003596| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003386| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003502| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.002983| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003984| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.003374| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003607| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.003347| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.003986| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004010| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003369| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[31/100] | loss train:0.003511| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.003701| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003327| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3782 from 2008-03-19 to 2023-03-27\n",
      "V training\n",
      "Epoch[1/100] | loss train:0.056001| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009185| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007842| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007302| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007363| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006678| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007266| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005923| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007850| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006345| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005988| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006888| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006240| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005759| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005987| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005601| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006207| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006033| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005678| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005595| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006474| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006284| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006196| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005555| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007177| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005581| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006089| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006166| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005767| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005928| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005941| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005977| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006026| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005727| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "VMC training\n",
      "Epoch[1/100] | loss train:0.059394| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014844| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011608| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013231| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012114| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010841| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012190| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012323| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010987| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010501| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011130| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010571| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011410| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010724| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011658| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011607| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010302| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011128| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009881| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010244| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009861| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010153| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009900| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009787| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010239| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009898| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010544| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010022| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010205| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009592| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010003| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010946| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010265| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009693| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010305| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010715| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009453| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009692| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009552| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008104| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008516| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008101| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007849| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007972| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007966| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008143| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008293| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008136| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007931| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007667| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008166| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008194| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008119| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008169| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007999| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008078| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007898| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007827| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007647| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007893| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007939| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008057| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008031| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007850| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007719| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007910| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007786| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007870| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007598| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008468| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.008146| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.008006| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.007664| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.008096| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007866| lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007850| lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007906| lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007856| lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007764| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WAB training\n",
      "Epoch[1/100] | loss train:0.056368| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012761| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011891| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010623| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010321| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009852| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010616| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009200| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009946| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009642| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009383| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009790| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009309| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009067| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009315| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009461| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010146| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009154| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009060| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009202| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008905| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009793| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009236| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009271| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008391| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009052| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008713| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008676| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008101| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008743| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008715| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008703| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008408| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008936| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007945| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008771| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008567| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008730| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008529| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008869| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007821| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007561| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007366| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007515| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007490| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007218| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/100] | loss train:0.007238| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007329| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007125| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007549| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007346| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007228| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007280| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007374| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007189| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007155| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007308| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007262| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007245| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WBA training\n",
      "Epoch[1/100] | loss train:0.059731| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014783| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012538| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012317| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011346| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010912| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011621| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011724| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011660| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010642| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011711| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010317| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011314| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010619| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011490| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010924| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011202| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011469| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011188| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011068| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010878| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011296| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WMT training\n",
      "Epoch[1/100] | loss train:0.061226| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016096| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013248| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012573| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011341| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010641| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011443| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011504| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010274| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010243| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010412| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010750| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011097| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010381| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010867| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009553| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010009| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009154| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009354| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009371| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009971| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009313| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010601| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009082| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009195| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009699| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008631| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008758| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009539| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009655| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008460| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009303| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009854| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008539| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008827| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008806| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008805| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009067| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008399| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007817| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006971| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007326| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007303| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006881| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007138| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006935| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007296| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007169| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007567| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007179| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007016| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007235| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007167| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007185| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4458 from 2005-07-08 to 2023-03-27\n",
      "WBD training\n",
      "Epoch[1/100] | loss train:0.077242| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018105| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016917| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015287| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014700| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013094| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012220| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015136| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012854| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015213| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011574| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012186| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012080| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014185| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012373| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011718| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010374| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010468| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010490| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011057| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010805| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011218| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009606| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010743| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010994| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009432| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012259| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010929| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012709| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010063| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011591| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009503| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010893| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009878| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011537| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WM training\n",
      "Epoch[1/100] | loss train:0.063813| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013701| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011193| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011078| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009229| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011475| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009379| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009407| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010752| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009211| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010838| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009274| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009000| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010113| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009143| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009126| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009436| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008328| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008319| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009490| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008926| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009028| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008869| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008469| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009294| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007922| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009217| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008817| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008582| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008518| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009546| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/100] | loss train:0.008705| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008626| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008708| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008095| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007082| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006877| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006689| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006699| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006506| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006621| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006791| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006634| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006400| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006675| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006858| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006940| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006536| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006784| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006547| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006414| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006583| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006332| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006463| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006770| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006727| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006620| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006316| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006249| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006595| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006501| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006175| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006384| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006397| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006563| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006950| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006730| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006505| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006634| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006612| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006091| lr:0.001000\n",
      "Epoch[77/100] | loss train:0.006385| lr:0.001000\n",
      "Epoch[78/100] | loss train:0.006724| lr:0.001000\n",
      "Epoch[79/100] | loss train:0.006101| lr:0.001000\n",
      "Epoch[80/100] | loss train:0.006286| lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006616| lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006333| lr:0.000100\n",
      "Epoch[83/100] | loss train:0.006553| lr:0.000100\n",
      "Epoch[84/100] | loss train:0.006136| lr:0.000100\n",
      "Epoch[85/100] | loss train:0.006730| lr:0.000100\n",
      "Epoch[86/100] | loss train:0.006611| lr:0.000100\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WAT training\n",
      "Epoch[1/100] | loss train:0.068562| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016930| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013366| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013492| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012368| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011349| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011020| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010932| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011441| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012980| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010856| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009951| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010967| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011857| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010838| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009503| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011570| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009503| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010251| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009202| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009473| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010263| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009413| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010337| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010083| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009390| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009556| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009680| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009982| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010544| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WEC training\n",
      "Epoch[1/100] | loss train:0.068077| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014375| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012353| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010744| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012169| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010572| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010988| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009611| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011559| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009241| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009938| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009238| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009080| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009195| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011384| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009504| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009310| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009457| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008810| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008865| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008873| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009080| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009004| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008539| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008904| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008262| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008168| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008114| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008086| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009058| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008531| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008884| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008445| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008411| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008291| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008955| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007610| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008374| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007261| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006915| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007128| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007533| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006653| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006758| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006976| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006628| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006967| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007053| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007051| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007070| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007109| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006865| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007076| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006837| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006801| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006926| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WFC training\n",
      "Epoch[1/100] | loss train:0.070042| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014679| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012113| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013302| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012770| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012687| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011733| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011902| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011560| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012270| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010954| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011994| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010826| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010669| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010356| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010735| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010211| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010612| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011006| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010981| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009914| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010204| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010814| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010311| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010532| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[26/100] | loss train:0.010341| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010387| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010549| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010734| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009632| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010221| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010346| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009968| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011118| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010072| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010977| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010040| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011027| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010256| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010540| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5581 from 2001-01-02 to 2023-03-27\n",
      "WELL training\n",
      "Epoch[1/100] | loss train:0.055735| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015461| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011613| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011984| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010803| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010421| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009576| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009215| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009184| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009927| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009881| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008999| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010191| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008492| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009746| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008964| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010105| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009563| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008504| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008619| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009033| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008652| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009932| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008313| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008744| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008310| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009032| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008985| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008789| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008678| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008639| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008761| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008484| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008704| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008447| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008720| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WST training\n",
      "Epoch[1/100] | loss train:0.081226| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015289| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014716| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012687| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011420| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013470| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011112| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009656| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011517| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009928| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010197| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009713| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009568| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009512| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009753| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012050| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009600| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011160| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009548| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011504| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009737| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009809| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008900| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009588| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011865| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009868| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010115| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009377| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009778| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009090| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010123| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009824| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009260| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WDC training\n",
      "Epoch[1/100] | loss train:0.072756| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016054| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013934| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012494| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013027| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013114| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011807| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011030| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011358| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011204| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011791| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010209| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010692| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012533| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011200| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011824| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011426| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011695| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010684| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011014| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010293| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011771| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1953 from 2015-06-24 to 2023-03-27\n",
      "WRK training\n",
      "Epoch[1/100] | loss train:0.060117| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.042601| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017340| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012923| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011491| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015492| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013432| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013509| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014784| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013110| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012340| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011281| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015308| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.018354| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014730| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.016826| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014088| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013600| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011767| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011060| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012566| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013226| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014747| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.018506| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012303| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014059| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.018232| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.014735| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014837| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009887| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.019883| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.015231| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.018251| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012415| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012700| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.023577| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010261| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.034110| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013768| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010629| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WY training\n",
      "Epoch[1/100] | loss train:0.071769| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018338| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014128| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013617| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015785| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012465| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013489| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011804| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012765| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011750| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011198| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011146| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/100] | loss train:0.012554| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011874| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012967| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011120| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012167| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011012| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011839| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011748| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012669| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011100| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011085| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011053| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011827| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011184| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013034| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011207| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WHR training\n",
      "Epoch[1/100] | loss train:0.071468| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016734| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013212| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013162| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014219| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011413| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010860| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012574| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012177| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011318| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012129| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010528| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011493| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011486| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010076| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010927| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011510| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010288| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010480| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010971| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010127| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010296| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010702| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011257| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010752| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010868| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010556| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010267| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011116| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011013| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010420| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010985| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010622| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "WMB training\n",
      "Epoch[1/100] | loss train:0.055655| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017796| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017515| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013796| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014048| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013596| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013145| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013429| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013055| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012584| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012751| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013560| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012501| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013790| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013160| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011842| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012329| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012723| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012811| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011519| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011812| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011850| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011201| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012145| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011291| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011136| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010916| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011406| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011559| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011362| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011879| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011224| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011154| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011343| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011109| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010505| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011472| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011641| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010195| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011749| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010006| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009275| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009322| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009407| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009442| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009033| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009345| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009391| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009276| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008534| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009324| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008979| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009155| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009145| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009143| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009463| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009404| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009371| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009506| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009198| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 1819 from 2016-01-05 to 2023-03-27\n",
      "WTW training\n",
      "Epoch[1/100] | loss train:0.053825| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.007704| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007546| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005859| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006145| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005291| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005475| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004500| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005111| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004616| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005329| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005411| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005751| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004406| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004834| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005116| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004888| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004715| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004622| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004740| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004404| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004738| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004897| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004725| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004185| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004670| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004692| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004600| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004345| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004238| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.004613| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004580| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004627| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004104| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004492| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.004638| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004523| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.004405| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004491| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004489| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003850| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003987| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.003885| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003873| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.003657| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003619| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.003712| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.003680| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.003785| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.003832| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.003651| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.003919| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.003693| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[54/100] | loss train:0.003588| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.003665| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.003523| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.003657| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.003614| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.003616| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.003827| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.003516| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.003513| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.003721| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.003926| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.003616| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.003659| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.003655| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.003654| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.003571| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.003605| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.003536| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.003755| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "GWW training\n",
      "Epoch[1/100] | loss train:0.085478| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018125| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014368| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012900| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011649| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013099| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014748| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011329| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010765| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011663| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010477| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012436| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010131| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011309| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010594| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010516| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009851| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010865| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011922| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011144| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009960| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011308| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009421| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010387| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008933| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010327| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009995| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011241| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010149| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009861| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010297| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009491| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009700| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009689| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009359| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5139 from 2002-10-25 to 2023-03-27\n",
      "WYNN training\n",
      "Epoch[1/100] | loss train:0.073997| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018858| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015632| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014430| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013931| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013259| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011798| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013105| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012221| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012717| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011540| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012110| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011023| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012628| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011799| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011805| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011442| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012102| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011589| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012016| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011501| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011181| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010655| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011290| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010925| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011186| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012284| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011988| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010450| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009895| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012064| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011223| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011117| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011083| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011565| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011513| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010938| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011285| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011697| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009367| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009308| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008986| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009563| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009630| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009649| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008637| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009108| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009186| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009101| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009281| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008848| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009073| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009415| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009345| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008929| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009309| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "XEL training\n",
      "Epoch[1/100] | loss train:0.054980| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013389| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010887| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010289| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010425| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010421| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011007| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012059| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009597| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010625| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008918| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010227| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009167| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011044| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008852| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009063| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008859| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008629| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009387| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009791| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009318| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008954| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008840| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008412| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009385| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008834| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008428| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008912| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008812| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008960| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008620| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008542| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008819| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2881 from 2011-10-13 to 2023-03-27\n",
      "XYL training\n",
      "Epoch[1/100] | loss train:0.041520| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009438| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007136| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006741| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006747| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005737| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005994| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005813| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005695| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005665| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005302| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005790| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005867| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005036| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004879| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[16/100] | loss train:0.005493| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006580| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004980| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005068| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005000| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005487| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005296| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004939| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004930| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004818| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005299| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005623| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005480| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006010| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005194| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005547| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005615| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005972| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004865| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005276| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "YUM training\n",
      "Epoch[1/100] | loss train:0.063498| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013648| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011872| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012622| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010000| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011283| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009400| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011223| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010227| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009280| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009681| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009533| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009442| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008301| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008640| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009316| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009203| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008560| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009320| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009415| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008948| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009135| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008676| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007904| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008611| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008923| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008944| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008072| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008159| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008704| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008682| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008416| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008736| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009089| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "ZBRA training\n",
      "Epoch[1/100] | loss train:0.090706| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.022501| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016573| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014617| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013824| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012473| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012377| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012952| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011972| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013318| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012278| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012240| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012250| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012143| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010877| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011012| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011515| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010353| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011705| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009875| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009890| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011630| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010831| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009964| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011254| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010025| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010889| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5452 from 2001-07-25 to 2023-03-27\n",
      "ZBH training\n",
      "Epoch[1/100] | loss train:0.077795| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014488| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013309| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012149| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012979| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011068| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013026| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011803| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010821| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011373| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010445| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010855| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010367| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010616| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012602| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010496| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011442| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009953| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010640| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010699| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009596| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010865| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010198| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009529| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010235| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011028| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010007| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010983| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010147| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009307| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010621| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010311| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010300| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009665| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009431| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009702| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5888 from 1999-11-01 to 2023-03-27\n",
      "ZION training\n",
      "Epoch[1/100] | loss train:0.059354| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018513| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015969| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016957| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014730| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013349| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015412| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013427| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013912| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014586| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013022| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013418| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012791| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014156| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013861| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012786| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013448| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013340| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013609| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012329| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013937| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013359| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014101| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012462| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013686| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014111| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013487| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012133| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012817| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013165| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013028| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013307| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012839| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012585| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.013264| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012343| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.013338| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012834| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 2555 from 2013-02-01 to 2023-03-27\n",
      "ZTS training\n",
      "Epoch[1/100] | loss train:0.057040| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008581| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007001| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006628| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005172| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006578| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005835| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005951| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005339| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005479| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005108| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005070| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005512| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004583| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004420| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004396| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005241| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004550| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004667| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004375| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004215| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005325| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004575| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005075| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004485| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004540| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005413| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.003919| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004000| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004684| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003879| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004377| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004424| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004241| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004200| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.004270| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005311| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.004210| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004050| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004768| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.004028| lr:0.001000\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    date_data, close_price_data, num_data_points, display_date_range = get_data(config, ticker)\n",
    "    \n",
    "    scaler = Normalization()\n",
    "    normalized_close_price_data = scaler.fit_transform(close_price_data)\n",
    "    \n",
    "    data_x, data_x_unseen = prepare_data_x(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    data_y = prepare_data_y(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    \n",
    "    split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "    model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    n_epochs_stop = config[\"training\"][\"epoch_stop\"]\n",
    "    \n",
    "    print('{} training'.format(ticker))\n",
    "    for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "        loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch[{}/{}] | loss train:{:.6f}| lr:{:.6f}'\n",
    "                  .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, lr_train))\n",
    "        if loss_train < best_loss:\n",
    "            best_loss = loss_train\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    torch.tensor(data_x_unseen)\n",
    "    x = torch.tensor(data_x_unseen).float().to(config[\"training\"][\"device\"]).unsqueeze(0).unsqueeze(2) # this is the data type and shape required, [batch, sequence, feature]\n",
    "    prediction = model(x)\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "    prediction[0] = scaler.inverse_transform(prediction[0])\n",
    "    \n",
    "    df = pd.DataFrame([[next_day,prediction[0]]], columns = ['date','close price'])\n",
    "    path = \"csv/TimeSeries/\" + ticker + \"_predict.csv\"\n",
    "    df.to_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96e8cb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished in 11338.326119661331 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Program finished in {} seconds\".format(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
