{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37e13e9",
   "metadata": {},
   "source": [
    "This notebook explores a machine learning algorithm to predict the stock prices of SPY, the S&P 500 ETF, and is intended to utilize functions that can be easily translated to a python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01bcfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.9.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: alpha_vantage in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alpha_vantage) (3.8.4)\n",
      "Requirement already satisfied: requests in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alpha_vantage) (2.28.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->alpha_vantage) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->alpha_vantage) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->alpha_vantage) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->alpha_vantage) (2022.12.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: pandas_market_calendars in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.1.4)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas_market_calendars) (1.5.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas_market_calendars) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from pandas_market_calendars) (2.8.2)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas_market_calendars) (4.2.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.24.2)\n",
      "Requirement already satisfied: pyluach in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
      "Requirement already satisfied: korean-lunar-calendar in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jbber\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\jbber\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install alpha_vantage\n",
    "!pip install scikit-learn\n",
    "!pip install pandas_market_calendars\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26129c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas_market_calendars as mcal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb7b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file (placing here for now, some fields will change on later impplementations)\n",
    "\n",
    "config = {\n",
    "    \"alpha_vantage\": {\n",
    "        \"key\": \"2JMCN347HZ3BU9RC\", \n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"key_adjusted_close\": \"5. adjusted close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 30,\n",
    "        \"train_split_size\": 1,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"xticks_interval\": 90, # show a date every 90 days\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since for now we are only using close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\",\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epoch\": 100,\n",
    "        \"epoch_stop\": 10,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c35333f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "# tickers = np.array(sp500[0]['Symbol'])\n",
    "# if ('BF.B' in tickers)\n",
    "#     tickers[tickers.index('BF.B')] = 'BF-B'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tickers = ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ATVI', 'ADM', 'ADBE', 'ADP',\n",
    "       'AAP', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE',\n",
    "       'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN',\n",
    "       'AMCR', 'AMD', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK',\n",
    "       'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA',\n",
    "       'AAPL', 'AMAT', 'APTV', 'ACGL', 'ANET', 'AJG', 'AIZ', 'T', 'ATO',\n",
    "       'ADSK', 'AZO', 'AVB', 'AVY', 'BKR', 'BALL', 'BAC', 'BBWI', 'BAX',\n",
    "       'BDX', 'WRB', 'BRK.B', 'BBY', 'BIO', 'TECH', 'BIIB', 'BLK', 'BK',\n",
    "       'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO',\n",
    "       'BF-B', 'BG', 'CHRW', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH',\n",
    "       'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE',\n",
    "       'CNC', 'CNP', 'CDAY', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG',\n",
    "       'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX',\n",
    "       'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP',\n",
    "       'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CTVA', 'CSGP', 'COST',\n",
    "       'CTRA', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA',\n",
    "       'DE', 'DAL', 'XRAY', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DISH',\n",
    "       'DIS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DTE', 'DUK', 'DD',\n",
    "       'DXC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV',\n",
    "       'LLY', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EQT', 'EFX', 'EQIX',\n",
    "       'EQR', 'ESS', 'EL', 'ETSY', 'RE', 'EVRG', 'ES', 'EXC', 'EXPE',\n",
    "       'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FICO', 'FAST', 'FRT', 'FDX',\n",
    "       'FITB', 'FRC', 'FSLR', 'FE', 'FIS', 'FISV', 'FLT', 'FMC', 'F',\n",
    "       'FTNT', 'FTV', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GEHC',\n",
    "       'GEN', 'GNRC', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GL', 'GPN',\n",
    "       'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'PEAK', 'HSIC', 'HSY', 'HES',\n",
    "       'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ',\n",
    "       'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'ILMN', 'INCY',\n",
    "       'IR', 'PODD', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG',\n",
    "       'IVZ', 'INVH', 'IQV', 'IRM', 'JBHT', 'JKHY', 'J', 'JNJ', 'JCI',\n",
    "       'JPM', 'JNPR', 'K', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI',\n",
    "       'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS',\n",
    "       'LEN', 'LNC', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB',\n",
    "       'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH',\n",
    "       'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM',\n",
    "       'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ',\n",
    "       'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP',\n",
    "       'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'NDSN',\n",
    "       'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI',\n",
    "       'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OGN', 'OTIS',\n",
    "       'PCAR', 'PKG', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PNR', 'PEP',\n",
    "       'PKI', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'POOL',\n",
    "       'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC',\n",
    "       'PSA', 'PHM', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX',\n",
    "       'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP',\n",
    "       'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE',\n",
    "       'NOW', 'SHW', 'SPG', 'SWKS', 'SJM', 'SNA', 'SEDG', 'SO', 'LUV',\n",
    "       'SWK', 'SBUX', 'STT', 'STLD', 'STE', 'SYK', 'SYF', 'SNPS', 'SYY',\n",
    "       'TMUS', 'TROW', 'TTWO', 'TPR', 'TRGP', 'TGT', 'TEL', 'TDY', 'TFX',\n",
    "       'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG',\n",
    "       'TRV', 'TRMB', 'TFC', 'TYL', 'TSN', 'USB', 'UDR', 'ULTA', 'UNP',\n",
    "       'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VRSN', 'VRSK',\n",
    "       'VZ', 'VRTX', 'VFC', 'VTRS', 'VICI', 'V', 'VMC', 'WAB', 'WBA',\n",
    "       'WMT', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC',\n",
    "       'WRK', 'WY', 'WHR', 'WMB', 'WTW', 'GWW', 'WYNN', 'XEL', 'XYL',\n",
    "       'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474520c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to account for API limitations (on the weekend)\n",
    "\n",
    "# index = tickers.index('REG')\n",
    "# tickers = tickers[(index+1):]\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5cf266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-04-03'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "next_weeks = mcal.date_range(mcal.get_calendar('NYSE').schedule(start_date=today, end_date=(today+relativedelta(months=1))), frequency='1D')\n",
    "next_weeks = [date.strftime('%Y-%m-%d') for date in next_weeks]\n",
    "next_weeks = next_weeks[1:]\n",
    "next_day = next_weeks[0]\n",
    "next_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77bcc8",
   "metadata": {},
   "source": [
    "# Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671e4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from the configuration file\n",
    "def get_data(config, ticker):\n",
    "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"]) \n",
    "    data, meta_data = ts.get_daily_adjusted(ticker, outputsize=config[\"alpha_vantage\"][\"outputsize\"])\n",
    "\n",
    "    date_data = [date for date in data.keys()]\n",
    "    date_data.reverse()\n",
    "\n",
    "    close_price_data = [float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]]) for date in data.keys()]\n",
    "    close_price_data.reverse()\n",
    "    close_price_data = np.array(close_price_data)\n",
    "\n",
    "    num_data_points = len(date_data)\n",
    "    display_date_range = \"from \" + date_data[0] + \" to \" + date_data[num_data_points-1]\n",
    "    print(\"Number data points\", num_data_points, display_date_range)\n",
    "\n",
    "    return date_data, close_price_data, num_data_points, display_date_range\n",
    "\n",
    "class Normalization():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "    \n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    # use the next day as label\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "# Class to prepare data for training and LSTM model\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # right now we have only 1 feature, so we need to convert `x` into [batch, sequence, features]\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "# neural network model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        \n",
    "        # layer 2\n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "    \n",
    "# function for training LSTM model\n",
    "def run_epoch(dataloader, is_training=False):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        y = y.to(config[\"training\"][\"device\"])\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += (loss.detach().item() / batchsize)\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    return epoch_loss, lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab08e8",
   "metadata": {},
   "source": [
    "## Predict the Next Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9665520a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MMM training\n",
      "Epoch[1/100] | loss train:0.066839| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015922| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011171| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010863| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010050| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010715| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009718| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010068| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009039| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009948| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009361| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009384| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009316| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008631| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008655| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009768| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008094| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009030| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009002| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008212| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008832| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009135| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009288| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008915| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009308| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008463| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AOS training\n",
      "Epoch[1/100] | loss train:0.087029| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016027| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012069| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010618| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010512| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009735| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010013| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009291| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012020| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009369| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010272| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009528| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008509| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009050| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009640| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009034| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008400| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008997| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008680| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008816| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008580| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009651| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008809| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008550| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008464| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ABT training\n",
      "Epoch[1/100] | loss train:0.079460| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013539| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011730| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011347| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012194| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009421| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010204| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010140| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011075| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009828| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009642| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010152| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008778| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010370| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010918| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008954| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008899| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009634| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010331| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007947| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009213| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010446| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009216| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009496| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008101| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008625| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008845| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008136| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009419| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2580 from 2013-01-02 to 2023-03-31\n",
      "ABBV training\n",
      "Epoch[1/100] | loss train:0.058192| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009003| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007131| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007079| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005788| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006800| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006223| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005629| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006026| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005152| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004657| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004621| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004927| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004795| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004733| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005604| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004787| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005002| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004649| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004382| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005395| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005419| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004942| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005696| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004995| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005564| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004356| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004408| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004481| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004900| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005041| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004086| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004742| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005092| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004244| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.004368| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004845| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005343| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004108| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004512| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003785| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003714| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.003324| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003817| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.003552| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003472| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.003451| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.003369| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.003331| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.003516| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.003585| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.003421| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.003394| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5460 from 2001-07-19 to 2023-03-31\n",
      "ACN training\n",
      "Epoch[1/100] | loss train:0.080979| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014688| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011497| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011369| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011634| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012038| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009247| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010928| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008482| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010580| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009642| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010728| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009928| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008178| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010468| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010370| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010155| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/100] | loss train:0.010119| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008708| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008511| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008792| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008749| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009086| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008485| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ATVI training\n",
      "Epoch[1/100] | loss train:0.049947| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015757| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012745| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012041| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011534| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009840| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011920| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009424| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010734| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009813| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009702| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009787| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011262| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009494| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009750| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009433| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010163| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009653| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ADM training\n",
      "Epoch[1/100] | loss train:0.072548| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015651| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014745| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014539| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012293| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015926| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012195| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011200| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014739| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013089| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011396| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011616| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011761| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011901| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011613| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010819| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012899| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011417| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010832| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012013| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010987| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010451| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009965| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011125| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012010| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012370| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010422| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011217| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011239| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010962| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011591| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010869| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ADBE training\n",
      "Epoch[1/100] | loss train:0.092180| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018059| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015776| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015157| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012653| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011170| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014125| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011938| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011540| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011250| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010859| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011129| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010327| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010678| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012547| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011753| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011289| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010753| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010757| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009988| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011242| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013354| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010205| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009610| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013334| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010356| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010149| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009732| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009301| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010284| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010358| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009207| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009901| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009419| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010279| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008480| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010611| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011307| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010665| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008680| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007930| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008338| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008077| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008196| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008362| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008072| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007925| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008139| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007472| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007518| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007512| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007421| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007152| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007266| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007853| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007976| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007466| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007901| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007118| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007691| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007320| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007957| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007598| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007601| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007278| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007453| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007216| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007432| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007527| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5891 from 1999-11-01 to 2023-03-30\n",
      "ADP training\n",
      "Epoch[1/100] | loss train:0.071239| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014948| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014319| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013159| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011482| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014169| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011027| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010281| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012228| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009527| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010334| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010927| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009836| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009941| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011407| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008897| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009860| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009442| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010751| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008947| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009255| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009162| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009729| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009677| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009523| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009435| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5371 from 2001-11-29 to 2023-03-31\n",
      "AAP training\n",
      "Epoch[1/100] | loss train:0.078410| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014461| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012307| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013199| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/100] | loss train:0.010733| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011522| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008962| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010467| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010704| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011225| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009392| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010559| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010198| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011358| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009343| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008920| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010111| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009273| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009118| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010695| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009544| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009732| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009204| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009481| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009122| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008902| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009660| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009426| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009395| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008848| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008805| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009275| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008582| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009370| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009143| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009270| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008583| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008943| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009094| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008373| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007965| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007534| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007350| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007653| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007575| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007213| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007875| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007578| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007243| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007546| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007014| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007356| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007557| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007251| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006929| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007109| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007158| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007434| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007284| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007381| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007168| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007444| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007327| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007192| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007094| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AES training\n",
      "Epoch[1/100] | loss train:0.091690| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020999| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.020757| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017632| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.018700| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016602| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015600| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015998| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014653| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014839| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015000| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.018482| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015859| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015892| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012118| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013574| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.015530| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014263| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014042| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.015427| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014510| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013313| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014986| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.014472| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012661| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AFL training\n",
      "Epoch[1/100] | loss train:0.080646| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015247| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016725| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012594| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012062| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011329| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012598| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012305| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011244| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011638| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009897| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010812| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012052| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011522| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010479| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010470| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010517| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010054| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009741| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010668| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010179| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010089| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010263| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009275| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010674| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009408| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010594| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009688| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011830| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009950| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009505| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010550| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5879 from 1999-11-18 to 2023-03-31\n",
      "A training\n",
      "Epoch[1/100] | loss train:0.063788| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015117| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015776| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012140| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015372| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011860| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012970| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013428| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011769| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011346| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012156| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011207| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012097| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010679| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011579| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012436| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012385| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010212| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011731| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013118| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011222| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012213| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011447| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009989| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010807| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011000| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010273| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011426| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010459| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010290| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009833| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011113| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011182| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009777| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010780| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011328| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010366| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010660| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008956| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008122| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008153| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[44/100] | loss train:0.008126| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007716| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008480| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008044| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008520| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008429| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008177| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007970| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008431| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007901| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008531| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007860| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "APD training\n",
      "Epoch[1/100] | loss train:0.069953| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015306| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013972| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016143| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012007| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011785| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010370| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010739| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011732| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010098| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011663| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010583| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009851| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009662| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010513| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010131| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010339| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011279| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010444| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009552| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010794| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010067| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010027| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009282| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009911| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009275| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008787| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008860| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008920| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009166| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009434| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010200| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009196| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009063| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010230| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008072| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007851| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007617| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007613| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007662| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007525| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007459| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007562| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007680| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007517| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007207| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007345| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006997| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007650| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007408| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007474| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007407| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008170| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007588| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007164| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007397| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006806| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006852| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007402| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007460| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007706| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007834| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007469| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007426| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007074| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007433| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007223| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AKAM training\n",
      "Epoch[1/100] | loss train:0.085157| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.022897| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.025062| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.019431| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.020465| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.018491| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.020044| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.020027| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.020733| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.020599| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.023329| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.020747| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.018231| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.017760| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.020322| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.018929| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.018110| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.016098| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.018524| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.019120| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.016522| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.016254| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.017353| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.016918| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.019298| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.017097| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.018885| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.015271| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.021867| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.018647| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.015586| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.015961| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.015928| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.016720| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.015933| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.016708| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.016851| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.017517| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ALK training\n",
      "Epoch[1/100] | loss train:0.061301| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015541| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014698| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015165| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011291| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011917| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010805| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012424| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010666| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010051| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012324| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010431| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009612| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010802| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010210| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009507| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011244| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009846| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009887| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009964| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009461| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009437| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009762| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009923| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010226| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010017| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009608| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010748| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010473| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010197| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010110| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010037| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ALB training\n",
      "Epoch[1/100] | loss train:0.079137| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016344| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016189| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017310| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015423| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013525| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.017097| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015289| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/100] | loss train:0.013472| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013160| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013473| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012992| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012012| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013528| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013733| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.015446| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013408| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012849| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011602| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014357| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011607| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011766| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012279| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011966| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011561| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013027| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012594| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011664| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012876| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011089| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010946| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011704| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011550| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011644| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012881| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012544| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012409| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010825| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011572| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011411| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010114| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009418| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008340| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008933| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008926| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008574| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009013| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009278| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008770| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008642| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008790| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008923| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008289| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008752| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009103| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009437| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009305| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009275| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008619| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008139| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007966| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009429| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007531| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008524| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008723| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008985| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008911| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.009327| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008391| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.009354| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.009498| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.008882| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.008622| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ARE training\n",
      "Epoch[1/100] | loss train:0.064695| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014288| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013352| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010685| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012085| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010728| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011160| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011951| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009354| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010645| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010178| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010507| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011528| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010561| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009418| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009878| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009926| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010170| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5578 from 2001-01-30 to 2023-03-31\n",
      "ALGN training\n",
      "Epoch[1/100] | loss train:0.086798| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019033| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017647| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013209| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013821| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013524| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012891| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013305| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012345| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013170| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012719| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012209| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015524| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012203| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010788| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011234| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011645| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011541| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011585| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011341| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012060| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010976| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011553| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010666| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009286| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010846| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010460| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011134| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009622| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012541| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012257| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010216| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009715| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011042| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2358 from 2013-11-18 to 2023-03-31\n",
      "ALLE training\n",
      "Epoch[1/100] | loss train:0.056493| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011764| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008417| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007426| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007668| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006536| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006108| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007349| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006205| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006544| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006043| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005640| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006523| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005708| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005835| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006247| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006190| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005889| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006567| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005885| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005735| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005107| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005545| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006184| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005523| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005282| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005716| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005103| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006224| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005923| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005786| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005054| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005575| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005346| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005641| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005548| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005296| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005362| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004951| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005430| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.004862| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.004692| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004580| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004090| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/100] | loss train:0.004739| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004651| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004618| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004582| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004314| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004245| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004383| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004602| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004367| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004489| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LNT training\n",
      "Epoch[1/100] | loss train:0.072173| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012813| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013106| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010255| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009989| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009242| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010262| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010012| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009038| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008594| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009858| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009196| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009483| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009998| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008891| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008698| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009628| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008739| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008218| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009600| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008606| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009640| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008314| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008088| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009342| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008729| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008512| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008490| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008292| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008443| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008620| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008593| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008665| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008132| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ALL training\n",
      "Epoch[1/100] | loss train:0.048680| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013469| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012509| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010254| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010183| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011640| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012181| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011734| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009836| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010915| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009576| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009729| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011532| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009864| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009760| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010273| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009737| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009785| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009750| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010170| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009528| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009435| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009093| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010000| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009326| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010058| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009667| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008745| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009964| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009703| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009630| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008957| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008895| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010312| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009157| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008967| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009457| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009251| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4687 from 2004-08-19 to 2023-03-31\n",
      "GOOGL training\n",
      "Epoch[1/100] | loss train:0.051169| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010215| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016339| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014621| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009053| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009526| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010610| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010803| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008805| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008719| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008893| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009994| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008222| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009314| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007332| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009273| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008355| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009316| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007105| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007912| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008231| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009333| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008059| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007618| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007037| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008408| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008003| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008752| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007440| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008091| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007912| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008204| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007694| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008595| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007992| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2270 from 2014-03-27 to 2023-03-31\n",
      "GOOG training\n",
      "Epoch[1/100] | loss train:0.052701| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009026| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006876| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006931| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005593| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005974| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006849| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005165| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005058| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005507| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005130| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004715| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005254| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004838| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004668| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005209| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005299| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004442| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004445| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005485| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004574| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004597| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004588| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004709| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004082| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004053| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004693| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005574| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004774| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004604| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003984| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004187| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004774| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004192| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.003977| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.004032| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004331| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.003731| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.003846| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004129| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003460| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003374| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/100] | loss train:0.003217| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003396| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.003282| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003287| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.003259| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.003500| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.003469| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.003448| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.003294| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.003448| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.003243| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MO training\n",
      "Epoch[1/100] | loss train:0.063082| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012385| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010220| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009774| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008790| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008768| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008459| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008756| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008678| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008215| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008684| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007790| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008884| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008231| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008395| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008533| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008212| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007755| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007934| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007950| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008721| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007843| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007771| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008387| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008031| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007921| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007750| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007991| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007955| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007803| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007484| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007685| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008142| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008276| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007865| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007895| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007576| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008010| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007773| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007935| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006983| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006872| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006485| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006387| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006608| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006349| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006552| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006752| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006353| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006375| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006472| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006443| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006613| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006345| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006690| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006518| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006521| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006495| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006377| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006552| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006684| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006728| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006292| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006485| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006353| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006636| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006422| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006722| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006601| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006534| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006805| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006633| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006667| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AMZN training\n",
      "Epoch[1/100] | loss train:0.078281| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015820| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012765| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012447| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010740| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012510| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013194| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010412| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010982| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011015| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010722| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010669| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012532| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009636| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009974| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011413| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010391| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009421| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009737| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009827| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010767| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010900| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010507| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008663| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010159| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008954| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009291| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009676| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009349| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008487| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009106| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008990| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008896| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008994| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009943| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009363| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008697| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008643| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009135| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007631| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007934| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007162| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007252| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007230| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007161| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007615| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007078| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007203| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006511| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006765| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006810| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007809| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006814| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007062| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006804| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007205| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007600| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006903| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007043| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 960 from 2019-06-11 to 2023-03-31\n",
      "AMCR training\n",
      "Epoch[1/100] | loss train:0.084296| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016422| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012132| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010834| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009486| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008408| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008671| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007692| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007662| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007487| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007270| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008235| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007466| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006828| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008035| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007952| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008223| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007483| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007105| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[20/100] | loss train:0.007269| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007414| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006861| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007064| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006951| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AMD training\n",
      "Epoch[1/100] | loss train:0.086402| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021339| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019013| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.018620| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015665| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013651| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015230| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015649| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013925| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014491| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012349| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013018| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015024| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012683| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013510| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013673| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014992| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014938| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014103| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014083| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.015903| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AEE training\n",
      "Epoch[1/100] | loss train:0.092531| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017372| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012168| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011035| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011760| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010186| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010108| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010070| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009999| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009917| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009891| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009127| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009472| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009357| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009668| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010040| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009842| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008580| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009488| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009150| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008439| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009196| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008555| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009945| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008725| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008506| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008102| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008523| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008526| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009415| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008734| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008587| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008452| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008889| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008378| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008438| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008789| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4408 from 2005-09-27 to 2023-03-31\n",
      "AAL training\n",
      "Epoch[1/100] | loss train:0.045688| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013167| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012508| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011128| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010240| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009876| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011035| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010397| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009850| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009924| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008837| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009220| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009971| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009127| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008951| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009608| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009461| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008985| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009338| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009600| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AEP training\n",
      "Epoch[1/100] | loss train:0.061617| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014073| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011416| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010566| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010553| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010551| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011506| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009402| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009719| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010768| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009426| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010177| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008488| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009385| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009404| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010167| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009231| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009138| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008952| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008915| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008632| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009374| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008910| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AXP training\n",
      "Epoch[1/100] | loss train:0.070028| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016674| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015947| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013000| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014158| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012810| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012841| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012273| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011712| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011406| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012531| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011781| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010239| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010634| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012226| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012328| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011912| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011412| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010933| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012205| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010553| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012499| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AIG training\n",
      "Epoch[1/100] | loss train:0.058539| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011591| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011910| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009913| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009688| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010199| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008671| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009172| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008788| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009163| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009596| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009258| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009176| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008701| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008411| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008380| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008755| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008723| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008915| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007955| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008639| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008110| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008601| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009485| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009082| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/100] | loss train:0.007918| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008219| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008692| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007789| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008445| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008363| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007745| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008051| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008504| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007732| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008268| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007813| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008231| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007961| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007214| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006980| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007040| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006387| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007067| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007011| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006912| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006978| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006603| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007193| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007157| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006886| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007203| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006505| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AMT training\n",
      "Epoch[1/100] | loss train:0.072739| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013108| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013231| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010340| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012362| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011253| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009331| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010888| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013510| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011209| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009790| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010593| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009180| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010580| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010418| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009834| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009905| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008413| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009928| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008868| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009713| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009248| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008861| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009352| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008883| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009490| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008354| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008802| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008363| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008467| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008851| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009796| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008252| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008995| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010248| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009309| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008480| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009173| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008458| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007511| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007379| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007465| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007178| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007203| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007173| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007171| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006959| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007080| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007081| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007134| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007162| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006977| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006829| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007232| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007172| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006633| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006887| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007334| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006900| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006964| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007142| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006988| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006861| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006844| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006708| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006842| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3762 from 2008-04-23 to 2023-03-31\n",
      "AWK training\n",
      "Epoch[1/100] | loss train:0.063844| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009733| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011102| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008238| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008137| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006822| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008013| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006692| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006117| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006477| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006692| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006510| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007545| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006231| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006305| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006664| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006778| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006324| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006839| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4416 from 2005-09-15 to 2023-03-31\n",
      "AMP training\n",
      "Epoch[1/100] | loss train:0.060016| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011224| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010021| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009599| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010156| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008764| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009248| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008305| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008882| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007520| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007420| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009725| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009409| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007466| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008882| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007117| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008122| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007753| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009085| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007281| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006992| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007977| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008157| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007430| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008078| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008545| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007486| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007728| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007380| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007742| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007399| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ABC training\n",
      "Epoch[1/100] | loss train:0.064223| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015456| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015173| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011412| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011415| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010491| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011312| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010840| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010608| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011025| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010748| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012478| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010154| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009096| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/100] | loss train:0.009436| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011097| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009666| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009707| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009334| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009089| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009329| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010213| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010512| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008722| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009791| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008923| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008374| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009397| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010014| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009368| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009155| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009123| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009501| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008982| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008278| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009523| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008565| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008428| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008413| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007168| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007505| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007099| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007483| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006746| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006718| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006703| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006841| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006968| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006734| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007150| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007238| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006992| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006805| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007305| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007242| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007243| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AME training\n",
      "Epoch[1/100] | loss train:0.074558| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014283| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013028| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011847| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011151| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009697| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010655| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010607| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009346| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008503| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010922| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010912| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009075| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009106| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010849| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009332| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009200| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009613| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008616| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008810| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AMGN training\n",
      "Epoch[1/100] | loss train:0.057923| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014481| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011775| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011131| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010457| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009870| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011708| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010376| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009765| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010210| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011474| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008880| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009281| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009910| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009797| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010133| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009646| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009603| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009686| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008459| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009204| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009429| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009708| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008795| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009113| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009233| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008715| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008826| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "APH training\n",
      "Epoch[1/100] | loss train:0.061533| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012774| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014684| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010751| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013662| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010918| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009748| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010485| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010107| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010381| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009204| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009736| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008835| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009772| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009529| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009437| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009237| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009593| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009278| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008363| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009401| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009695| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009654| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008231| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010267| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008852| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008624| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009146| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009922| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008611| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007979| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008595| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008602| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008858| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008047| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008253| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008743| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008868| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007433| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006906| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007273| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006793| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006618| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006951| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006967| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007469| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006753| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006462| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006584| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006812| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006654| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006639| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006647| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006939| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006756| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006303| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006590| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006430| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006808| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006608| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007100| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006918| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006982| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006551| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006819| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006422| lr:0.001000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5891 from 1999-11-01 to 2023-03-30\n",
      "ADI training\n",
      "Epoch[1/100] | loss train:0.070899| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016473| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014181| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013071| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012968| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011769| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010546| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010643| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012414| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010635| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010858| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012532| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009755| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010876| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010653| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010316| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012688| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010660| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012308| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011523| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010482| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012132| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010730| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ANSS training\n",
      "Epoch[1/100] | loss train:0.062615| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018596| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012195| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011317| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011467| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011091| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011224| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010965| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010368| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011183| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010171| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009279| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011485| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010343| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009077| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010575| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012226| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011032| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010225| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010631| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009320| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009414| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008725| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010731| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009694| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008846| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010319| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009099| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009907| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009331| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009965| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009291| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009480| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AON training\n",
      "Epoch[1/100] | loss train:0.055042| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014245| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012665| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011595| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009802| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010790| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011268| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009479| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010769| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010277| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009768| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011477| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010195| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010010| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008969| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010748| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008880| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009973| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009617| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009071| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009813| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009997| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009116| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009142| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009494| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009577| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008893| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "APA training\n",
      "Epoch[1/100] | loss train:0.080015| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019726| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016657| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015654| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015686| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015257| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015137| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013443| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013552| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013972| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014090| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013020| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012953| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014185| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014083| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011764| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012796| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012392| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013358| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013624| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012859| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013616| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013436| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013553| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012313| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012910| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AAPL training\n",
      "Epoch[1/100] | loss train:0.060267| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017830| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014988| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014577| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011191| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013741| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011310| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012601| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011291| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011595| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011405| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010816| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011895| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012156| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011980| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010100| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011652| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009525| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010359| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011164| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010394| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010885| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011075| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009838| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009154| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011455| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010455| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011237| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010167| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011011| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009171| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010106| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009957| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009375| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010333| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AMAT training\n",
      "Epoch[1/100] | loss train:0.068051| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015367| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018224| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014994| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014123| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013508| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014805| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013305| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013765| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014155| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014883| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/100] | loss train:0.013630| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014635| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013478| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014780| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012496| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012827| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012063| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014210| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012366| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012171| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011331| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012305| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012000| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012514| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011360| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011359| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013795| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.013497| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010191| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012127| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010878| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011066| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012487| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011200| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010191| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010914| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012214| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013131| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.012062| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2860 from 2011-11-17 to 2023-03-31\n",
      "APTV training\n",
      "Epoch[1/100] | loss train:0.055156| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013272| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010498| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009471| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010164| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010435| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008685| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008378| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007671| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008266| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008938| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008451| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009330| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007925| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007756| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007097| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007805| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007297| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007548| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007224| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007857| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007908| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007823| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009189| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007190| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006972| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007968| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007404| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008167| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006553| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006898| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008845| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007522| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008281| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007849| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006449| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008249| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008118| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007873| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007908| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005757| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005808| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005802| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006337| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005928| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006154| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005636| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005207| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005529| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006291| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005575| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005840| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005826| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005934| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006200| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006217| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005644| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005712| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ACGL training\n",
      "Epoch[1/100] | loss train:0.057900| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015051| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013128| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014165| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011374| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011263| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011563| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010780| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012444| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010503| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009968| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011693| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010582| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010884| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012799| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010417| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010748| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009473| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011058| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009632| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010321| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011030| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010345| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009387| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009396| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010317| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009515| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009675| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009860| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009020| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008851| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010081| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010006| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009026| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009551| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008810| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009266| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009155| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009403| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007231| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007299| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007236| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007405| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007324| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006952| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007613| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006926| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006850| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007022| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006848| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007620| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006895| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006630| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007194| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007281| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007180| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007297| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006773| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006895| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006629| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007268| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007346| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006938| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006938| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006758| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007192| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007229| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006961| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006852| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006799| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2221 from 2014-06-06 to 2023-03-31\n",
      "ANET training\n",
      "Epoch[1/100] | loss train:0.048631| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010891| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006682| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007737| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008596| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6/100] | loss train:0.007706| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006216| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005079| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005310| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005598| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005505| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005374| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006116| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005557| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006258| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005701| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004788| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005842| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005269| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006150| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004910| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005830| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005126| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006785| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005149| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005375| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004779| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005828| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004845| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004460| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005339| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005968| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007686| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005175| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005450| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005300| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004926| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005234| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004919| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004733| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AJG training\n",
      "Epoch[1/100] | loss train:0.095526| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014142| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016566| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011550| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011597| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011125| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015031| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010065| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009271| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012962| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012213| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009092| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011404| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010755| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009823| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009760| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009672| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009070| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010457| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009943| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011446| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010220| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010058| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010998| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009545| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011662| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009106| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008421| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008693| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008320| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009004| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008363| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008494| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008951| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009425| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010082| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008329| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009112| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009266| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007799| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007073| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007370| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007026| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006715| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006759| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007202| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006505| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007167| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006549| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006851| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006658| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006699| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006577| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006258| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006795| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006621| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006383| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006544| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006928| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007082| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006654| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006102| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006683| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006944| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006582| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006465| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006482| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006465| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006996| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006775| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006592| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006820| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4822 from 2004-02-05 to 2023-03-31\n",
      "AIZ training\n",
      "Epoch[1/100] | loss train:0.077230| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013045| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011076| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009702| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008339| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010679| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008945| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009541| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009167| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008628| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008959| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008133| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008815| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009708| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009288| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008549| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008618| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008503| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008150| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008083| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007773| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008416| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008365| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008183| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007873| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008396| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007397| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008889| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008301| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008857| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006891| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008016| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007625| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007979| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007748| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007306| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009541| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007930| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007390| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006413| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006748| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006554| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005925| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006507| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006331| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006194| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005908| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006185| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006108| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006495| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006400| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006377| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006176| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005935| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006351| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006427| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[58/100] | loss train:0.006158| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "T training\n",
      "Epoch[1/100] | loss train:0.073284| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015861| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013242| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011782| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011625| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011383| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011189| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012672| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011309| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011220| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010775| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010088| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011433| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010495| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011081| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010587| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010453| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010798| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010608| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010630| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010240| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010625| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ATO training\n",
      "Epoch[1/100] | loss train:0.061090| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010896| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010326| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010188| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009681| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009777| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008773| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008321| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008605| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009567| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008512| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008574| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008567| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008368| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008568| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008194| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008672| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008255| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008594| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008150| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008089| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008105| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008439| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008282| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008362| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008041| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007863| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008157| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008464| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009157| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007765| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008912| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008352| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007625| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008288| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008463| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008139| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007938| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007983| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007212| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006782| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006449| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006849| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006618| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006680| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006477| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006562| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006583| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006762| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006562| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006520| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006619| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ADSK training\n",
      "Epoch[1/100] | loss train:0.083959| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016410| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014403| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012494| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011956| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013547| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012108| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011284| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012865| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011238| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010934| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011739| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010618| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010897| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010123| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011487| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012615| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011741| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010627| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010327| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010650| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011105| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011222| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009683| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010837| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009742| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011109| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010626| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011950| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010002| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011182| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008930| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009641| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009392| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009506| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009455| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010833| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010007| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010611| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008655| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008644| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008244| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007903| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008131| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007930| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008143| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008210| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008056| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008037| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007929| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008001| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007338| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007842| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008546| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008021| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007953| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007865| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008205| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007985| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007694| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008061| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008220| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AZO training\n",
      "Epoch[1/100] | loss train:0.077247| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015783| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013516| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012515| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012974| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010890| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010229| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010537| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010633| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010165| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010592| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010056| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012420| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009318| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012998| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009123| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010583| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009987| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009786| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011088| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009849| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010821| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010928| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[24/100] | loss train:0.009159| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010379| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009654| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AVB training\n",
      "Epoch[1/100] | loss train:0.057731| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014664| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011722| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011456| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010405| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009808| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010212| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009669| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009743| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009672| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009464| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010454| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009567| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009345| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009504| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009272| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009147| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009602| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010129| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008573| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009332| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009298| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009014| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009460| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010176| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009052| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009396| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009139| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009306| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "AVY training\n",
      "Epoch[1/100] | loss train:0.075743| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018485| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014047| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011766| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015357| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012584| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013498| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014382| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011811| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011293| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012958| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012207| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011924| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013117| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011306| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012266| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011322| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013209| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010119| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010424| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013002| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012553| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010048| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010121| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011819| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010586| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009030| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010109| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011155| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010298| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010792| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010429| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011238| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011945| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010761| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009993| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1446 from 2017-07-05 to 2023-03-31\n",
      "BKR training\n",
      "Epoch[1/100] | loss train:0.063530| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015905| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016069| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011015| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009397| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010450| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008047| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009581| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010499| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010500| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010848| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009701| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011425| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011607| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008208| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009264| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006821| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010139| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010700| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011310| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011291| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008813| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008628| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012976| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009196| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011311| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008249| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BALL training\n",
      "Epoch[1/100] | loss train:0.059589| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013068| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010788| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014050| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012050| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011464| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011376| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010279| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011248| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010512| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010669| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010008| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009501| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010224| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010395| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009316| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009293| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010329| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010799| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009391| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009590| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009251| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010970| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010939| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010115| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009102| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008836| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010330| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009452| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009443| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008496| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008614| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009969| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007992| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009200| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009060| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008656| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009720| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007975| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007336| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007436| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007405| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007335| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007427| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007236| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007478| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007554| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007409| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007430| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007019| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007203| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006671| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006950| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007667| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007421| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007314| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007285| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007383| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006996| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007164| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007507| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006962| lr:0.001000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BAC training\n",
      "Epoch[1/100] | loss train:0.054839| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017580| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014658| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013569| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014144| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012464| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014404| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012552| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014027| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013472| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012378| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012032| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011889| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012604| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013127| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012357| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011811| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012257| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012581| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012502| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012278| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011491| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011992| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012050| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011635| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011370| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012624| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010969| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011791| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011402| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011857| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011063| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011345| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012076| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011448| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012128| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011599| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011591| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BBWI training\n",
      "Epoch[1/100] | loss train:0.068976| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016127| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012799| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012284| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012390| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013893| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010449| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010601| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011260| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010976| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011333| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010658| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011309| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010427| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010309| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010753| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010064| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009962| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010579| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009613| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009872| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010661| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010127| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009355| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009704| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009976| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009422| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009264| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009225| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009787| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010279| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009381| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009362| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008694| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009827| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010572| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010305| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008107| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008305| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008072| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008236| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007680| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008125| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007865| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007935| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007925| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008034| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008245| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007356| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008413| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007542| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007583| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007769| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007943| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007971| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007868| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007923| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007658| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007860| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BAX training\n",
      "Epoch[1/100] | loss train:0.061533| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013276| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013208| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011857| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010771| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010477| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009614| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010360| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010147| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009252| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010243| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010340| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009160| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009196| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008806| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009662| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009608| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009537| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009489| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008888| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009815| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008780| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009812| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008632| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009846| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009747| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008575| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009269| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008887| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009323| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008511| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008556| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009492| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008872| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008392| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007671| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007552| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007498| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007121| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007211| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007343| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007259| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007474| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007634| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007460| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007121| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007184| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007219| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007288| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007118| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007669| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007078| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007339| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007231| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007620| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007456| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007296| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006957| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[64/100] | loss train:0.007171| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007165| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007542| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007377| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007378| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007294| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007078| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007158| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006942| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007209| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006906| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007131| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007179| lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007195| lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007145| lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007207| lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007163| lr:0.001000\n",
      "Epoch[81/100] | loss train:0.006968| lr:0.000100\n",
      "Epoch[82/100] | loss train:0.007460| lr:0.000100\n",
      "Epoch[83/100] | loss train:0.007187| lr:0.000100\n",
      "Epoch[84/100] | loss train:0.007050| lr:0.000100\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BDX training\n",
      "Epoch[1/100] | loss train:0.063521| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012162| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011528| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010110| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009552| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009281| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009305| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009131| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008385| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008789| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009470| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008653| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009066| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009260| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008161| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008028| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008544| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008792| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008368| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008369| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008176| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008283| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008106| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008257| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008034| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008675| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WRB training\n",
      "Epoch[1/100] | loss train:0.065054| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013469| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014612| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012541| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011820| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010353| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011706| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010611| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010962| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010974| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010300| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010211| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010251| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009813| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010401| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010035| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008483| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010627| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009672| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009870| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009171| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010463| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009795| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010864| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010178| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009858| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009843| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5891 from 1999-11-01 to 2023-03-30\n",
      "BRK.B training\n",
      "Epoch[1/100] | loss train:0.053074| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015890| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013314| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013013| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012150| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011183| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012223| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009301| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010066| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011179| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009771| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011113| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010113| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009494| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009764| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010003| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009416| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008523| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010961| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010176| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010224| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008844| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008930| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008901| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008353| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010518| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009220| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009071| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009435| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008467| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009144| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009372| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009765| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008960| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010066| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BBY training\n",
      "Epoch[1/100] | loss train:0.065059| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018398| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016319| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.018630| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014638| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013948| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015616| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013737| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013365| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012964| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013414| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012444| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014421| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013995| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012697| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012038| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012067| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013542| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012189| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012221| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011829| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012544| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010979| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012221| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011090| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010999| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011381| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010781| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012581| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011869| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011948| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011358| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011315| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011434| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011744| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.013441| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011244| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011421| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BIO training\n",
      "Epoch[1/100] | loss train:0.076022| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014517| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013240| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012424| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011462| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013655| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010987| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012175| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/100] | loss train:0.011290| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011160| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010551| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010210| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010613| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009587| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011259| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010374| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009702| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011891| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009698| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009401| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011045| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009849| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009654| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010550| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009722| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010735| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008838| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009331| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010053| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010358| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009711| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009867| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009697| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009994| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009477| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010064| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010176| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TECH training\n",
      "Epoch[1/100] | loss train:0.085460| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016866| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016915| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014437| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012921| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013357| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014739| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011863| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015011| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012021| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011532| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013796| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011459| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011730| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011047| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012578| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012093| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011547| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012130| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010208| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010281| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011533| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011222| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011794| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010433| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011133| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010270| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010233| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010626| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009535| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011043| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010136| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011957| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010406| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010272| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011070| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009886| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010391| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010733| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011320| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BIIB training\n",
      "Epoch[1/100] | loss train:0.070603| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016920| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015241| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013610| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012890| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011570| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011047| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011462| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011212| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011584| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011796| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010879| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012331| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010868| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011340| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009988| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010827| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011432| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011008| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011781| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010877| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010335| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012040| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009893| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010341| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010801| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010856| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010519| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010422| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010883| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010515| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010651| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010287| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010517| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BLK training\n",
      "Epoch[1/100] | loss train:0.061582| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014409| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012439| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011523| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011238| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013204| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012695| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010156| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011881| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010558| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010455| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009145| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010397| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009597| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008970| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008809| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010394| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010929| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009401| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008370| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009397| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010869| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008869| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009191| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009510| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008899| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008618| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009003| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010078| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009674| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BK training\n",
      "Epoch[1/100] | loss train:0.067390| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018857| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017637| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017008| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014441| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015140| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015264| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014103| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015307| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013634| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014509| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014322| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015568| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014126| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014977| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014568| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013557| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013587| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014117| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014861| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013999| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014102| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013338| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013988| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014463| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[26/100] | loss train:0.013676| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.015020| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013725| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.013903| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013600| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013577| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.014475| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013719| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BA training\n",
      "Epoch[1/100] | loss train:0.070816| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017039| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014025| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014467| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013701| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012550| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012009| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014514| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011388| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011102| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010759| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011567| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011754| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010950| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010948| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010340| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010023| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010434| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011168| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010249| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010820| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010911| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010339| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010314| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010242| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010947| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BKNG training\n",
      "Epoch[1/100] | loss train:0.068547| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012969| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013342| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010478| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010303| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009989| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010000| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010631| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010811| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008567| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009228| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010539| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009289| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008833| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009583| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009513| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008976| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009256| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008978| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BWA training\n",
      "Epoch[1/100] | loss train:0.064943| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015642| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012146| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011074| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012250| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011347| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010688| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010874| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011279| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011588| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009951| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010529| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009685| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010900| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009880| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011240| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010159| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011055| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009909| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010210| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009620| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009643| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010236| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009297| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010059| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009319| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010325| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009528| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009536| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009719| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009617| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010004| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009796| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009191| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010703| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009069| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008303| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008536| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008161| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008260| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008156| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008095| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007972| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008153| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007961| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007886| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008093| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008372| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008471| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008241| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008107| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007908| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008158| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008000| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008346| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008094| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BXP training\n",
      "Epoch[1/100] | loss train:0.066133| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015391| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011525| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013303| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010779| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011713| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010784| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010196| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010257| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010061| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010128| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010605| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009602| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011018| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009078| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009779| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010024| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009589| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009865| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010031| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009189| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009753| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009260| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008806| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009229| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009772| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008734| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009757| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009313| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009340| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008969| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009797| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008989| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009034| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008184| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007742| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008201| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007859| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007809| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007887| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/100] | loss train:0.007978| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008195| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007580| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007929| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007911| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007757| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007857| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007983| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007979| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007819| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008144| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007577| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007806| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007490| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007859| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007831| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007808| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007524| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007920| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007829| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008081| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007958| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007777| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007684| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BSX training\n",
      "Epoch[1/100] | loss train:0.066812| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015066| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011804| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011825| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010336| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011487| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010812| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010610| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010542| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010102| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009764| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011125| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010267| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010028| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009875| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009749| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009780| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009838| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010256| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010030| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009906| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009563| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009873| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009867| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009460| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009539| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009937| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010411| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008568| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009292| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009688| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009190| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009631| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009223| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008782| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009873| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008500| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008222| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007572| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007853| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007951| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007874| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008051| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008081| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007611| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007748| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007818| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007800| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007762| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BMY training\n",
      "Epoch[1/100] | loss train:0.056127| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013134| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012618| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011727| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010579| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011302| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010414| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010337| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011096| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010667| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009516| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009874| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010723| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010282| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009812| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009309| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009131| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010357| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009187| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008894| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008667| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010616| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008765| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009873| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009108| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008966| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009795| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009604| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009045| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009679| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009386| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3437 from 2009-08-06 to 2023-03-31\n",
      "AVGO training\n",
      "Epoch[1/100] | loss train:0.058398| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011036| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009124| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008357| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007385| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007441| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007433| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008227| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009252| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006857| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007576| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007252| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005827| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006865| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006310| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006875| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006193| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006508| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005875| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005941| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006030| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007675| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006213| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4029 from 2007-04-02 to 2023-03-31\n",
      "BR training\n",
      "Epoch[1/100] | loss train:0.052044| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011646| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010510| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007838| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007567| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007583| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007311| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006910| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007484| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006579| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006356| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007168| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007870| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008360| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006467| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006435| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006268| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006338| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007265| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006239| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006474| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006352| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006353| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006972| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006461| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006370| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006807| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005984| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006940| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005753| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006083| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[32/100] | loss train:0.005661| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006329| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006151| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006186| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005721| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005893| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005673| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006035| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005910| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005205| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.004973| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004945| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005030| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004911| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004772| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004800| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004983| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004908| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004864| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004815| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004676| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005003| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004706| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004643| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004575| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004959| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004548| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004710| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004816| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004633| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004805| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004738| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004651| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004796| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004853| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.004790| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.004919| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BRO training\n",
      "Epoch[1/100] | loss train:0.067609| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014389| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013156| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013140| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011698| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013522| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012369| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011323| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011899| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010760| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010367| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010771| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010367| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011260| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010174| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010104| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010064| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011335| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010031| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009416| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009334| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010820| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009901| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010289| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010735| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010150| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009027| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008980| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008793| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010344| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008165| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009021| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010395| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009961| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008549| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009195| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008776| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008695| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007464| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006861| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006877| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007271| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006908| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007250| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007494| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006562| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007045| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006809| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007066| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007267| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007434| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006383| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006929| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006851| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007597| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006505| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006997| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006725| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007676| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006532| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007450| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006965| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5891 from 1999-11-01 to 2023-03-30\n",
      "BF-B training\n",
      "Epoch[1/100] | loss train:0.059852| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013352| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011167| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011083| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010348| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009946| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009720| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009878| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009445| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010874| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009295| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010183| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009351| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009280| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010332| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009378| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009337| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009361| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009838| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009033| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009089| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009570| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008743| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008973| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008786| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008402| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009756| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008616| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008512| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009034| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009261| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008261| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008288| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007632| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008290| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008507| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008515| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008809| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007888| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008633| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007878| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006947| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007057| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006851| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007123| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007199| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006932| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006818| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006838| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007212| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006808| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006752| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006655| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006637| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006836| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006911| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007086| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007012| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006985| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006891| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006631| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007070| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006887| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007405| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[65/100] | loss train:0.006729| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006507| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006886| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006712| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006997| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006991| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006812| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006979| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006967| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006749| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.006917| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006924| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5450 from 2001-08-02 to 2023-03-31\n",
      "BG training\n",
      "Epoch[1/100] | loss train:0.064401| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018798| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016494| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015595| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015795| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014743| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014083| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014059| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013667| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013285| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014904| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013673| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013133| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015680| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013621| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012511| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013641| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013148| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013365| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011928| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013360| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012864| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011671| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011760| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013405| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012456| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013070| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012825| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012548| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012082| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012916| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011711| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012225| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CHRW training\n",
      "Epoch[1/100] | loss train:0.063823| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016871| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013488| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012527| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012670| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012548| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010819| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011338| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011106| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010684| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011251| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010096| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011386| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010287| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010911| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010859| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009802| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010877| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010871| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011549| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009529| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009538| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010326| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009835| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010421| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009536| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010068| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009994| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009481| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010321| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010077| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009545| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009354| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010016| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009629| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008965| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009407| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009103| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009667| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009492| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008145| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008065| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008082| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007459| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008186| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007733| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007789| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007829| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007872| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007578| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007486| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008032| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007509| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007865| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CDNS training\n",
      "Epoch[1/100] | loss train:0.077266| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021690| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014643| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013593| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012314| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011930| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014384| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010382| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011314| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010849| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014079| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013016| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010682| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009842| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011263| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011237| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010247| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010428| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011043| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011951| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010729| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010327| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011108| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2147 from 2014-09-22 to 2023-03-31\n",
      "CZR training\n",
      "Epoch[1/100] | loss train:0.056458| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012902| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010106| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012775| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011457| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009678| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007749| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005770| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008105| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009216| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011113| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006043| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005854| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.022335| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010692| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010035| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005586| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005692| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007435| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010964| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008486| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006564| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005198| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009025| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009044| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005937| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007245| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005130| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007430| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005730| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005625| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007275| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006831| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008214| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009046| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006680| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006354| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005914| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CPT training\n",
      "Epoch[1/100] | loss train:0.070375| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015028| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013995| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011446| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011410| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012429| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011344| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011476| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009912| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011302| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012066| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010040| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009845| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010353| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009671| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009192| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010527| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009819| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009766| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008980| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010088| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010366| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009156| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009174| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011358| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009540| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008768| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009356| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010118| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009055| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010262| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010142| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009427| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009457| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009455| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009476| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009834| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CPB training\n",
      "Epoch[1/100] | loss train:0.065216| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014589| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012418| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012632| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012083| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012251| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011103| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011270| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010662| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012557| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010673| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010294| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010274| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009379| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011159| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010655| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010441| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010794| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010590| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010186| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010750| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010210| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009957| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "COF training\n",
      "Epoch[1/100] | loss train:0.071052| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021930| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019265| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015878| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.018125| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015406| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015012| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.017046| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013592| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015359| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013382| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014845| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013463| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014236| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013586| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013724| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013831| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014901| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.015032| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013709| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014295| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CAH training\n",
      "Epoch[1/100] | loss train:0.065164| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018292| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016767| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014400| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015305| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012598| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015466| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012320| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014077| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012807| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013452| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013233| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013055| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012317| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011962| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013079| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012087| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011465| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013283| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012048| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012602| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012525| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012012| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012727| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010886| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012181| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011137| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011581| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010732| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011059| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011540| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012538| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011470| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011179| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011104| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011583| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011147| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012007| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011974| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "KMX training\n",
      "Epoch[1/100] | loss train:0.078120| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014687| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016199| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014125| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011233| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012378| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012046| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014912| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010761| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011897| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011636| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010826| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011474| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010118| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010308| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012284| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011285| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012509| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010840| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010876| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011117| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010008| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010117| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011629| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010642| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009766| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010537| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010498| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011328| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010124| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009997| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010547| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010204| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010472| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009832| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CCL training\n",
      "Epoch[1/100] | loss train:0.065603| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017850| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017557| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014428| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014783| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013092| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013664| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012607| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013568| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013379| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014514| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012830| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013269| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012535| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013983| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012913| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013190| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012554| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012743| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012701| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013095| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014003| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013331| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012078| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012885| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012203| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012568| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012266| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012783| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013101| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013398| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013056| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011964| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012592| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012485| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012419| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012497| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012185| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013032| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.012143| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010978| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.010663| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.010055| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.010305| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.010653| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010087| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.010694| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010039| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.010467| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009888| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009715| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009987| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.010362| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.010290| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.010099| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.010100| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.010360| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.010036| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.010180| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010099| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.010233| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 765 from 2020-03-19 to 2023-03-31\n",
      "CARR training\n",
      "Epoch[1/100] | loss train:0.038997| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008869| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.005367| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.004408| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.003871| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004018| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.003623| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.003235| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.003413| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.003132| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.002828| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.002704| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.003279| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.002564| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.002446| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.002829| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.002732| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.002618| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.002408| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.002956| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.002147| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.002722| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.002593| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.002558| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.002817| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.002764| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.003080| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.002749| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.002311| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.002729| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.002400| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2182 from 2014-07-31 to 2023-03-30\n",
      "CTLT training\n",
      "Epoch[1/100] | loss train:0.063215| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009040| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007221| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006707| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006090| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006562| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007561| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004409| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004428| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004618| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005510| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005572| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004546| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004453| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005015| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004468| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004656| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005438| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CAT training\n",
      "Epoch[1/100] | loss train:0.052547| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014014| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013126| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016328| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014014| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011481| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012345| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.016016| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012758| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011127| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010802| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011631| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010244| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010254| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011153| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010959| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011203| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010276| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010049| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010847| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011086| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010644| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009898| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011099| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010979| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010366| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009742| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010343| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010379| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010155| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010177| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010221| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010919| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010931| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008700| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008062| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008034| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007680| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007390| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007938| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007657| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007905| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007447| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007412| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007663| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007830| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[53/100] | loss train:0.007827| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007640| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007817| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3222 from 2010-06-15 to 2023-03-31\n",
      "CBOE training\n",
      "Epoch[1/100] | loss train:0.037587| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009072| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008699| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007383| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005877| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006251| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005839| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005863| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005137| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005426| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005662| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005477| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005431| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005497| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007041| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005786| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005665| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005377| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005746| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4734 from 2004-06-10 to 2023-03-30\n",
      "CBRE training\n",
      "Epoch[1/100] | loss train:0.073848| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013295| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013213| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012917| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010540| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011216| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010559| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011661| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009323| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009994| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010237| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008173| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008749| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010152| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009388| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008828| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009119| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008810| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008271| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008828| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008591| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010172| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2458 from 2013-06-27 to 2023-03-31\n",
      "CDW training\n",
      "Epoch[1/100] | loss train:0.051040| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.007447| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006434| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006115| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005612| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006066| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004538| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004652| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004762| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005316| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004482| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004867| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005218| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004228| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.003984| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004131| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004158| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004338| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003940| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004527| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004121| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004453| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004855| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003799| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.003944| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004240| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004211| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004095| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004751| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004252| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003883| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005321| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004325| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003911| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4580 from 2005-01-21 to 2023-03-31\n",
      "CE training\n",
      "Epoch[1/100] | loss train:0.059014| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013926| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018652| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015157| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011189| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010154| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009113| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008533| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012178| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009118| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011222| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.018422| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013918| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011357| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008072| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008623| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009616| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008974| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008648| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007742| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008823| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007948| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008056| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009082| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008604| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008629| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011357| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008739| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008437| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012608| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5361 from 2001-12-13 to 2023-03-31\n",
      "CNC training\n",
      "Epoch[1/100] | loss train:0.066123| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015793| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011684| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011901| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011070| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010807| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010250| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010337| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009986| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009533| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011279| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010295| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008702| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010025| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009227| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009696| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009064| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008839| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008792| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008943| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009339| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007883| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009784| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008288| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008493| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008456| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009174| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008835| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008813| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008248| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008785| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CNP training\n",
      "Epoch[1/100] | loss train:0.067909| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013616| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014939| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012270| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011381| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010220| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010777| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009638| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010755| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009362| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011340| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009451| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010686| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/100] | loss train:0.011164| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010031| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009155| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010593| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009615| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010446| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009528| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009569| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009598| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010347| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008674| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008798| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009482| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008999| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009480| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009852| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008641| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009783| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009180| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008716| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009678| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008225| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007869| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007548| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007520| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007791| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007373| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007412| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007424| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007506| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007784| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007824| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007729| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007363| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007458| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007884| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007400| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008085| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007364| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007817| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007479| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007546| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007532| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007287| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007641| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007507| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007699| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007492| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007446| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007545| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007275| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007649| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007861| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007532| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.007488| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007337| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007700| lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007831| lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007307| lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007527| lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007319| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 1242 from 2018-04-26 to 2023-03-31\n",
      "CDAY training\n",
      "Epoch[1/100] | loss train:0.065666| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.007640| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006286| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006077| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005293| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005305| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005160| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006148| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005123| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004755| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005210| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004593| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004385| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004539| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004587| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004812| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004765| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004726| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005136| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004397| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004330| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004356| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004200| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004717| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004173| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004331| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004220| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004254| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004569| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005106| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003984| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004059| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003871| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003958| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004215| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003927| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004151| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.004173| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.005186| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.003920| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003789| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003497| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.003728| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003449| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.003683| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003774| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.003364| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.003609| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.003410| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.003402| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.003283| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.003496| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.003327| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.003712| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.003439| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.003303| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.003382| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.003468| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.003551| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.003294| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.003565| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4440 from 2005-08-11 to 2023-03-31\n",
      "CF training\n",
      "Epoch[1/100] | loss train:0.070858| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015972| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013552| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012478| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011667| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011471| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011486| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014374| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012540| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010947| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010538| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012377| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011035| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010873| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010122| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010598| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011361| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009924| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010248| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009982| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010113| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009488| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010306| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010472| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010574| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010560| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010760| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010283| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010856| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010235| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009550| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5729 from 2000-06-23 to 2023-03-31\n",
      "CRL training\n",
      "Epoch[1/100] | loss train:0.092458| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016895| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014564| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013597| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.045475| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.017323| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.015291| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.017557| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014779| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013916| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.017184| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.033541| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.043233| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014961| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SCHW training\n",
      "Epoch[1/100] | loss train:0.062880| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016655| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016211| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013340| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015535| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014209| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013793| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012947| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012547| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012393| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012861| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012430| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011048| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012036| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012236| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011723| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012292| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012632| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011415| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011794| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011948| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011867| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013342| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3333 from 2010-01-05 to 2023-03-31\n",
      "CHTR training\n",
      "Epoch[1/100] | loss train:0.077512| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010035| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008964| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006685| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006533| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006282| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008516| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005711| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006929| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006956| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005825| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005476| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006244| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005271| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005919| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006363| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006230| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005747| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006039| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006127| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006592| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006110| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005173| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005013| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005992| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005474| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006594| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005978| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005202| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005480| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006234| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005628| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004745| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004988| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005102| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005330| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005755| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005445| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.005227| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005365| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.004699| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.004681| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004559| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004206| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004230| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004039| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.004157| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004246| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004123| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004284| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004752| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004473| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.004219| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004232| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004089| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004001| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004326| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004400| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004395| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.004594| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.004155| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.004199| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.004358| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.004430| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.004450| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.004337| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CVX training\n",
      "Epoch[1/100] | loss train:0.070117| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019525| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015391| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015382| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013725| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013667| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012629| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012815| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012611| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012901| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011036| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013389| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011454| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012667| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011820| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010496| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011858| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012360| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012519| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010408| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009958| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013148| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011986| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012120| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010712| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010433| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010451| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011598| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011525| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011835| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010718| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4325 from 2006-01-26 to 2023-03-31\n",
      "CMG training\n",
      "Epoch[1/100] | loss train:0.062434| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014090| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010743| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009830| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010861| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013165| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010906| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007958| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012922| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008596| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008985| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008408| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009786| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008523| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014114| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008027| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011407| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007952| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008663| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007581| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007922| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009854| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008087| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012580| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010049| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007739| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010586| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010144| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007840| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CB training\n",
      "Epoch[1/100] | loss train:0.073548| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014225| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/100] | loss train:0.012366| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011663| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013775| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011074| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010090| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011291| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010926| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010403| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010944| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011212| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009954| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009301| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009105| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009723| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009582| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009494| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010018| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009412| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010123| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009981| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009395| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008962| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009259| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008982| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009591| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009258| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008725| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008786| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009379| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009082| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009343| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008705| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009272| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007388| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007347| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007377| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007680| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007752| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007222| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007669| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007069| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007289| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007095| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007401| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006889| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006809| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007054| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007212| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007192| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007097| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007149| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006790| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007049| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007195| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007038| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007027| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007033| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007454| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007215| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007082| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006815| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007228| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CHD training\n",
      "Epoch[1/100] | loss train:0.066759| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014184| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011603| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011433| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009932| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010686| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010200| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010839| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009581| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010738| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011290| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008996| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008949| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009925| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009363| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009563| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009144| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010187| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009427| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008401| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009012| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009424| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008611| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009081| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009346| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008529| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008721| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008794| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008437| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009188| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CI training\n",
      "Epoch[1/100] | loss train:0.055621| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014412| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013661| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011279| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011032| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011126| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010853| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012410| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011044| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012244| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011292| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010378| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010043| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011157| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010259| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012390| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010662| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009534| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009586| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009412| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009964| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009482| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010048| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008717| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010100| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009097| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010463| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008957| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009914| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009667| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010137| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008276| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009538| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009909| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009428| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009410| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009622| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008654| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007786| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007472| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007716| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007006| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007670| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007540| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007625| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007412| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007241| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007491| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007901| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007438| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007697| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007112| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CINF training\n",
      "Epoch[1/100] | loss train:0.076849| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014661| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012032| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013637| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011696| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012741| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010728| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011331| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010743| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011498| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010236| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/100] | loss train:0.010614| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010674| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010651| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010800| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009075| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009424| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009675| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010797| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010471| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010159| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009673| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009952| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010194| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009446| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CTAS training\n",
      "Epoch[1/100] | loss train:0.065457| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015087| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016540| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012849| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011703| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011710| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012609| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009994| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011699| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010452| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009609| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010576| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009224| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013731| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009833| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010266| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009205| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009348| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009021| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009579| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007576| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009746| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009579| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009707| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009106| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009259| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009417| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011080| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009064| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009955| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CSCO training\n",
      "Epoch[1/100] | loss train:0.067930| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019890| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015781| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013574| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013002| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012073| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012306| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014247| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012238| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011038| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013149| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011740| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011733| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011315| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011613| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012391| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011331| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012523| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011904| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011290| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011053| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010788| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011846| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010790| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011522| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011275| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010862| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011300| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010577| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011598| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010860| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010581| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011430| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010569| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010402| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010767| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011918| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009311| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009316| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008938| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009115| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009284| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008917| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009195| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009724| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008708| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009018| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008930| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008862| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008573| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009123| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009201| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009450| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009005| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009105| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009012| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009012| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008898| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009002| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.009084| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "C training\n",
      "Epoch[1/100] | loss train:0.080846| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014668| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011764| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010684| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010762| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011322| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009192| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009715| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009335| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009789| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009388| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008630| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008886| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009373| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008907| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009099| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008866| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008713| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008976| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008277| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009060| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008397| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008941| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008561| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008608| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008947| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008670| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008299| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008250| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008189| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008578| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008057| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008740| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008546| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008776| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007892| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008523| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007897| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008872| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007089| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007093| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006965| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007157| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007131| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007145| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007325| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007069| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007063| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006958| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006965| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006908| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007236| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007031| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[55/100] | loss train:0.006995| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006941| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007050| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006879| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006911| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007044| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007021| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007014| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007221| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006800| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007058| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006804| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006965| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007108| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007011| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007041| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007039| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006937| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007128| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006995| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2145 from 2014-09-24 to 2023-03-31\n",
      "CFG training\n",
      "Epoch[1/100] | loss train:0.078514| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.046737| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019063| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009331| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011582| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010914| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010809| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013223| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011067| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014232| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008719| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011053| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008294| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013425| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011852| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014698| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008396| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006619| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006778| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012132| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011419| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010604| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008659| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006920| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010696| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008192| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009690| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CLX training\n",
      "Epoch[1/100] | loss train:0.056166| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014221| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011847| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011035| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010913| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010077| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012155| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009488| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009978| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010208| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010492| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010182| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010190| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009318| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010809| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009235| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009425| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010092| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009445| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009713| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010320| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008679| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009472| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009307| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009267| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009840| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008641| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009167| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008389| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009399| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008032| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008615| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009718| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008620| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008419| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008399| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008260| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007421| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007357| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006913| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007380| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006643| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007066| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007125| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006786| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006990| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006858| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006841| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006998| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007075| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007025| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006986| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5114 from 2002-12-06 to 2023-03-31\n",
      "CME training\n",
      "Epoch[1/100] | loss train:0.071197| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012863| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009906| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008502| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010994| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009548| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009348| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008404| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008964| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008058| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008038| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009233| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008595| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008108| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008205| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008974| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009040| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008621| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008920| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CMS training\n",
      "Epoch[1/100] | loss train:0.044196| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012590| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010947| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012141| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009126| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010013| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009905| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008757| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009068| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008882| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009307| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008780| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009323| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009823| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009187| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008498| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009203| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008877| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008883| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008882| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009025| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008705| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008078| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008487| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008689| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008613| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008169| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008842| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008107| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008536| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008647| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009127| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007942| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008682| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007934| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[38/100] | loss train:0.008125| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008430| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008222| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007287| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007215| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006632| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006809| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007074| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006667| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006535| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007177| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006815| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007020| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006928| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007125| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006728| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006762| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006637| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006838| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006754| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "KO training\n",
      "Epoch[1/100] | loss train:0.075544| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012578| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011607| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011202| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010697| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009875| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010605| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009205| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010595| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010302| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008905| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010627| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009031| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009945| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008567| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009105| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009576| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008905| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010030| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008612| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008551| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009070| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009615| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008464| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008328| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009105| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008825| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008120| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010144| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009837| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008837| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008141| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009027| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008176| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008907| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008679| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008189| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008831| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CTSH training\n",
      "Epoch[1/100] | loss train:0.061569| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013802| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011690| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011574| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010565| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011676| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009581| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010461| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009404| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009607| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010089| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009188| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010734| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009143| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009569| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009320| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008894| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009385| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008726| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008468| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009700| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009298| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009579| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008436| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008893| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008150| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008774| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008844| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009331| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009093| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008707| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008521| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008401| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009009| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009234| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CL training\n",
      "Epoch[1/100] | loss train:0.059732| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012693| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010265| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010636| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010028| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009453| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009240| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009458| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008427| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009698| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009672| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009009| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009617| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009782| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008272| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008348| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008856| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008477| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008484| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008718| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008400| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008328| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008900| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008397| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008855| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CMCSA training\n",
      "Epoch[1/100] | loss train:0.050228| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013093| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011666| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011774| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010011| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011091| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009327| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009389| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009933| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009530| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009447| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010312| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010241| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009727| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009458| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009503| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CMA training\n",
      "Epoch[1/100] | loss train:0.074591| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015888| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016722| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015530| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014336| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014163| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015122| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014307| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013013| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014320| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014246| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012955| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013341| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012719| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012090| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013506| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/100] | loss train:0.013030| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012661| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012038| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011603| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012290| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012489| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012468| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012004| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011551| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011780| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012536| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012301| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012942| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011324| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012430| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012259| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012569| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012582| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011694| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011805| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012200| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011609| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012731| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011912| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CAG training\n",
      "Epoch[1/100] | loss train:0.038485| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013294| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012161| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010426| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010214| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010438| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009732| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009759| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009762| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009199| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010075| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009153| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009918| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008980| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009082| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009858| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009029| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009735| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009810| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008889| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009296| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009026| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009234| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009366| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008400| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009653| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009413| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008789| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008946| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009217| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009528| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008765| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008325| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009189| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009033| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007994| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007784| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007533| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007410| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007577| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007470| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007325| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007683| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007312| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007489| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007403| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007620| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007189| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007457| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007415| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007445| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007771| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007483| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007479| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007484| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007354| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007244| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007620| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "COP training\n",
      "Epoch[1/100] | loss train:0.059852| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021626| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016010| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015522| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014383| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014475| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013167| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012798| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013990| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012771| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012873| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015408| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012278| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015162| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012800| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014117| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013599| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012886| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012799| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013174| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012064| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013473| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013289| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012852| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012334| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012450| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012514| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012192| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012377| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010822| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012105| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011285| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011786| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011854| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011708| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011555| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010581| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011162| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011766| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009665| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009489| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008619| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008718| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008813| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009148| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008462| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009826| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008834| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009005| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008467| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008616| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008242| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008214| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009163| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008588| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008147| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008880| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008671| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008351| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008518| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008221| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007985| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008577| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.009165| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007877| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008953| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.008330| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008576| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.008091| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008653| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.008086| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.008282| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.009324| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.008518| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.008843| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ED training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.083903| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016489| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013244| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010528| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010992| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010028| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011029| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010723| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009710| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009119| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010254| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010083| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009675| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010530| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009272| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009053| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009177| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008919| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009125| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009255| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009226| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009913| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009203| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009131| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009489| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009729| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009193| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "STZ training\n",
      "Epoch[1/100] | loss train:0.054429| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013464| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011597| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009916| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010269| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010211| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009124| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008796| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009729| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008834| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008873| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009214| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008930| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008685| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009466| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008069| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008742| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008396| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008863| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009082| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008435| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008933| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008403| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007985| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008385| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008372| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008817| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007905| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008273| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008417| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008323| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007729| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008413| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008035| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007618| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008023| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007433| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008335| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008003| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006970| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006848| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006349| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006579| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006724| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006515| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006965| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006571| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006750| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006814| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006665| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006669| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006562| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 293 from 2012-03-12 to 2023-03-31\n",
      "CEG training\n",
      "Epoch[1/100] | loss train:0.048888| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016450| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011550| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008496| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006137| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006416| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007342| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011336| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008127| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006352| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006535| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.003109| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009370| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004490| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.003705| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006554| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.002017| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006103| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005374| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.002979| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003922| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004490| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.001998| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005792| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004908| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003004| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004005| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.002795| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004695| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003155| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.004569| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004096| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010558| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "COO training\n",
      "Epoch[1/100] | loss train:0.047934| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012379| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010807| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010444| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010950| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009416| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010126| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010316| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009183| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010575| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009487| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009470| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010065| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010165| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009607| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008550| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011367| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009611| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009838| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008892| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008657| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008228| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009441| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008920| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008391| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009384| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008978| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008668| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010063| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008928| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008063| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008444| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008868| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008520| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008663| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008625| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009340| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007695| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007159| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006928| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007407| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006759| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007013| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006572| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006752| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007046| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[50/100] | loss train:0.006661| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006887| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006731| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006974| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007224| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006847| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007356| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006805| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CPRT training\n",
      "Epoch[1/100] | loss train:0.066901| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014937| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012671| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012082| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012536| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009970| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010959| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012060| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010794| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008814| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011747| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010352| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011421| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010127| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009758| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011605| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010814| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009198| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008983| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009068| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GLW training\n",
      "Epoch[1/100] | loss train:0.124616| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.027336| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.020266| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.020029| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.019067| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.018351| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.019265| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.020649| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.017405| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016045| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.016892| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.018095| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015528| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.019899| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.016406| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.016180| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.016089| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.015644| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014956| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.019142| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.016432| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.015260| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014396| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.015426| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014828| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.018425| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.017314| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.016677| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.020509| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.015582| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.016062| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.015642| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.015931| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 971 from 2019-05-24 to 2023-03-31\n",
      "CTVA training\n",
      "Epoch[1/100] | loss train:0.039509| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.006311| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.005398| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.004352| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.003849| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.003394| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.003292| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.002890| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.002603| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.002973| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.002770| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.003074| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.003285| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.002455| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.002857| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.002616| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.002602| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.002903| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003114| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.002641| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.002861| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.002596| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.002678| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.002273| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.002850| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.002129| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.002403| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.002835| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004212| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.002396| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.002358| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.002357| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003064| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.002725| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.002887| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.002720| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CSGP training\n",
      "Epoch[1/100] | loss train:0.063168| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017031| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014535| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013167| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012110| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012351| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010700| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011526| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010072| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010612| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011079| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010427| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012652| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011016| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010439| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010315| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010398| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010955| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010014| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009524| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009670| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009401| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009901| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010843| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009472| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009366| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009277| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010173| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010021| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009979| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008508| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009347| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010014| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009267| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009422| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008939| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008733| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008145| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009378| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008010| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007483| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007284| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007253| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007403| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007498| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006966| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007606| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007297| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007504| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007634| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007310| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007577| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007332| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007037| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007029| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007183| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "COST training\n",
      "Epoch[1/100] | loss train:0.058941| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014930| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016566| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014852| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014720| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012318| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.010583| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010274| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010777| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012588| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010120| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011187| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012033| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009883| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011607| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009992| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010185| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010832| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009686| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011742| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010157| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011614| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010178| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009408| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008943| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010952| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010173| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010659| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009484| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010261| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009823| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009685| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009364| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009037| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CTRA training\n",
      "Epoch[1/100] | loss train:0.079741| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017331| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013934| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012705| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012508| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013372| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012035| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011791| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010928| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010681| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010558| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011311| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011033| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010553| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011418| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010767| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010812| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010579| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009810| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010234| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010247| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011607| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010657| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010270| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009777| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010578| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009849| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011156| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009997| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009933| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010211| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010576| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010409| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009991| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CCI training\n",
      "Epoch[1/100] | loss train:0.075483| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013729| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014229| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013113| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012285| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011235| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010653| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010372| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010485| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011269| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009598| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009385| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009535| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009941| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009163| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010465| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010157| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010197| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009626| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009155| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008915| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009315| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009012| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009017| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009091| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008580| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009318| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008343| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009380| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009018| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008373| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009064| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009205| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008591| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009426| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008420| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008913| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009028| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CSX training\n",
      "Epoch[1/100] | loss train:0.072528| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014053| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013115| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012928| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010781| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011443| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012053| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009863| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010238| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010384| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012391| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010882| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010724| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009712| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010042| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009921| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009400| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010398| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009645| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010406| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009993| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010491| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009344| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009308| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009566| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009421| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009005| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009908| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009093| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008628| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008626| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009385| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008983| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008728| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009478| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008544| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007573| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007689| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007355| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007269| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007229| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007173| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007551| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007581| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007962| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007118| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007337| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007021| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007276| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007403| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006893| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007337| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007706| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007665| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007270| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[61/100] | loss train:0.007438| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007175| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007890| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007288| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007394| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007096| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CMI training\n",
      "Epoch[1/100] | loss train:0.068446| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013305| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012926| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013827| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010506| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010306| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011499| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009297| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010092| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010024| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011044| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009343| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009312| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010247| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009633| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009375| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010096| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "CVS training\n",
      "Epoch[1/100] | loss train:0.060683| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012720| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011899| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010507| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010049| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009798| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009889| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010008| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009571| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009503| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008669| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009854| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009188| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008452| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009128| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010270| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008721| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009129| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009157| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009100| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008581| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008638| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008766| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DHI training\n",
      "Epoch[1/100] | loss train:0.066629| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015983| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015633| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014016| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011365| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014082| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012504| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011603| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012108| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011894| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011276| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011590| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013829| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013355| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010269| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012383| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010519| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011307| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010474| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010343| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010343| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010511| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011752| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010945| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011198| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DHR training\n",
      "Epoch[1/100] | loss train:0.072543| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016798| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013657| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012040| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013026| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012840| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010988| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012346| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010148| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011346| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011488| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010197| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010104| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012570| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010700| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010159| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010613| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009684| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009842| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009762| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009122| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009857| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009407| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010311| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011269| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008875| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009216| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009237| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010546| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009894| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010219| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010280| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010069| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009018| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010297| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008688| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008719| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010376| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009683| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008212| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008339| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006721| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007543| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007361| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007722| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007750| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007316| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007227| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007611| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007963| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007511| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007604| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DRI training\n",
      "Epoch[1/100] | loss train:0.056435| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015922| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012819| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011245| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012067| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010106| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010716| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011344| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011566| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010607| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010667| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011534| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010549| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009998| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010397| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009739| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009359| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009639| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010249| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010242| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010477| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009616| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010434| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010130| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010133| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009630| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009627| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DVA training\n",
      "Epoch[1/100] | loss train:0.065761| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016284| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013075| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011818| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012421| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6/100] | loss train:0.011147| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011884| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010802| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011072| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011671| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010681| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011461| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011150| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011334| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009882| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011185| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009985| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010240| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010417| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010059| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009995| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011331| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010515| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010013| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DE training\n",
      "Epoch[1/100] | loss train:0.073633| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017577| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014458| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014482| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013456| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011352| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013060| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012682| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012932| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014238| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011884| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011052| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010848| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009961| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010415| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011885| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011619| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011428| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014202| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014029| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010844| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011378| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011490| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011077| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4007 from 2007-05-03 to 2023-03-31\n",
      "DAL training\n",
      "Epoch[1/100] | loss train:0.062717| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012244| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011438| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010150| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010868| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009607| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009541| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010250| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009889| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010010| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009179| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008590| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008561| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007784| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007912| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009139| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008233| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008461| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008341| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008065| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008958| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007575| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008990| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008073| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008412| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008223| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007821| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008272| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007176| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009439| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008551| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008409| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008016| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007653| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007974| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008177| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006951| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009191| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008931| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006653| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007352| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006808| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006793| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006753| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006627| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006311| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007286| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006256| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006138| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006748| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006590| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006969| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006874| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006561| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006694| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006323| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006872| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006693| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006213| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "XRAY training\n",
      "Epoch[1/100] | loss train:0.059907| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015159| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014092| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013049| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013180| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011875| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011195| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011491| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011743| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011194| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011601| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011163| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011891| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010186| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010939| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010373| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010414| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010461| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009805| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010091| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010696| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009937| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010983| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011136| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010714| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010433| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010505| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010938| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010994| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DVN training\n",
      "Epoch[1/100] | loss train:0.068826| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018887| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017496| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017506| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.017855| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.017694| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015390| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015157| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014427| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015548| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015412| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014174| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015623| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013927| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015177| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014464| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014482| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013949| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.016070| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014093| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.015100| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013595| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013958| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013580| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013818| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014149| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013856| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013122| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.015262| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[30/100] | loss train:0.013828| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013438| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012663| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013161| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012405| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.015008| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.013371| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012638| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.013531| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013697| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.013265| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.011642| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.011180| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.011751| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.011209| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.010665| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.011313| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.010963| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.011366| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.010817| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.010981| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.011403| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.011321| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.011144| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.010983| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.011245| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4523 from 2005-04-14 to 2023-03-31\n",
      "DXCM training\n",
      "Epoch[1/100] | loss train:0.083757| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017929| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015076| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012336| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012201| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013620| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011090| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010639| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012019| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009557| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010422| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011196| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011646| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011039| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009937| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009800| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009239| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010483| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008991| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008274| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009500| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008754| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008989| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010102| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009544| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009965| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011408| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009109| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008041| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009472| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008879| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008996| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009007| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007752| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009623| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009199| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007547| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006430| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006679| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007021| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007039| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007475| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007034| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007312| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006978| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006801| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006194| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006937| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006659| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006121| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006757| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006235| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007037| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006641| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006666| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007240| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007055| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006624| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006493| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006587| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2633 from 2012-10-12 to 2023-03-31\n",
      "FANG training\n",
      "Epoch[1/100] | loss train:0.070647| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013206| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010977| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009958| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009381| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008648| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008108| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008261| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007437| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007870| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007899| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008036| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008988| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007775| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007393| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008391| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007124| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008175| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007929| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007294| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007435| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007125| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006868| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006594| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007878| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006740| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007348| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007063| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006882| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007725| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006462| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007127| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006687| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006741| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007770| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007348| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006758| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007312| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006973| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006401| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006402| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006021| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006216| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005854| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006356| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005414| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005842| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005741| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006189| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005649| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006135| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005643| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005918| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005548| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005775| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005908| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4637 from 2004-10-29 to 2023-03-31\n",
      "DLR training\n",
      "Epoch[1/100] | loss train:0.081575| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012517| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010773| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011271| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009486| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009581| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008808| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008926| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008628| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009455| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007356| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008016| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008155| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007446| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007327| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/100] | loss train:0.008193| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007788| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007667| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007011| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007772| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007647| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006932| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007887| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007396| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007544| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007216| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007927| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007194| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007333| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006936| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007144| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006835| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007804| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007593| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007213| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006992| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007060| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007708| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007118| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006082| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006396| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006024| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005749| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005870| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.005820| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005904| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005910| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.005864| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005871| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.005851| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005638| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005815| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005573| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.005728| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.005907| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.005909| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.005633| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.005584| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005582| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006018| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.005895| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006474| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.005679| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3978 from 2007-06-14 to 2023-03-31\n",
      "DFS training\n",
      "Epoch[1/100] | loss train:0.062660| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012965| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011326| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009486| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009464| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008129| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007445| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010059| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007657| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008525| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008718| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008926| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007969| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008828| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007818| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007426| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008045| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007488| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007575| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008258| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008114| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008052| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007721| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007092| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007031| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007437| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006695| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007571| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007359| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008252| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007889| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007907| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006866| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007850| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007227| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007222| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007205| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DISH training\n",
      "Epoch[1/100] | loss train:0.050098| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019365| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017191| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015567| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015622| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.017251| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014513| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012826| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014192| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014217| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013830| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015156| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014658| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013505| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014815| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014305| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013905| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014117| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DIS training\n",
      "Epoch[1/100] | loss train:0.074978| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015393| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012301| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013680| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010924| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011344| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010221| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011296| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009911| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009881| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010022| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010804| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010531| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009528| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011427| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010880| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010798| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010367| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010181| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011370| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009725| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010268| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009273| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009683| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009877| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009929| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008824| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009715| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009588| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010591| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008888| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009533| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009197| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009709| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009874| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009254| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3367 from 2009-11-13 to 2023-03-31\n",
      "DG training\n",
      "Epoch[1/100] | loss train:0.055434| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010092| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010183| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008448| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006857| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007224| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007377| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007165| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006048| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006742| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006231| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006990| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007864| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005639| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006436| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006618| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/100] | loss train:0.005984| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006719| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006503| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005876| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007873| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005862| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005214| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005685| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006371| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005829| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005964| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005422| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007211| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005073| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005420| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005914| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005679| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005383| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005682| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007610| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.005906| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006146| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005274| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.004969| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.004632| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.004700| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.005398| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004410| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005424| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.005516| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.004060| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.004388| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004279| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.004082| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005654| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.004383| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.004362| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.004343| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.004508| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.004804| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.004779| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DLTR training\n",
      "Epoch[1/100] | loss train:0.076040| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016402| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015077| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012251| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011645| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013428| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011377| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011325| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011971| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011292| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010579| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011398| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011364| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012082| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010221| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009879| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010625| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011059| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009918| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009486| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009807| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010613| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009764| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011774| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009455| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010672| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008905| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009205| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009791| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010264| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010625| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009538| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009587| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009213| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009622| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009119| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010098| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "D training\n",
      "Epoch[1/100] | loss train:0.054282| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014117| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013201| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011005| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009786| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011439| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010474| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008933| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009777| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009369| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009605| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010431| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009304| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009171| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009487| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008695| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009029| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008701| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009012| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008736| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008579| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008740| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009484| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008827| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008913| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008725| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009157| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008530| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008892| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008651| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008418| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009389| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008639| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008758| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008366| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008204| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008753| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008530| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007377| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007052| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007053| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007069| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006921| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006885| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006895| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007174| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007073| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007157| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007563| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006860| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006934| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006863| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006890| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007111| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006959| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006801| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007112| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006878| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007065| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006765| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006900| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007130| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007021| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007107| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006963| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007127| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007402| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007101| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006922| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007011| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4714 from 2004-07-13 to 2023-03-31\n",
      "DPZ training\n",
      "Epoch[1/100] | loss train:0.064806| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011572| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010004| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011282| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008591| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010255| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008085| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008895| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007511| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007513| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008279| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007925| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/100] | loss train:0.009637| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007557| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007265| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007992| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008625| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008354| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008110| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007294| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008344| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007008| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008785| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007674| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008107| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007957| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007256| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007497| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007775| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008858| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007855| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007765| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DOV training\n",
      "Epoch[1/100] | loss train:0.061691| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013891| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014683| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010737| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010402| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010069| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011676| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010088| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014225| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012037| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012067| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010913| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009439| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010047| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009410| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010190| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010133| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009076| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011029| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010439| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010249| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010547| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010136| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010248| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010103| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009265| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009591| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011008| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1017 from 2019-03-20 to 2023-03-31\n",
      "DOW training\n",
      "Epoch[1/100] | loss train:0.057456| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011493| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008121| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006816| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005988| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007008| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006957| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006329| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005947| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005793| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005568| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005143| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005226| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006437| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006241| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005947| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004805| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006154| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006868| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007232| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006551| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004995| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005269| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004943| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004995| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005903| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005573| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DTE training\n",
      "Epoch[1/100] | loss train:0.066177| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012664| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012100| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011790| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009568| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009976| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010104| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010272| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008273| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009600| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008979| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009020| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008757| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009057| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008886| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007938| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008848| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009102| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008832| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008345| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008394| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008769| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008181| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008832| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007889| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008646| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008351| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007721| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008849| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008624| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008837| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008277| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008088| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007752| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007780| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008449| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008747| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008139| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DUK training\n",
      "Epoch[1/100] | loss train:0.064569| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015206| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013688| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011497| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012050| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011160| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010703| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011092| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010758| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009605| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009989| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009479| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010488| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010400| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009900| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009830| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009445| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010233| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009183| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009170| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009695| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008734| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009054| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009722| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010113| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009739| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009137| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009309| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009168| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009482| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008413| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009000| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008886| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007843| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007589| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007516| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007422| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007234| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007527| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/100] | loss train:0.007235| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007282| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007184| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007729| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007820| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007149| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007433| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007167| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007555| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006988| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007136| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007125| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007697| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007299| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007739| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007290| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007360| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007364| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007341| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007452| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DD training\n",
      "Epoch[1/100] | loss train:0.065742| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017155| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015250| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015651| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011686| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014044| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012534| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011532| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012389| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012449| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012509| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012856| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012624| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011371| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011314| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012679| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011505| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011790| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012972| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011192| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011729| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011736| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010842| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010204| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011697| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010915| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012341| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010517| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011363| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010655| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010703| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011138| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010898| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010872| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1552 from 2017-02-01 to 2023-03-31\n",
      "DXC training\n",
      "Epoch[1/100] | loss train:0.045658| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.007462| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.005345| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005120| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.004529| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004615| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004212| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.003834| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.003905| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.003452| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.003744| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004640| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.003591| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.003659| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.003997| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.003278| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004123| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003360| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004020| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003583| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003240| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003370| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.003553| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003336| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.003722| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.002904| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.003340| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.003212| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.003107| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003436| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003142| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.003644| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003576| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003792| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.003111| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003247| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EMN training\n",
      "Epoch[1/100] | loss train:0.055626| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015324| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011262| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012868| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011240| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010539| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011001| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011053| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009069| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010120| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009560| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010523| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009721| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010606| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010124| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009639| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009747| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009045| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010120| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009111| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009392| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009308| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009364| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008825| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009535| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009732| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009843| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008645| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009387| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009394| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009371| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008960| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009551| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010226| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008997| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007863| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007899| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007926| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007582| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007644| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007278| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007579| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007598| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007510| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007413| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007290| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007419| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007039| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007246| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007209| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007377| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007721| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007713| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007380| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007539| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007515| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007193| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007812| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ETN training\n",
      "Epoch[1/100] | loss train:0.076233| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017519| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014159| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013537| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013406| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011157| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010883| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012011| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010233| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10/100] | loss train:0.011930| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011647| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011480| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011984| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009847| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011261| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011368| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010785| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009969| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010032| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011883| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010714| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011322| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009310| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011990| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009850| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009403| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009628| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009967| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010341| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010374| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009562| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009893| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009892| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EBAY training\n",
      "Epoch[1/100] | loss train:0.087186| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016611| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015104| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015943| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014161| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014350| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013976| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012056| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014316| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013274| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014303| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012297| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012171| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011853| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012850| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012513| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012093| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011304| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012555| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010711| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012308| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011715| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011179| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010972| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013113| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011144| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011382| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010396| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010983| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011575| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011897| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011831| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010747| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010922| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011873| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010623| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011205| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ECL training\n",
      "Epoch[1/100] | loss train:0.061208| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015287| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014360| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011375| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012527| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009779| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009628| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009927| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010233| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011342| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009913| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009068| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009597| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009646| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010777| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009634| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008980| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010066| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009927| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009791| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008963| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009477| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008823| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009074| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009175| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009480| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008867| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008860| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009071| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009155| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008811| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010023| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008444| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008988| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009084| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008571| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008286| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007350| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007371| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007065| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007176| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007002| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007380| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007485| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007100| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007480| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006872| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007252| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007246| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007101| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007139| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007377| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007048| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007032| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007305| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007298| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007695| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EIX training\n",
      "Epoch[1/100] | loss train:0.060989| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013298| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012613| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011738| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010718| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010418| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010514| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010211| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009941| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010117| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009375| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009591| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009287| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009416| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009696| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009121| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009986| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009748| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009356| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009658| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009503| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009367| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009300| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008657| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009220| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009372| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009595| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008583| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009278| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009078| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009558| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008794| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009235| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008751| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[40/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008075| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007701| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007417| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007657| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007403| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007619| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007601| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007590| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007023| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007128| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007679| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007452| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007675| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007340| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007552| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007408| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007420| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007451| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007683| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5791 from 2000-03-27 to 2023-03-31\n",
      "EW training\n",
      "Epoch[1/100] | loss train:0.279519| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.024324| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.024922| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013131| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011967| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016098| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011566| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.027711| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011798| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013062| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.058560| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.016710| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.037436| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014215| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011655| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010495| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011900| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010462| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.035482| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014428| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011935| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009854| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014068| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011380| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.016755| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.021999| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011063| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.030735| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010745| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009301| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.078852| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013638| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.025132| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011739| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010502| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009309| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.031736| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.014831| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013774| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010114| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EA training\n",
      "Epoch[1/100] | loss train:0.067112| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012532| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011895| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010885| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010764| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010531| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011133| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010073| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010276| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009795| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009658| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010710| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009788| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009759| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010490| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009952| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009566| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009608| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009120| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009877| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009821| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008898| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009847| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009193| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009970| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008926| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009543| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009848| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009539| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009418| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009804| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009272| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5392 from 2001-10-30 to 2023-03-31\n",
      "ELV training\n",
      "Epoch[1/100] | loss train:0.074444| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015280| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011192| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012811| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013543| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012561| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010883| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008696| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010516| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011640| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011110| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009406| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012201| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007971| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009688| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009200| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008716| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009898| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008587| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010887| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009680| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009933| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010198| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LLY training\n",
      "Epoch[1/100] | loss train:0.082167| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018739| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.020452| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014840| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013285| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014488| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010992| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013357| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010973| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013582| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010655| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011767| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015094| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013227| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012532| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011808| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011272| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.015426| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010798| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012050| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EMR training\n",
      "Epoch[1/100] | loss train:0.066191| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014249| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014581| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012594| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012596| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012632| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012103| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011200| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012044| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010630| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011526| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010089| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010929| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012017| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012144| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011125| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011016| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010831| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011094| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010018| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/100] | loss train:0.010183| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010156| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010350| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009982| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010261| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010741| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010452| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011623| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009582| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009993| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012017| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010118| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010716| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009851| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010199| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009451| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009263| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010079| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010339| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010591| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009272| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008326| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007801| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008225| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008526| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007972| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008529| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008332| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008343| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008392| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008044| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007645| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007945| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007733| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008152| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008043| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007812| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008344| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008468| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007649| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008467| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008141| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2769 from 2012-03-30 to 2023-03-31\n",
      "ENPH training\n",
      "Epoch[1/100] | loss train:0.044312| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008580| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007523| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009024| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008779| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006902| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008101| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007004| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007745| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006320| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006037| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007980| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005439| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007090| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006441| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007789| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006712| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007494| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006429| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006241| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006344| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007695| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006438| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ETR training\n",
      "Epoch[1/100] | loss train:0.066085| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014901| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015106| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012286| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011951| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011352| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014656| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010636| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011556| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012244| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011103| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010995| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010538| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011160| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010466| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010480| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010682| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010977| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010924| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010395| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010158| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010049| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011366| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010822| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010242| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009118| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010216| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010311| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009743| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010067| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010515| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009801| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009255| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009677| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009985| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EOG training\n",
      "Epoch[1/100] | loss train:0.064574| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016004| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015788| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013288| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012467| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011883| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012728| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011023| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010613| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011273| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011845| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011298| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012456| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010788| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012026| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010969| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011022| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011137| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012183| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2805 from 2012-02-08 to 2023-03-31\n",
      "EPAM training\n",
      "Epoch[1/100] | loss train:0.061930| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010043| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009617| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009439| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007504| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007589| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009588| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006202| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008017| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005929| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006611| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006503| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005867| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006437| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007486| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006684| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007217| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007394| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005872| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007111| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005873| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005672| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008882| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006096| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006758| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006187| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007389| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006132| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005383| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005613| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005942| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006438| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006583| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006491| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006588| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005849| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005647| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.006884| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.006522| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EQT training\n",
      "Epoch[1/100] | loss train:0.067958| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2/100] | loss train:0.016217| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015467| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013227| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014586| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013231| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013133| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013576| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012591| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013414| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013602| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013801| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012640| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012230| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012776| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011214| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012987| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012023| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011455| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011353| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011798| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011981| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011467| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013360| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011577| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012380| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EFX training\n",
      "Epoch[1/100] | loss train:0.057548| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014986| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012312| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012185| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011310| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013636| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013926| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011450| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011134| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011555| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010544| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010579| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010716| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009895| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010607| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010064| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009721| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010333| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010653| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008633| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009932| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010437| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010649| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010546| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009296| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010832| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008813| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010044| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009234| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009618| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5695 from 2000-08-11 to 2023-03-31\n",
      "EQIX training\n",
      "Epoch[1/100] | loss train:0.059201| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013412| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011439| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013015| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010458| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011551| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011406| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010118| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011419| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010772| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009061| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009269| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009628| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009783| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009372| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009406| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008743| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009004| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010221| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008413| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009229| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009685| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010371| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008817| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008894| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008912| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008399| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008116| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009251| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008197| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009242| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007650| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008538| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008329| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008682| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008023| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007333| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007436| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006919| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006931| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006818| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007038| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006958| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006746| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006645| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006694| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006885| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006723| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006698| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007045| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006814| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006816| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006916| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006952| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006781| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EQR training\n",
      "Epoch[1/100] | loss train:0.076262| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015587| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016308| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011019| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012479| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012235| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010694| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011337| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010868| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012487| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009469| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011179| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010292| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010187| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010507| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009543| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011190| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009693| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009212| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010501| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009364| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009236| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009566| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009121| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009949| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008912| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009353| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009208| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008676| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009746| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009246| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009599| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008653| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008633| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009500| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008155| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007313| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007741| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007327| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007816| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008200| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007454| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007514| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007754| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[50/100] | loss train:0.007737| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007888| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007537| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ESS training\n",
      "Epoch[1/100] | loss train:0.059576| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015497| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012042| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011169| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010735| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010219| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009803| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010025| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009515| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010417| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009541| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009778| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009755| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008694| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009434| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009887| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009046| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008925| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010264| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008661| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010087| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008256| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008666| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009289| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008717| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009406| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009660| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009163| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008927| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008672| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009654| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008775| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EL training\n",
      "Epoch[1/100] | loss train:0.076495| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016295| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012773| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012640| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011215| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012036| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012058| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011463| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010890| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011435| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010128| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010450| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011830| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009743| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009980| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010734| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009120| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010491| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009405| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010148| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009540| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008962| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009613| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011376| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009515| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009806| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008996| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009473| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009806| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009720| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009598| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009952| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2005 from 2015-04-16 to 2023-03-31\n",
      "ETSY training\n",
      "Epoch[1/100] | loss train:0.054952| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011907| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007791| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006511| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005685| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005895| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005870| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005388| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005295| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005817| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004661| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007695| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005303| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005198| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004922| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006433| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005218| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005437| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004654| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005238| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005166| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005721| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004705| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004693| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005590| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005467| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005130| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006032| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005635| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RE training\n",
      "Epoch[1/100] | loss train:0.071097| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015054| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014143| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013150| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012344| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010111| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010870| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010133| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012829| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010201| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010728| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010039| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011513| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009718| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009678| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009935| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008248| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010233| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008970| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009228| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010102| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009214| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009371| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009589| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008807| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009774| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009473| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1216 from 2018-06-04 to 2023-03-31\n",
      "EVRG training\n",
      "Epoch[1/100] | loss train:0.067313| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016038| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012048| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010047| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012341| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010918| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009376| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010560| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010332| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009254| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008931| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009409| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009103| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008852| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009524| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011733| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009722| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009059| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008970| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009333| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008893| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008801| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009688| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010712| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009282| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009079| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008716| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008206| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008819| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008196| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007762| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/100] | loss train:0.008786| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008037| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008418| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008163| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007664| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008880| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008537| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006998| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007074| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006967| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007036| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006686| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007212| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006532| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006658| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007086| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006946| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006589| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006405| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006673| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006366| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006721| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006601| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006436| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006831| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.005949| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006711| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006610| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006228| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006124| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006068| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006870| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006412| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006080| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006076| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007155| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ES training\n",
      "Epoch[1/100] | loss train:0.065067| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011872| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012146| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010682| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010056| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012052| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009582| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012260| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009942| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008907| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009865| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008893| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008415| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008705| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008961| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008376| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009399| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008570| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008773| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008648| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008702| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008531| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008276| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008911| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009117| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008856| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008587| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008456| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008896| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008544| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008600| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008353| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008381| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EXC training\n",
      "Epoch[1/100] | loss train:0.051214| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015559| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014633| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015456| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015666| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013873| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012292| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014157| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012321| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011213| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011867| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011523| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012596| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011712| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011739| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012444| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011779| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012712| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010915| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011129| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011478| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012654| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011174| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012622| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011637| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010978| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011109| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010639| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012225| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011174| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011079| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011637| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010604| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011407| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011039| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012164| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009884| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010848| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010042| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010341| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009096| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009016| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008920| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008270| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008954| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008555| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008380| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008409| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008009| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008475| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008427| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008465| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008358| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008372| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007920| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008811| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008410| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008564| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008684| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008455| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008655| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008527| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008127| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008544| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008504| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4455 from 2005-07-21 to 2023-03-31\n",
      "EXPE training\n",
      "Epoch[1/100] | loss train:0.049338| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012542| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010446| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011235| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010136| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010636| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010314| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010368| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009744| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010634| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010981| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010009| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008610| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009086| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009579| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008497| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009449| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009059| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008438| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011179| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009391| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010174| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008416| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008883| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009107| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008327| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007897| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[28/100] | loss train:0.008693| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009796| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010187| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010312| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007923| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008709| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008195| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009485| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010303| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008735| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "EXPD training\n",
      "Epoch[1/100] | loss train:0.075630| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016979| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014960| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016481| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014184| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014659| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011847| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014576| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012471| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010714| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012799| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011596| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011552| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012283| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010767| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011191| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010075| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011323| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010534| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011281| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010288| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012148| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010292| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010155| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009626| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010419| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011185| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010147| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011973| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010673| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010249| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010453| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010459| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009836| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010431| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4690 from 2004-08-16 to 2023-03-31\n",
      "EXR training\n",
      "Epoch[1/100] | loss train:0.072893| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011953| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012882| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010655| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009723| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008536| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008045| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009973| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008598| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009889| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008251| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007642| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008256| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007377| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009162| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009096| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008127| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008142| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008663| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007725| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007551| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008009| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008131| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007409| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "XOM training\n",
      "Epoch[1/100] | loss train:0.076284| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018689| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014125| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013787| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012191| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014520| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014526| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013623| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012623| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015726| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012279| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013336| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014828| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010601| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013315| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010631| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012085| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011466| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012480| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011298| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012687| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011718| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011578| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012231| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FFIV training\n",
      "Epoch[1/100] | loss train:0.053062| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014128| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015065| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011048| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010864| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010866| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012287| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012338| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010123| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011124| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011086| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011267| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010706| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009744| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010975| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009846| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011147| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009650| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010393| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010052| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010583| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009673| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009405| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010551| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009640| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010343| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009927| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009959| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010839| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009323| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010608| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009797| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010030| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009650| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010077| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010567| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009543| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010028| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009252| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010018| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008562| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008394| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007970| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008020| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007998| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007877| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008128| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007716| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007914| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008054| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008082| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008059| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007708| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008126| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008061| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008215| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008119| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007971| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007812| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007649| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008303| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008051| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008127| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008167| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008035| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008020| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007949| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007953| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[69/100] | loss train:0.008136| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007891| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FDS training\n",
      "Epoch[1/100] | loss train:0.068024| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018013| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011877| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012820| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010860| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013166| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011614| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010505| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010491| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011129| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010413| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009796| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010245| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009408| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009577| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010321| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011017| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009649| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010926| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009999| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009765| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010700| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009587| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009765| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FICO training\n",
      "Epoch[1/100] | loss train:0.086595| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017589| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016777| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013629| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014225| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013498| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013563| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011583| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011838| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012093| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010339| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011055| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011528| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011986| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011285| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009666| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009460| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012265| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009361| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009236| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010659| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011620| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009723| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010900| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010195| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011119| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009038| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009285| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010490| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010351| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009280| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009189| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009429| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009921| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008761| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009544| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008823| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008056| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007307| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007460| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007625| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007323| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007745| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007706| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007989| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007294| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007710| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007532| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007842| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007196| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007766| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007176| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007464| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007208| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007451| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007284| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007181| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007853| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006901| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007396| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007808| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007676| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007752| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006976| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007484| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007038| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007110| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007562| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007791| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FAST training\n",
      "Epoch[1/100] | loss train:0.051408| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015389| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012541| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011646| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011212| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011765| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011240| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010777| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011334| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010661| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010922| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009627| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010604| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009929| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009943| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010066| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009188| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010958| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008881| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009749| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010056| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010002| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009265| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010494| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009909| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008784| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009129| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009529| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010207| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009570| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009808| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008507| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009639| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009021| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009530| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010234| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008740| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009018| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010076| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008671| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007420| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007361| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007323| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007484| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007137| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007442| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007464| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007623| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007418| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007373| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007291| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006901| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007461| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006776| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007157| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007284| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007205| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006977| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007151| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007254| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006877| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007259| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[63/100] | loss train:0.007305| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007405| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FRT training\n",
      "Epoch[1/100] | loss train:0.059616| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014574| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012121| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011156| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011387| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010721| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011433| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009625| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010773| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009570| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010024| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009824| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008787| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010155| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009757| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009976| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009031| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009709| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009750| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010016| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009151| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009223| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008899| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FDX training\n",
      "Epoch[1/100] | loss train:0.060522| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013767| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012531| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012913| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013212| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010918| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012629| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012154| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011557| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011188| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010824| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009918| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011119| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011597| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010325| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011750| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011343| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010444| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011314| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012472| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010373| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010132| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FITB training\n",
      "Epoch[1/100] | loss train:0.066259| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019595| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015836| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015760| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015065| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013189| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014332| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013295| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015298| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013314| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013551| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013149| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013101| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012757| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013115| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013273| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012025| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012978| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012133| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012269| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013974| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012295| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012659| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011706| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011573| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012340| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011943| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011934| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012162| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011691| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011979| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013415| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012347| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011638| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012572| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3098 from 2010-12-09 to 2023-03-31\n",
      "FRC training\n",
      "Epoch[1/100] | loss train:0.062309| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012587| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011222| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008466| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009422| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007585| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006185| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008660| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006754| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006179| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008408| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006920| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006399| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006268| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007381| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007300| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006878| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006029| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008897| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006101| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008098| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006170| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006286| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006327| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006270| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007289| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005879| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006457| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007060| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006041| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008307| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006004| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007056| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006094| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006036| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006487| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005914| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4119 from 2006-11-17 to 2023-03-31\n",
      "FSLR training\n",
      "Epoch[1/100] | loss train:0.090373| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017929| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016834| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015577| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016476| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016315| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015133| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014160| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012159| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014965| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014730| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015606| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012324| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012392| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013530| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013857| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013126| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011602| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013413| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012962| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013462| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013232| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014425| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011901| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013175| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010818| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012535| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012443| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012267| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.014047| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012767| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013408| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012965| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012696| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012498| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.013119| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FE training\n",
      "Epoch[1/100] | loss train:0.059914| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017125| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015277| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/100] | loss train:0.014674| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013145| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013541| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013799| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012896| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012927| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012761| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013100| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013238| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013779| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012539| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013237| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012957| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013210| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012568| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012343| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012352| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011731| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012064| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012191| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012528| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012521| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011738| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011640| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011810| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011587| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012580| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012193| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012105| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012235| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011628| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011442| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011529| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011847| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012543| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011231| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010075| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009924| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009921| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009867| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009964| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009731| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009660| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009219| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009877| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009559| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009522| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009091| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009443| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009667| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009432| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009318| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009690| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009753| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009355| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009693| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.009562| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009527| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5480 from 2001-06-20 to 2023-03-31\n",
      "FIS training\n",
      "Epoch[1/100] | loss train:0.066463| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014108| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012800| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011144| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012046| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010595| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011915| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009962| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010022| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012247| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009942| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009865| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009144| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010047| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009368| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009806| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009638| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009761| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008832| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008769| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010102| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009115| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008896| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009690| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010649| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009755| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010136| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FISV training\n",
      "Epoch[1/100] | loss train:0.074192| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012457| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010259| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011074| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011299| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010110| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009857| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009527| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009115| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009206| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009833| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009570| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009302| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009274| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008230| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008633| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009600| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009270| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008260| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009255| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009206| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008386| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009293| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009524| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008255| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3094 from 2010-12-15 to 2023-03-31\n",
      "FLT training\n",
      "Epoch[1/100] | loss train:0.059635| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011021| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008584| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010665| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007691| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007804| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006887| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006582| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006130| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006651| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006938| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006909| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006469| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006217| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006929| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006299| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006779| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006793| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006071| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006512| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006571| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006199| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005917| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005613| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006219| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005716| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006483| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005896| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006051| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006042| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006269| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006037| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005796| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005784| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FMC training\n",
      "Epoch[1/100] | loss train:0.060980| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013292| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013565| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011414| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011097| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009869| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009557| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010544| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009359| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010442| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009686| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/100] | loss train:0.009893| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010888| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009329| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009924| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010108| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009654| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009462| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009552| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009717| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009374| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009459| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009096| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009122| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008780| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010396| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008877| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009579| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008509| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008143| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009123| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010134| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008183| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009250| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008604| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010729| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009353| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008320| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009778| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008630| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "F training\n",
      "Epoch[1/100] | loss train:0.079259| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.022275| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019318| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.020372| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.019517| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.019017| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.018818| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.019592| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016377| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016246| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.016717| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014974| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015921| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015922| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014218| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014924| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.017416| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014935| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.016396| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014651| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.015702| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014513| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014794| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013937| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.015540| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014693| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.015146| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.016123| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014851| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.014946| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.014626| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.014094| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.014056| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.015453| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3364 from 2009-11-18 to 2023-03-31\n",
      "FTNT training\n",
      "Epoch[1/100] | loss train:0.068985| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011344| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008793| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006813| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013602| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008068| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006517| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006358| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008559| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007300| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015733| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010614| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007289| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007327| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006323| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010622| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007270| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006609| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010138| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014440| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006299| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007358| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011957| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006765| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007353| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009952| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008393| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006560| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007617| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005981| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008199| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008804| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007226| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006115| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009526| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006917| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007988| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.005910| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008143| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.005474| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.005082| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.005021| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.005295| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.004515| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.004626| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.005356| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.004885| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008174| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.005093| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.004629| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.005858| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.005174| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.005005| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006326| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 1699 from 2016-07-01 to 2023-03-31\n",
      "FTV training\n",
      "Epoch[1/100] | loss train:0.052261| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013400| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013570| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011855| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.022808| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016021| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.017494| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011901| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011976| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014424| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010508| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009367| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.019675| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.019640| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013610| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.015192| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.016344| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009349| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009542| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008526| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011279| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011635| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010917| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010448| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009002| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011012| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009134| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009715| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011238| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.015969| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1023 from 2019-03-12 to 2023-03-31\n",
      "FOXA training\n",
      "Epoch[1/100] | loss train:0.046464| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014773| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011546| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009311| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010740| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008891| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009561| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009274| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008597| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008729| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008555| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008069| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009271| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[14/100] | loss train:0.009113| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008466| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008372| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007131| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007787| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007932| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008034| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008394| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007896| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009402| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008349| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009067| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009868| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008556| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1022 from 2019-03-13 to 2023-03-31\n",
      "FOX training\n",
      "Epoch[1/100] | loss train:0.054366| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014663| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010206| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011495| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010534| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011199| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010318| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010300| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009814| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009054| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008810| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009207| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009487| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009863| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010926| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009441| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009926| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009613| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "BEN training\n",
      "Epoch[1/100] | loss train:0.067911| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018525| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013886| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012407| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012556| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013573| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011870| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012976| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012586| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010921| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011491| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011984| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011587| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011671| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011499| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011003| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011588| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011477| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011449| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010761| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011313| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011680| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011352| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011232| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010282| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010517| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010148| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010777| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011361| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011061| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010498| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010960| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010832| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010785| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010341| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010900| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010781| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010687| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009649| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009145| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008835| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009047| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009242| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009099| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009186| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009054| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008980| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008848| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009091| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009164| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008982| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "FCX training\n",
      "Epoch[1/100] | loss train:0.066510| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016822| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015283| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015223| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013762| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012749| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013959| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013296| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013986| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013108| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013036| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013159| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012499| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013044| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013131| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013219| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013670| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012934| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012211| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012637| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012994| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012567| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012146| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012645| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012562| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012431| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012208| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011981| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012202| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012607| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012381| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011972| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012073| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011868| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012391| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011470| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011875| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.013117| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012126| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.012841| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.011214| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.010465| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.010903| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.010046| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.010199| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010409| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.010362| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010530| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.010001| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.010390| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.010139| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.010709| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.010242| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.010317| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.010091| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009814| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.010182| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.010506| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.010383| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010020| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.010393| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010364| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.010560| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.010246| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.010407| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.009555| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.010490| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.010040| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.009976| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.010607| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.010595| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.010186| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.010065| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.009982| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.010377| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[76/100] | loss train:0.010553| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5612 from 2000-12-08 to 2023-03-31\n",
      "GRMN training\n",
      "Epoch[1/100] | loss train:0.074154| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017480| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017047| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.018459| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014145| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012855| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011987| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011924| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010592| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010676| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011725| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011491| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010607| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011329| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011232| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010899| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012676| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010374| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009886| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009440| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009147| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010215| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009870| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010214| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010767| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009369| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010548| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010067| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009381| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010803| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009646| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IT training\n",
      "Epoch[1/100] | loss train:0.070713| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014933| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013251| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012884| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012052| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014123| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014051| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012306| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011633| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013370| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010324| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011226| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013411| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011899| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010588| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011666| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010592| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009016| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010777| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011479| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011070| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011690| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009602| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011064| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010227| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010772| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010684| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 61 from 2023-01-04 to 2023-03-31\n",
      "GEHC training\n",
      "Epoch[1/100] | loss train:0.025787| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.005990| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006033| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007712| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.004125| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.002800| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004385| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004106| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004042| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.002821| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.002926| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.002563| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.002571| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.003015| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.002162| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.002841| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.002952| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.002785| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.002375| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.001792| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.002337| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003502| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.002502| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.002858| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.002667| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.002067| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.002230| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.002385| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.002205| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.002339| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GEN training\n",
      "Epoch[1/100] | loss train:0.087872| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016728| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016449| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014243| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012230| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012871| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012232| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011764| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012307| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012140| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010868| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010901| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011968| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012011| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010691| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011577| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011932| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011828| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012337| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011766| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011235| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012422| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011701| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011269| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011388| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3307 from 2010-02-11 to 2023-03-31\n",
      "GNRC training\n",
      "Epoch[1/100] | loss train:0.082361| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011947| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010044| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011521| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010354| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011436| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008980| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009355| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007738| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010241| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007355| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007411| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008042| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009846| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008089| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007567| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006060| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008133| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008581| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007458| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006718| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010447| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007075| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006868| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006585| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006297| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009241| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GD training\n",
      "Epoch[1/100] | loss train:0.053649| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010533| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010421| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010961| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011482| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010165| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009112| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010124| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008954| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009046| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009840| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008950| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008990| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/100] | loss train:0.009045| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008805| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008722| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008497| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008676| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009058| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008502| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008931| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008227| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008838| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008810| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008425| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008992| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008319| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009676| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008735| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008235| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008415| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GE training\n",
      "Epoch[1/100] | loss train:0.060613| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017549| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015787| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013488| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013831| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015129| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013015| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013652| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012718| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015153| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012666| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012680| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012876| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013829| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012741| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011962| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014552| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012407| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012406| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013033| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012876| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013217| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012358| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013207| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012881| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012313| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GIS training\n",
      "Epoch[1/100] | loss train:0.076965| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015295| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012145| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012879| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014822| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010555| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011709| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010532| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010366| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010741| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010111| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010535| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010787| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010995| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009660| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011478| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010060| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009993| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009969| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009305| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009761| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009496| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009274| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009420| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009296| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009935| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010451| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009592| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009772| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009859| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009453| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009553| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010003| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3112 from 2010-11-18 to 2023-03-31\n",
      "GM training\n",
      "Epoch[1/100] | loss train:0.064191| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014968| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012612| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010574| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011782| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011398| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010447| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011791| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011312| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011616| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010103| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011009| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010605| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013809| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010768| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009884| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009135| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012083| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010851| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009441| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010214| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009449| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010610| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013622| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012638| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010634| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GPC training\n",
      "Epoch[1/100] | loss train:0.053346| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015484| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012593| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010595| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012813| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011606| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011920| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010123| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010976| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010826| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011966| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009979| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010215| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008834| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010163| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011312| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010214| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010256| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009138| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010718| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010342| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009999| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010503| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GILD training\n",
      "Epoch[1/100] | loss train:0.067090| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012838| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011092| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011972| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009418| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010061| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010032| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009129| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011701| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010082| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009793| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009934| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009068| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010098| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009055| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009800| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009343| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009031| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008841| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009029| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008753| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008645| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008998| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008648| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/100] | loss train:0.009901| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008529| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009157| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009302| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009577| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008947| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008972| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008402| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008986| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008515| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008571| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007332| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007561| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007399| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007017| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007452| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007083| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007557| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007142| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007157| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007212| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007058| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007741| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007289| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007336| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GL training\n",
      "Epoch[1/100] | loss train:0.079365| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012776| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011482| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012383| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010723| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010499| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009903| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010360| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010200| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010513| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010300| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009663| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009029| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010526| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009520| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009061| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008537| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010437| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009647| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009414| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008750| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009156| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009934| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009895| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008905| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009063| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008659| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5588 from 2001-01-16 to 2023-03-31\n",
      "GPN training\n",
      "Epoch[1/100] | loss train:0.074597| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015525| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012326| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011803| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010935| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011423| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009867| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010918| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010287| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009729| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010822| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010102| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008976| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010328| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009821| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009027| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009151| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008788| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010092| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009261| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009269| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009709| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007940| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009343| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009326| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009078| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008576| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008953| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010389| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009505| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008517| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008477| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008768| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GS training\n",
      "Epoch[1/100] | loss train:0.057629| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015013| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014547| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012583| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014376| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012793| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012940| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011924| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014075| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012631| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011066| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011517| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011837| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014156| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012478| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010511| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012258| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012237| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010280| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011865| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012061| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010441| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010947| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011188| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011985| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010900| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011225| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011021| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010995| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011339| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011159| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011252| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011313| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011737| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010400| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010697| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009447| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009300| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008599| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008784| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008666| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008539| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007945| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008479| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007950| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008909| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008288| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008073| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007817| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008270| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008245| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008382| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008302| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008456| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008458| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008096| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008192| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008058| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008313| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HAL training\n",
      "Epoch[1/100] | loss train:0.072185| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017182| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016035| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015163| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015377| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014895| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013081| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014530| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012827| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013160| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014384| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/100] | loss train:0.015046| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012740| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013592| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013384| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013758| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012152| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012978| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013502| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012862| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012231| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011848| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012916| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013664| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013297| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011827| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012890| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013108| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012582| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013371| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012859| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012484| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013384| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011757| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012535| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012617| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012420| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011509| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013121| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.012177| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.011127| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.010527| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.010237| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.010117| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.010350| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010756| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.010170| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010470| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.010415| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.010358| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.010112| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009926| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009904| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.010357| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009953| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.010341| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.010163| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009702| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.010478| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010024| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.010328| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010334| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.009817| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.010670| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.010673| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.009771| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.010421| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.010534| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HIG training\n",
      "Epoch[1/100] | loss train:0.054034| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016193| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015776| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015532| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014317| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013135| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013221| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014219| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013199| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013781| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012340| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012186| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012768| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012781| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013412| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013002| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012254| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012392| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013049| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012671| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012102| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011829| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012830| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011336| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012057| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012371| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012149| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012302| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011935| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011588| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012985| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011031| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011608| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012703| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012163| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011600| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011751| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012105| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011890| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011457| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010220| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.010170| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.010238| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009324| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009851| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009702| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009657| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009872| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009664| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009967| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009880| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009775| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009698| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009770| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HAS training\n",
      "Epoch[1/100] | loss train:0.069929| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014811| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012396| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011311| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010119| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010871| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010402| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009333| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009473| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010212| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010578| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008948| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010126| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009212| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009996| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009101| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008691| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008669| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009228| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008757| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009346| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009786| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009964| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009259| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008968| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009357| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008389| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008398| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009595| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008708| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008046| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009095| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009364| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009055| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009643| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008784| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008729| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008750| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008762| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007973| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007528| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007433| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007442| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007545| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007540| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007436| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007575| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007719| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007520| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007545| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007332| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007264| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007449| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[55/100] | loss train:0.007473| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007619| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007664| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007379| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007446| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007646| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007627| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007810| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007433| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3036 from 2011-03-10 to 2023-03-31\n",
      "HCA training\n",
      "Epoch[1/100] | loss train:0.053633| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008899| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008185| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008543| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007230| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005949| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007200| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006486| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005963| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006712| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006352| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005922| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006184| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005914| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005215| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005910| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005616| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006057| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005384| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006889| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005283| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005629| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006024| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005993| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004938| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005749| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005541| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006205| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005397| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004708| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006176| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005365| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005226| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005057| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005230| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005099| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005389| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005873| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.005984| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004852| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5891 from 1999-11-01 to 2023-03-31\n",
      "PEAK training\n",
      "Epoch[1/100] | loss train:0.060045| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015175| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013168| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012401| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011599| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011219| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012657| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012004| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012526| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011105| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010991| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010516| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010596| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011131| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010551| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010433| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011481| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010595| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010760| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011456| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011551| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010709| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010079| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010009| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010554| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010192| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010153| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010509| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010299| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010918| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010350| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010985| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009641| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010580| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010182| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010018| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010691| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010208| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010406| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010390| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008995| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008778| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008430| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008730| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008519| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008650| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008557| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008353| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008423| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008611| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008579| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008401| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008490| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008677| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008614| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008474| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008321| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008430| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008491| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008318| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008528| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008365| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008524| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008540| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008926| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008022| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008259| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.008285| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008212| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.008423| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008361| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.008300| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.008519| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.008582| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.008587| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.008398| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HSIC training\n",
      "Epoch[1/100] | loss train:0.062932| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012155| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010440| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010531| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010469| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010887| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010763| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010080| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008989| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009798| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008407| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010128| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008889| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008889| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008927| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008868| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009223| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009025| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HSY training\n",
      "Epoch[1/100] | loss train:0.067145| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016162| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012996| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012813| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010769| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009719| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010973| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012040| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010611| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010929| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010159| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010691| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010957| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011650| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/100] | loss train:0.011339| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HES training\n",
      "Epoch[1/100] | loss train:0.085818| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.024894| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.021216| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017840| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015887| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.021337| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015725| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.016891| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016130| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016569| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014848| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.016505| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014219| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015678| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015545| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.015011| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013545| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.015232| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.015281| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.017583| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.015436| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014242| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.015357| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.014448| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013511| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014968| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013721| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013732| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.013087| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.015451| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.015762| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.015108| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013619| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.013759| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.015158| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012735| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.014077| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.014273| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.014686| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.014982| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.012313| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.011433| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.011564| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.011605| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.011263| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010921| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.011094| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.011499| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.011532| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.011193| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.011292| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.011742| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.010832| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.011804| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.011056| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.011723| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.011227| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.010906| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.010805| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010730| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.011551| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010837| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.011911| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.011403| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.011304| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.010789| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.011458| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.011578| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.010868| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.010847| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 1876 from 2015-10-19 to 2023-03-31\n",
      "HPE training\n",
      "Epoch[1/100] | loss train:0.079623| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014537| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011118| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010021| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009087| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009060| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008160| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008153| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008174| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007614| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008337| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007950| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008047| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008359| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008013| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007834| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007734| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008234| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008192| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008697| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2341 from 2013-12-12 to 2023-03-31\n",
      "HLT training\n",
      "Epoch[1/100] | loss train:0.070272| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012789| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007603| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007779| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007403| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008887| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007577| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006180| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006454| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006382| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006006| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007392| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008539| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006867| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006868| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006759| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005728| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005361| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007678| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005882| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005143| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006834| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005112| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005171| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006587| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005752| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005563| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005337| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006318| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006316| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005831| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007681| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006353| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HOLX training\n",
      "Epoch[1/100] | loss train:0.065361| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014496| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015580| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011572| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010764| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011457| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010978| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011195| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010075| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011106| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009974| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009499| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010242| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010870| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011540| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009473| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009629| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009976| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010299| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009344| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010053| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008953| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009587| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009712| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010288| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009005| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008834| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009590| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010126| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009534| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008893| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009110| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008744| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008708| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009405| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008611| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008080| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007636| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007557| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007096| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007513| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007641| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007352| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007256| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007488| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007386| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007551| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007001| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007285| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007370| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007160| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007421| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007027| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007367| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007040| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006908| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007598| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007191| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006887| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006964| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007475| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007580| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007163| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007453| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007122| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007521| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007316| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007176| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007538| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HD training\n",
      "Epoch[1/100] | loss train:0.069844| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014962| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014714| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011687| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010474| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013349| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013570| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009538| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010124| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009427| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010328| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010665| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009292| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009620| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010417| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010012| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010298| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008851| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009106| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007910| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010310| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008518| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010031| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009293| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008796| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008581| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008990| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008993| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HON training\n",
      "Epoch[1/100] | loss train:0.062270| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013603| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011675| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012498| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010317| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010283| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009128| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009426| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010424| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010362| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009801| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009354| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010517| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008502| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010015| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009186| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008800| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009586| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009228| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009191| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009258| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008823| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009031| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HRL training\n",
      "Epoch[1/100] | loss train:0.067095| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012599| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012403| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009653| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010577| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008816| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009394| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009221| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008603| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008824| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008939| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008599| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009143| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008988| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008292| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009188| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008555| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008141| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008275| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008690| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008568| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008345| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008193| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007747| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007447| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007699| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007815| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008494| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008287| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007859| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008224| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007940| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008137| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008301| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009046| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HST training\n",
      "Epoch[1/100] | loss train:0.066698| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017598| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014943| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014401| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014259| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013468| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014178| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012916| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013116| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014625| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013260| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013323| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013494| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013510| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013821| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013357| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013288| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013551| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HWM training\n",
      "Epoch[1/100] | loss train:0.069815| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018703| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016516| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014791| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016764| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014683| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014876| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013234| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013784| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014267| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014702| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015047| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013920| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013209| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/100] | loss train:0.012956| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013159| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014481| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012685| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013464| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012759| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013177| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013829| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012832| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013070| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012460| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013739| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013618| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013094| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012482| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012804| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013599| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012006| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013434| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012643| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.013348| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012843| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012549| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012944| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012540| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011834| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010768| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.010995| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.011300| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.010870| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.011215| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010473| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.010644| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010706| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.010463| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.010640| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.011052| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.010253| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.010225| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.010636| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.010878| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.010805| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.010775| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.010156| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.010889| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010671| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.010512| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010713| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.010761| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.010810| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.010595| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.010752| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.010576| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.010971| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HPQ training\n",
      "Epoch[1/100] | loss train:0.090340| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020542| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017324| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.020599| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.017131| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014688| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.016659| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.016752| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016744| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016358| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014319| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013153| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013688| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014592| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.016371| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013857| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013772| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.015373| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014882| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013594| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013977| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012888| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.015470| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013329| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.016237| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014288| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.014037| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.014010| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.015262| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013772| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013131| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013228| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HUM training\n",
      "Epoch[1/100] | loss train:0.053366| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014502| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012201| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011959| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010521| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010005| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009193| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010229| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009958| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010128| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009479| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009912| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010012| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010080| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008612| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009377| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008450| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010748| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008531| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010402| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008987| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009075| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009391| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008902| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009091| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009552| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010056| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "HBAN training\n",
      "Epoch[1/100] | loss train:0.051887| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017531| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016309| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014279| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014366| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015565| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013729| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013471| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015427| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013959| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013806| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013551| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012233| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014151| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013751| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013699| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014414| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012466| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012862| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013442| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013255| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012782| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014016| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3028 from 2011-03-22 to 2023-03-31\n",
      "HII training\n",
      "Epoch[1/100] | loss train:0.048908| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008879| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007039| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006518| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006592| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005590| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005966| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006352| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006408| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005605| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005330| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005245| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005208| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004973| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005457| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005327| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005660| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005643| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006131| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004907| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004934| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004741| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005761| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005012| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004987| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005544| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/100] | loss train:0.005202| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005044| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005017| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004915| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005420| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004969| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IBM training\n",
      "Epoch[1/100] | loss train:0.074672| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016193| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013220| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013502| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012120| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012710| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011152| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012553| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012164| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011683| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011744| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011201| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010605| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010423| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011236| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010578| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010619| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010872| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010626| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010618| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010327| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010202| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010512| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011037| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010640| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011134| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010459| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010387| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010756| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011189| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010398| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IEX training\n",
      "Epoch[1/100] | loss train:0.059195| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013163| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012609| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009783| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009444| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010964| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009977| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010279| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008751| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009780| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009303| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009166| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008587| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009534| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009285| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008492| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009233| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009352| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008329| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008339| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008429| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008491| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008278| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008512| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008787| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008271| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008808| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008182| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008283| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008253| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008753| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008093| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007954| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008456| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008190| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008715| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007904| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008158| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006920| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007252| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006271| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006768| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006321| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006917| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006631| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006663| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006465| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006516| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006403| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006556| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006880| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IDXX training\n",
      "Epoch[1/100] | loss train:0.065879| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015635| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011621| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011824| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012516| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010144| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010936| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010438| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010499| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010706| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010963| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010676| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009690| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009979| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010401| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009568| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010674| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010004| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009128| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010786| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009431| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010721| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008612| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009850| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008427| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008681| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009989| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011559| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008797| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010862| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010785| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009420| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009769| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010032| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008870| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ITW training\n",
      "Epoch[1/100] | loss train:0.069432| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013278| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012334| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010713| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011433| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011367| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011527| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009490| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009968| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009361| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009377| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010347| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010746| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009463| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009260| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010325| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008737| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009756| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009586| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009895| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009388| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009023| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009141| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008228| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008743| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008820| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008652| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009411| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008373| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008269| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008235| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008630| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[34/100] | loss train:0.010205| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5705 from 2000-07-28 to 2023-03-31\n",
      "ILMN training\n",
      "Epoch[1/100] | loss train:0.091630| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016153| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012041| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012392| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011382| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011313| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011067| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011914| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011171| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010841| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011393| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011211| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010813| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010716| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011167| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011493| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010570| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010158| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010359| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010291| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010678| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010868| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010332| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011086| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010831| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "INCY training\n",
      "Epoch[1/100] | loss train:0.077538| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016972| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014968| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014178| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013240| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012691| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013898| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012861| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012338| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013000| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014498| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012094| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012911| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012530| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011650| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011885| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012451| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012844| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011722| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011671| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012456| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012331| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011884| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011986| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012463| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011257| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012321| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010788| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012087| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011891| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011362| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011329| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010916| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011694| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011560| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010968| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012075| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011664| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1482 from 2017-05-12 to 2023-03-31\n",
      "IR training\n",
      "Epoch[1/100] | loss train:0.043287| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008723| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006361| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005878| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.004935| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004725| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005081| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004626| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004377| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004343| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004414| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004691| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004330| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004123| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004140| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004278| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004548| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004313| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004446| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004411| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004513| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004290| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004169| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004349| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3999 from 2007-05-15 to 2023-03-31\n",
      "PODD training\n",
      "Epoch[1/100] | loss train:0.060382| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014188| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012199| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010625| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.090515| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011220| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.122634| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015080| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.017964| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.019946| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011490| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.020146| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008481| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008254| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.020475| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012705| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008935| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007201| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007325| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014962| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013372| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008783| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010813| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011727| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007900| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007753| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007237| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.033642| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "INTC training\n",
      "Epoch[1/100] | loss train:0.062508| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020015| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016629| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014872| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014785| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014406| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013567| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014127| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013995| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013057| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015044| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013492| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011904| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014908| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012915| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012837| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012810| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012024| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012715| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013058| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012376| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012111| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012834| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4372 from 2005-11-16 to 2023-03-31\n",
      "ICE training\n",
      "Epoch[1/100] | loss train:0.069072| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011166| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011670| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008939| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009476| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008250| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007823| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008413| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008555| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007986| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007745| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009219| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007174| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007872| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007737| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007055| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/100] | loss train:0.007515| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007900| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007786| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008343| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006911| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006953| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006690| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007235| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007891| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007323| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007336| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007588| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006468| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.007018| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.007718| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007295| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007358| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007246| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007117| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006627| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006568| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007270| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.007222| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IFF training\n",
      "Epoch[1/100] | loss train:0.070058| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013039| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012285| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011346| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010628| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010784| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010355| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010616| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010817| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011243| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009611| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009533| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009463| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009981| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009341| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009433| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009797| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010535| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008983| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009674| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009054| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008909| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009402| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008973| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009270| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009867| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008282| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008685| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008958| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009179| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008704| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008957| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009328| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008896| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009216| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009229| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008746| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IP training\n",
      "Epoch[1/100] | loss train:0.087439| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021179| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015758| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015221| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013510| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013764| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012842| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013197| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014173| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014474| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012472| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013358| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011996| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011891| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012806| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013388| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012748| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013169| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011624| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012647| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012495| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011855| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012217| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010429| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012979| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011118| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011329| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010889| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011965| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011352| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011106| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012131| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010104| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012135| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011752| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011106| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010545| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011851| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011592| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009604| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009307| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009246| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009170| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008869| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009260| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009786| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009684| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009006| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008833| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009262| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009691| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009619| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009432| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009320| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009218| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009176| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009472| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009466| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009383| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IPG training\n",
      "Epoch[1/100] | loss train:0.064715| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016898| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015134| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013615| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013625| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014337| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014001| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011777| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011791| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011991| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011882| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012181| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011832| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012767| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011256| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012131| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011797| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011216| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012609| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010853| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011196| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010438| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011520| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010859| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010225| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011864| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010643| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010784| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011773| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011101| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011232| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010976| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010809| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010903| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010644| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "INTU training\n",
      "Epoch[1/100] | loss train:0.090428| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020933| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015678| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015365| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013729| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012207| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.013912| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011983| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012750| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011925| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012312| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011512| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014245| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011798| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011021| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010844| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010648| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013187| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012895| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012092| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012014| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010750| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012208| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010323| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010908| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011978| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011158| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011544| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009699| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011850| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009012| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011388| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011053| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011958| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008961| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011751| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009479| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010188| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008498| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008383| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008301| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008689| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007732| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008072| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008147| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007163| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007808| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007972| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008441| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007894| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007781| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007712| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007966| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008197| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007514| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007593| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5734 from 2000-06-16 to 2023-03-31\n",
      "ISRG training\n",
      "Epoch[1/100] | loss train:0.071490| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013785| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014849| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010811| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013033| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015916| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014648| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011209| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010107| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010091| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011309| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012419| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011457| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010876| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009970| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IVZ training\n",
      "Epoch[1/100] | loss train:0.060121| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018342| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017549| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017271| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015187| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.017210| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015218| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015906| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014657| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013788| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015050| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013220| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015496| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014274| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014763| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014307| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.015104| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013435| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014607| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013214| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013754| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014234| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013977| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013434| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013617| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014141| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.014440| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013930| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.013313| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013612| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1552 from 2017-02-01 to 2023-03-31\n",
      "INVH training\n",
      "Epoch[1/100] | loss train:0.061683| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.007995| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006013| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005840| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005473| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004464| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005149| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005432| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004488| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004093| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004266| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004269| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004208| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005645| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004327| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004218| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004101| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004633| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003848| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004102| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004260| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004460| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005188| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003930| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004108| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003735| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004962| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004479| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004037| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004201| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.004151| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.003724| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003595| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003546| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.003568| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003545| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004166| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.003927| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004389| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004169| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003769| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003541| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.003402| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003296| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.003417| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003371| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.003145| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.003497| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.003261| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.003362| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.003290| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.003337| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.003290| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.003165| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.003355| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.003072| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.003497| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.003242| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.003324| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.003351| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.003280| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.003042| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.003304| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.003310| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.003369| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.003259| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[67/100] | loss train:0.003311| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.003205| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.003278| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.003335| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.003115| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.003313| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2492 from 2013-05-09 to 2023-03-31\n",
      "IQV training\n",
      "Epoch[1/100] | loss train:0.056321| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009335| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006689| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006908| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006348| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006002| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005267| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005948| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006678| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005129| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005083| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005777| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005164| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004788| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005326| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004494| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004562| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005060| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004489| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004545| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004945| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004813| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004974| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006433| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004852| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005308| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004101| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004690| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004883| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005136| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.004815| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005007| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004305| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004542| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005650| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.004332| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004679| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "IRM training\n",
      "Epoch[1/100] | loss train:0.060215| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015979| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012948| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014071| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013281| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012030| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012012| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010148| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012549| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012313| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011476| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011266| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012267| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010699| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010844| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010861| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009310| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010740| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011542| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012263| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010763| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010605| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010946| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010865| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010062| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010724| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011285| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "JBHT training\n",
      "Epoch[1/100] | loss train:0.055384| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013112| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011519| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012563| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010273| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010847| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011633| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008965| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011033| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010679| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011677| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009771| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008875| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010054| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010533| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010071| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009702| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009634| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009725| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009800| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009326| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009901| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "JKHY training\n",
      "Epoch[1/100] | loss train:0.070026| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015237| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013308| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011119| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011091| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010054| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010395| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011204| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010626| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009308| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009311| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010062| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009169| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009266| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009305| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009111| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009930| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009238| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008972| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009844| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009927| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009506| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008686| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009762| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009173| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008503| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008802| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009476| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008944| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008091| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008569| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008507| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008706| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008911| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008372| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009653| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008758| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007951| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007579| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007217| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008055| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007244| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007347| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007206| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006794| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007101| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006695| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007146| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007562| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007383| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006726| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007308| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006939| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007177| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007108| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007046| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007175| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "J training\n",
      "Epoch[1/100] | loss train:0.073716| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016405| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015986| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014217| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013976| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013163| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012036| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013095| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/100] | loss train:0.014291| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013286| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013800| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013029| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011682| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011126| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012415| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012277| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011603| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011448| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010483| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012089| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011421| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012531| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012090| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011341| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011695| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013599| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010822| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011290| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012341| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "JNJ training\n",
      "Epoch[1/100] | loss train:0.058656| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012604| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011945| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010742| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010670| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010052| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009726| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009509| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010420| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008418| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009265| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008022| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009695| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008250| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008659| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008406| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008701| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008518| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008010| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008113| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008962| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008896| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008251| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008882| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007907| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009499| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008738| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008509| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008391| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008211| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008592| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008254| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007656| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008697| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009245| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008186| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007091| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007067| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006657| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006962| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006821| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006879| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007007| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006337| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006344| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006933| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006578| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006531| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006848| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006857| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006733| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006684| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006406| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007100| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "JCI training\n",
      "Epoch[1/100] | loss train:0.075143| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013260| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013418| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013336| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011260| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014164| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012073| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011914| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011826| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011867| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011011| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009932| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009360| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010727| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010568| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011534| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009878| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010290| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009450| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009353| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010590| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009415| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010142| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010837| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010418| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009992| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009813| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010117| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009419| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009357| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "JPM training\n",
      "Epoch[1/100] | loss train:0.059944| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015809| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012322| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012216| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012194| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012499| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010181| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011176| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012191| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010708| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010799| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011158| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010590| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010017| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010025| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010103| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011002| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009944| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010549| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009728| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009604| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009707| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009891| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010852| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010447| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009531| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010140| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009719| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010314| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010888| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009121| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009834| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010821| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010010| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009506| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009488| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009279| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008887| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010524| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008020| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008219| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007707| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007593| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007508| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007492| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007471| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007573| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007534| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008027| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007866| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007579| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[53/100] | loss train:0.007861| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007119| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007812| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007415| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007753| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007420| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007505| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007274| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007650| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007421| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007585| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007374| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "JNPR training\n",
      "Epoch[1/100] | loss train:0.124309| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.073735| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.029345| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.038731| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.036218| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.033937| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.031347| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.028122| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.036309| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.038301| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.031995| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.033632| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.030016| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.030111| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.033504| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.029544| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.032126| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.026010| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.027670| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.030624| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.034841| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.028733| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.026905| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.026118| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.026188| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.026170| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.030210| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.026138| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "K training\n",
      "Epoch[1/100] | loss train:0.069258| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012513| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011789| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011539| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010776| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013219| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009704| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010194| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009509| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011102| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009747| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009110| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009182| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009744| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010011| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010161| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009691| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009420| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009831| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009053| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009185| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009555| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009078| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009589| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009651| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009712| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009061| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008977| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009427| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008902| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009779| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009074| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009018| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009154| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008657| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008840| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007393| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007751| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007537| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007394| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007534| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007721| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007713| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007533| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007510| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007530| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007538| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3759 from 2008-04-28 to 2023-03-31\n",
      "KDP training\n",
      "Epoch[1/100] | loss train:0.052621| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010354| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009585| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008142| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007796| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007071| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007585| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006952| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007167| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007320| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006795| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007352| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006353| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006115| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005972| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007108| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006803| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005723| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006387| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006341| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006209| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006275| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005873| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006979| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006604| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006928| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006593| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005939| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "KEY training\n",
      "Epoch[1/100] | loss train:0.061013| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016523| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016745| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014943| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013836| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014641| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013011| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013342| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013059| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013242| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013994| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012915| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013194| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013634| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012159| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012617| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012933| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012413| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013064| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012711| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012114| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013122| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012468| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013004| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013023| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012972| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013046| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012904| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012442| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012572| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012377| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2127 from 2014-10-20 to 2023-03-31\n",
      "KEYS training\n",
      "Epoch[1/100] | loss train:0.058644| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008451| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.005788| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005418| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005757| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005303| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004673| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004422| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004714| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004237| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/100] | loss train:0.004117| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004490| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004102| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004412| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004587| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004542| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004268| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003791| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003569| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003874| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004832| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003679| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.003601| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003338| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004175| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003502| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.003408| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.003762| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.003960| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003968| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003213| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004111| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003789| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003568| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004427| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003825| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.003578| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.003457| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.003178| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.003448| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.002896| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003203| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.003103| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003188| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.002889| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.002671| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.002754| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.002832| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.003013| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.002957| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.002743| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.002911| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.002801| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.003093| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.002632| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.002930| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.002863| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.002866| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.003028| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.003070| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.002866| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.002986| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.002981| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.002892| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.002965| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "KMB training\n",
      "Epoch[1/100] | loss train:0.066039| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014484| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011283| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013874| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011569| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009678| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009808| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009126| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009597| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009261| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009677| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008839| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008359| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008671| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009186| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008679| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008877| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008878| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008617| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008482| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008519| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008288| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008533| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008245| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008378| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008028| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009487| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008251| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008374| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008293| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008010| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009122| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008485| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008165| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008063| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007930| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008234| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008067| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009011| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007831| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007427| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006941| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006821| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006548| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006765| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006803| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006815| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006711| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006460| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006708| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006882| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006760| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006858| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006666| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006811| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006668| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006761| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006817| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006934| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5889 from 1999-11-01 to 2023-03-31\n",
      "KIM training\n",
      "Epoch[1/100] | loss train:0.075430| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016859| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014649| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014505| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014598| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014397| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012577| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014602| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013054| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012911| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013478| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013540| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012604| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012365| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012267| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012895| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013847| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012423| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012943| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012569| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012528| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012122| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013189| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011816| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011783| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012393| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012008| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011389| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011892| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012144| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013014| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011857| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012407| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012594| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011876| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011858| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012157| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011460| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3054 from 2011-02-11 to 2023-03-31\n",
      "KMI training\n",
      "Epoch[1/100] | loss train:0.059473| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014353| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010449| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010346| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010199| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010284| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011224| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008655| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008921| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009950| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008306| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/100] | loss train:0.009409| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008411| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010303| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010768| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009064| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009176| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009215| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008731| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009311| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "KLAC training\n",
      "Epoch[1/100] | loss train:0.067014| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017980| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015002| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014813| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014341| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012398| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012418| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013734| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011627| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.017396| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011225| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012492| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011535| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012341| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012806| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011232| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011965| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011921| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011588| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012064| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011425| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1950 from 2015-07-06 to 2023-03-31\n",
      "KHC training\n",
      "Epoch[1/100] | loss train:0.050923| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008456| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006444| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005499| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.004996| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004809| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004452| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004416| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004483| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004568| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004277| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.003776| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004357| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004579| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004035| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004330| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.003928| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003855| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004717| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003881| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004031| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003814| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "KR training\n",
      "Epoch[1/100] | loss train:0.070893| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016074| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014522| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013177| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012284| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013288| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012363| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011331| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011874| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011890| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009154| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011601| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011046| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010787| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011392| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010991| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010442| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010753| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011181| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011557| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010505| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LHX training\n",
      "Epoch[1/100] | loss train:0.066415| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014233| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010486| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011487| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010961| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010564| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011109| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009612| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009410| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011404| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009434| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010334| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010382| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009348| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009888| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009163| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010744| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008964| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009948| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009098| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009554| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008737| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009098| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009045| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009720| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008237| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009487| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009390| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008697| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008675| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009146| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009388| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008855| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008987| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008865| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008359| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LH training\n",
      "Epoch[1/100] | loss train:0.080045| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014431| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015690| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011922| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012314| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010846| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011389| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010470| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011364| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009970| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012723| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009909| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010745| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009862| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009787| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011014| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009777| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010719| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009925| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009997| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009673| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010759| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009247| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010000| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009436| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010161| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009709| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009005| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010021| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009099| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009490| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009260| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009898| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009079| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009171| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008991| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009806| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008933| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007887| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007577| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007037| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007392| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007544| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007674| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007251| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007321| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007310| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007396| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[51/100] | loss train:0.007493| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007487| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007587| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LRCX training\n",
      "Epoch[1/100] | loss train:0.066084| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019078| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015837| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015126| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015342| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013900| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014516| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012863| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011844| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013493| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012031| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012465| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012090| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012463| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011267| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011501| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011563| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010741| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011556| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011248| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010948| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011812| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010076| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013272| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010567| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010369| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013269| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012820| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012052| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011369| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011253| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011387| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009848| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010484| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010577| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010113| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011076| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008659| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010312| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010107| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008268| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009286| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008052| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008309| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008444| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007945| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008415| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007717| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008189| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007871| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008090| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007701| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007852| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007759| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007892| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007537| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007679| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007901| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007984| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007660| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007736| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007662| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007937| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007760| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008134| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007965| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 1607 from 2016-11-10 to 2023-03-31\n",
      "LW training\n",
      "Epoch[1/100] | loss train:0.049456| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009834| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007669| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007369| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006429| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006289| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007313| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005930| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006818| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006199| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007408| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006513| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006638| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006174| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005794| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007048| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005828| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005603| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006594| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005208| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005107| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005314| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005468| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005738| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005885| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004994| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005263| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005343| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005450| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005477| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005721| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005562| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005811| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005643| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006069| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006273| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4605 from 2004-12-15 to 2023-03-31\n",
      "LVS training\n",
      "Epoch[1/100] | loss train:0.078009| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019918| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017234| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017845| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014978| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014395| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013960| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015510| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015134| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013757| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014346| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013370| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013551| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013639| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015191| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013465| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014637| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012953| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.015203| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014850| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013319| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013398| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013124| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.015013| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013510| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012499| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013031| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.014186| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.013109| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012747| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013160| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013500| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013733| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.014227| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.013134| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012293| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012327| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.013465| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.014098| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.013055| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010980| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.011074| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.010915| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.010767| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.010699| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010751| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.010296| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010637| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.010686| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.010195| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.010922| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.010549| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.011063| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.010452| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.010762| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.010170| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.010944| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.011159| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[59/100] | loss train:0.010704| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010814| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.010842| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010409| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.010314| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.010639| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.010104| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.010321| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.010860| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.011064| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.010853| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.010606| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.010162| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.010602| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.010865| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.010463| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.010473| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4144 from 2006-10-13 to 2023-03-31\n",
      "LDOS training\n",
      "Epoch[1/100] | loss train:0.066868| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010796| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009059| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009463| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007978| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008403| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007768| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008905| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008159| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008296| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008613| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007650| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007029| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007706| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007063| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006121| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007078| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007629| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006628| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007642| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006971| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007206| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006767| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006684| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007409| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007472| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LEN training\n",
      "Epoch[1/100] | loss train:0.064077| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020685| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016790| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016024| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015089| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012938| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013335| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013715| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013289| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014023| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013381| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013590| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013760| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012083| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012764| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013034| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012161| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012615| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013729| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011903| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011399| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011904| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011576| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013214| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010685| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013068| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011461| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010917| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011244| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011439| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012286| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011218| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011052| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010950| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011945| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LNC training\n",
      "Epoch[1/100] | loss train:0.088412| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018715| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017890| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016301| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015892| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016436| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015593| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015589| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015450| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014491| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015202| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015269| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013968| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014181| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015000| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014143| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014992| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.015596| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.015422| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.015199| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014994| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014301| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014011| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1133 from 2018-10-01 to 2023-03-31\n",
      "LIN training\n",
      "Epoch[1/100] | loss train:0.041213| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008641| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006398| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005113| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005112| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004774| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004653| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004508| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005584| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.003798| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004499| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.003503| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004936| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004026| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004053| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004110| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.003714| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003551| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003881| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004463| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004558| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003588| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4348 from 2005-12-21 to 2023-03-31\n",
      "LYV training\n",
      "Epoch[1/100] | loss train:0.068898| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012843| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012188| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010178| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009789| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009548| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009958| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010817| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012180| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009952| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009260| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009660| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008413| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009727| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009133| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009158| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010112| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008969| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009320| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008670| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009397| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007698| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008608| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008130| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008669| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008622| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007831| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008366| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008649| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008188| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008567| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4907 from 2003-10-03 to 2023-03-31\n",
      "LKQ training\n",
      "Epoch[1/100] | loss train:0.047825| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2/100] | loss train:0.015524| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013430| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010323| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011037| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010748| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010230| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010104| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009184| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008970| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008762| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008923| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011126| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008769| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009944| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008741| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008599| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007860| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008242| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008118| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008511| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007941| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009404| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008901| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008587| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008545| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008455| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008547| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LMT training\n",
      "Epoch[1/100] | loss train:0.049734| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012612| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011431| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010713| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011234| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009657| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010086| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009972| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010080| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009054| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010464| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008776| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009213| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008974| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009061| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008289| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008518| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009245| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008580| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009510| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008602| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008505| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008180| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009327| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008786| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009639| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008869| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008479| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008446| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008159| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008950| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009614| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009108| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008413| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008844| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007997| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008244| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008051| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007616| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006807| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007203| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006909| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006772| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006918| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007140| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006644| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007120| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006599| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006983| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006785| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006868| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006746| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006465| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006944| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006838| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006473| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006530| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006935| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006960| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006841| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006733| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007191| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006740| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "L training\n",
      "Epoch[1/100] | loss train:0.064335| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016816| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014796| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013275| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014792| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012828| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013052| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012642| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012365| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012710| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011435| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012538| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011896| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011707| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011959| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011828| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011327| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012451| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011912| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010691| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012031| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010700| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011306| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010998| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011799| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010921| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011835| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011418| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011122| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010853| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LOW training\n",
      "Epoch[1/100] | loss train:0.084063| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014299| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014406| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014940| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011696| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011387| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012971| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010895| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013068| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010891| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010923| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011201| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011609| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011068| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012020| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010693| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009786| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010942| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010273| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010349| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009647| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010558| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013584| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009550| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011296| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010454| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009858| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009054| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009466| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010717| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010213| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009238| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008787| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009768| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009159| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009526| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009592| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009297| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009250| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009647| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007566| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[42/100] | loss train:0.007782| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007382| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007504| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007467| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007313| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007447| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007764| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007332| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007113| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007243| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007008| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007377| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007639| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007497| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006962| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007352| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006861| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007264| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007609| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007111| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007458| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007215| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006930| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008093| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007661| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007608| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007244| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3255 from 2010-04-28 to 2023-03-31\n",
      "LYB training\n",
      "Epoch[1/100] | loss train:0.059440| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011946| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010131| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009527| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008581| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007815| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008985| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007491| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009030| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007953| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006939| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007596| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006866| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007089| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007531| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007123| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006685| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006922| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007186| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008074| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006886| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007136| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006870| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006691| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007184| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006752| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006829| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MTB training\n",
      "Epoch[1/100] | loss train:0.077088| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016499| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014822| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014672| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012924| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012444| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011007| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012310| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011587| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010891| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011930| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011580| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011555| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011141| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011025| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011159| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011074| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011767| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010872| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010192| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010802| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010322| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010954| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010989| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010431| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010678| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009878| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011086| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011065| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010830| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010772| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010714| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010392| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010324| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010198| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010270| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010453| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MRO training\n",
      "Epoch[1/100] | loss train:0.066368| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016773| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014442| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016470| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013823| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013542| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013032| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012406| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012524| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012982| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012556| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012214| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011910| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012504| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012859| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011899| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012596| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012077| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011751| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012335| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012099| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011579| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012212| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011746| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012617| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011590| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011767| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012227| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011701| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011271| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012060| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011038| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011676| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011107| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011229| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011509| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011605| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011425| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011523| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010439| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.010199| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009883| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009937| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009868| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009544| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009701| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009763| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009918| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009668| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009716| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009937| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009762| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009254| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009375| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009646| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009627| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009445| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.010125| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010068| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.009840| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009619| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.009750| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.009581| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2963 from 2011-06-23 to 2023-03-31\n",
      "MPC training\n",
      "Epoch[1/100] | loss train:0.062486| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010904| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009849| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009259| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008855| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6/100] | loss train:0.008018| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007140| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009161| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007143| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008360| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007677| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007571| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007307| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007746| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007360| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006414| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008426| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007453| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006081| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009902| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006549| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006249| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006935| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007450| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008537| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006949| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006895| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006945| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006405| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4632 from 2004-11-05 to 2023-03-31\n",
      "MKTX training\n",
      "Epoch[1/100] | loss train:0.069536| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011064| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012760| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009652| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008579| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010419| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008676| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007786| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008754| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008925| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007904| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009202| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007952| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007784| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007278| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008953| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008365| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007572| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009876| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008279| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008204| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007299| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007283| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009121| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008307| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MAR training\n",
      "Epoch[1/100] | loss train:0.067029| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016068| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014273| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012856| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013085| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011072| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011755| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011509| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011322| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011262| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011343| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010956| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010423| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010929| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010340| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010844| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009935| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010357| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010127| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009931| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010555| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010154| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011255| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009561| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009980| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010155| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009871| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010178| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009743| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010294| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010589| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009263| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009859| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MMC training\n",
      "Epoch[1/100] | loss train:0.056563| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014410| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014285| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013648| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010628| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011730| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011599| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011823| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010596| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010582| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010429| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014837| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010363| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010586| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010024| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011062| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009963| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011370| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010960| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010231| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009722| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009613| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010378| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009128| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010826| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008601| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009512| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011611| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008419| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009982| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010323| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009753| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009966| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009559| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009177| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009256| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009708| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007766| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008229| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007373| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007091| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007214| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007226| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007200| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007143| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006756| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007023| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006891| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006959| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007124| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006909| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007039| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007121| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007273| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006960| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006824| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MLM training\n",
      "Epoch[1/100] | loss train:0.073510| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019459| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013314| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015483| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013331| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012324| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011629| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013743| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012102| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013077| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013211| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012296| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011368| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009834| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011474| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011978| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011165| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010016| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010564| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[20/100] | loss train:0.009870| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011070| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010521| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010917| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010762| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MAS training\n",
      "Epoch[1/100] | loss train:0.076969| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017885| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013991| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013304| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014343| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012022| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012065| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013588| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011826| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012319| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011033| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011845| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010540| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011270| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011150| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011249| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010119| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010913| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010694| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010472| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010828| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010899| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011367| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009949| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009940| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010607| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010023| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010897| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010001| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009874| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010253| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010239| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010393| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010365| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010673| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009314| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009609| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010303| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010091| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008639| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007862| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007964| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007674| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008152| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008205| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008555| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007563| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007972| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007793| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007911| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007840| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008101| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007524| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008109| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008060| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008053| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008163| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008532| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007553| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008308| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007707| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008324| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007685| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4242 from 2006-05-25 to 2023-03-31\n",
      "MA training\n",
      "Epoch[1/100] | loss train:0.060961| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009973| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009056| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008177| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008365| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007725| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007307| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007120| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007491| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006078| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006590| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006811| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006198| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006765| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006649| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006679| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006376| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006577| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006191| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006142| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1853 from 2015-11-19 to 2023-03-31\n",
      "MTCH training\n",
      "Epoch[1/100] | loss train:0.059366| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009413| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007742| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006455| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005919| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005434| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005074| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004783| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004594| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004674| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005326| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005690| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005291| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005199| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004389| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.003997| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004946| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004908| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004831| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003818| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004080| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004210| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005483| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004183| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004131| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003919| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004045| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004128| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004915| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004553| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MKC training\n",
      "Epoch[1/100] | loss train:0.052242| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013305| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010330| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010804| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009436| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010254| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010390| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009787| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009394| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010079| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009594| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009728| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008624| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008911| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008442| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008937| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008846| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007947| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008338| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009379| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008680| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008250| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008400| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008468| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008582| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010110| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007917| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008950| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008653| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008642| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008730| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007328| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008237| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008396| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008593| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007994| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009141| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008674| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007113| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007079| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006847| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[44/100] | loss train:0.006460| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006516| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006722| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006441| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006656| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006311| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007012| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006487| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007163| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006781| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006940| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006532| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006538| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006759| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006891| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006768| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MCD training\n",
      "Epoch[1/100] | loss train:0.060520| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015141| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010548| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011020| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010342| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010377| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010028| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008886| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009326| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010286| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009251| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009540| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011088| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008856| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009513| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008515| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008771| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008740| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008094| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009123| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008415| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008623| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008885| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009270| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009032| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008366| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008225| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008417| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MCK training\n",
      "Epoch[1/100] | loss train:0.062012| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014568| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012207| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013384| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011589| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012292| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013763| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012293| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010816| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008604| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010724| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010593| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011666| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010589| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009587| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011191| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009497| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009472| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009953| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008758| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MDT training\n",
      "Epoch[1/100] | loss train:0.073094| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016137| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014286| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013185| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012736| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010837| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010884| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011630| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010436| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011272| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010623| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010353| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010037| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010172| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012448| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009866| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010472| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010206| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010067| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009814| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010313| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010627| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009645| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009895| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009480| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009739| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010566| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009408| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009641| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009505| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009519| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009420| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009703| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009380| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008758| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009899| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008978| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008482| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007997| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007421| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007912| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007894| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007992| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007811| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007865| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007905| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007506| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007408| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007912| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008037| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007929| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007877| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007550| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007929| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007666| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007475| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007655| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007687| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MRK training\n",
      "Epoch[1/100] | loss train:0.060305| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012443| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014788| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011370| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011320| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012006| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012743| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011089| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010621| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010483| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010796| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011014| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010666| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009913| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011125| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010110| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010760| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010753| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010540| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011717| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010157| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009430| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010674| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009433| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009471| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010122| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009608| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010092| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009959| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010709| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009873| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010026| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 2735 from 2012-05-18 to 2023-03-31\n",
      "META training\n",
      "Epoch[1/100] | loss train:0.042510| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010541| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008310| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008884| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006804| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007362| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006913| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006582| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006522| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007806| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007228| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007900| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006429| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006209| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006019| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007564| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006601| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006174| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007872| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006174| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005593| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005926| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005944| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007232| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007863| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006244| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006918| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006010| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006499| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006810| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006249| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5784 from 2000-04-05 to 2023-03-31\n",
      "MET training\n",
      "Epoch[1/100] | loss train:0.089278| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017294| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016225| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015045| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014558| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015126| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012161| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013963| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015630| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012097| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011850| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012345| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012953| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011861| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011734| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013614| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012537| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011718| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012987| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012195| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011992| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012322| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011357| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012178| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010936| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012733| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012832| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011451| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011462| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011834| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011770| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010797| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011387| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010791| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011199| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011159| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010209| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011020| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010961| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010571| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009391| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008885| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008556| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009047| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009203| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008440| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008739| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008940| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008763| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008884| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008947| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008597| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008907| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009119| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008878| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008982| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MTD training\n",
      "Epoch[1/100] | loss train:0.045947| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012808| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013893| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012155| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010817| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010446| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011055| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010499| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010761| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009255| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012260| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009975| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011311| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010682| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010362| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010142| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009402| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009949| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008765| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010044| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009238| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009832| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009560| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008545| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009363| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008797| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010238| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009689| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009368| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008412| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009170| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008770| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009485| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009294| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009104| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008770| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007124| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007183| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007120| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007227| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006599| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006774| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006757| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006485| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006807| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007109| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006820| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006986| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006742| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006688| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006932| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006658| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006796| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007035| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MGM training\n",
      "Epoch[1/100] | loss train:0.084180| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021295| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019691| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016177| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.019033| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015620| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015593| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.017201| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.017915| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.017948| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.016424| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014918| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.016706| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014354| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014951| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013479| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012475| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[18/100] | loss train:0.014008| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014714| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013336| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013916| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.015040| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.015136| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.014395| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013943| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014561| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.015462| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MCHP training\n",
      "Epoch[1/100] | loss train:0.053186| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016102| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013471| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010741| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013085| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012128| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012078| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010248| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012172| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012561| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011955| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009733| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011172| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011202| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009989| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009932| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011579| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010767| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009919| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011165| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010482| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012169| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009443| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010346| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010475| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009829| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009338| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010860| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010276| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009957| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010390| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010462| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010688| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009360| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011896| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009728| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009727| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MU training\n",
      "Epoch[1/100] | loss train:0.062527| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019025| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018147| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015349| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014304| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016212| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014637| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.017216| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014674| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014492| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015775| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013952| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013172| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015104| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013038| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013985| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012681| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014957| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013858| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014334| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014135| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.015192| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013121| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013110| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012942| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014062| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013675| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MSFT training\n",
      "Epoch[1/100] | loss train:0.072170| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016528| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014003| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014354| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015213| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011181| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014741| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010364| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011197| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011432| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011654| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011531| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010051| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011519| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011354| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010207| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012006| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012278| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010610| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010426| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011470| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011110| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010089| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MAA training\n",
      "Epoch[1/100] | loss train:0.057539| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013108| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016693| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011013| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013015| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011500| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012727| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010746| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011427| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010874| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009477| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010866| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010768| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008894| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011178| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010504| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009627| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011160| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009626| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010163| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009879| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009312| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010546| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1086 from 2018-12-07 to 2023-03-31\n",
      "MRNA training\n",
      "Epoch[1/100] | loss train:0.038219| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009938| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007119| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005932| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005172| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006905| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005975| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005958| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004395| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.003983| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004313| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004988| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004378| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004579| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.003546| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.003918| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005256| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004251| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004484| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003762| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003700| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003917| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004017| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004985| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005048| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MHK training\n",
      "Epoch[1/100] | loss train:0.073121| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017238| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014814| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012577| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012165| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011710| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010775| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012146| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012250| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012399| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013044| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011338| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/100] | loss train:0.011553| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010556| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012268| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010662| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012266| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009304| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011237| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012180| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009974| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010272| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011779| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010062| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011604| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010915| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009826| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4972 from 2003-07-02 to 2023-03-31\n",
      "MOH training\n",
      "Epoch[1/100] | loss train:0.065943| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014857| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014631| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011913| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011648| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011801| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013241| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010086| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011095| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009766| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010434| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008747| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009637| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009476| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009370| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012195| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010306| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009228| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010243| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008966| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008803| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008485| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009147| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009048| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009369| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009311| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007843| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008164| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007867| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008147| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009016| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008699| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009774| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008409| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007984| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009619| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TAP training\n",
      "Epoch[1/100] | loss train:0.065319| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015276| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013151| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013464| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013203| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011635| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011695| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012182| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011682| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010895| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010956| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011388| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011565| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011996| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011440| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010971| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011200| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011439| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011742| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010229| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011444| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011005| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010433| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011978| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010685| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010548| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010948| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009711| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010228| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010062| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010032| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010375| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010716| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010575| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010754| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010178| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011114| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010411| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5485 from 2001-06-13 to 2023-03-31\n",
      "MDLZ training\n",
      "Epoch[1/100] | loss train:0.064506| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014160| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010528| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011701| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009235| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009208| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009691| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009872| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009031| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010085| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009435| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008900| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009128| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008519| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009928| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010321| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008016| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008908| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010250| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009228| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010044| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008288| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008371| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008500| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009169| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008237| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008301| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4622 from 2004-11-19 to 2023-03-31\n",
      "MPWR training\n",
      "Epoch[1/100] | loss train:0.063241| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014381| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012018| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011394| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011000| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010290| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011514| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009872| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009965| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009085| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010419| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009334| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009192| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010227| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008026| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007979| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009536| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008300| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008668| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008445| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008711| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009567| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009546| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008771| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009263| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MNST training\n",
      "Epoch[1/100] | loss train:0.062018| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014813| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013770| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011572| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010916| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011625| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010478| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012364| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010485| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011970| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009626| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011102| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009345| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010148| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/100] | loss train:0.009721| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010896| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009166| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009271| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009376| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010834| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009103| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009651| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009170| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009587| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008752| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008627| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009266| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008707| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009319| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008741| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009084| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009034| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009958| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009087| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MCO training\n",
      "Epoch[1/100] | loss train:0.069613| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014984| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013655| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012376| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013811| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009950| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010773| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011356| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009917| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011341| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009835| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011971| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008936| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009991| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009620| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009187| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009431| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010603| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008488| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010365| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009146| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009060| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008829| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008571| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010426| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009113| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009590| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MS training\n",
      "Epoch[1/100] | loss train:0.090060| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.022190| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016392| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015679| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014850| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.017981| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.017929| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013021| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014593| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014310| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012729| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014380| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014347| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013832| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012819| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.015415| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014439| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013012| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011587| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013503| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013731| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012579| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012212| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012389| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011699| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013915| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012725| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011868| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012261| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MOS training\n",
      "Epoch[1/100] | loss train:0.089711| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020947| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018706| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.019031| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.018366| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.018759| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.019688| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.016722| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.018694| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015307| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.018040| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.017118| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.018255| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.020835| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.016809| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.017685| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.019290| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.015969| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.017855| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.015648| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "MSI training\n",
      "Epoch[1/100] | loss train:0.078522| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016969| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015934| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013790| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014748| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013961| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013401| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012409| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014036| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014202| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014793| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011939| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014811| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011448| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013078| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011956| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012648| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013714| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012484| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013121| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012880| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011895| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012563| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011875| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3870 from 2007-11-15 to 2023-03-31\n",
      "MSCI training\n",
      "Epoch[1/100] | loss train:0.052586| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012403| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008892| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008790| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008445| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007777| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007491| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007669| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007476| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008088| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008665| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006942| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007367| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006158| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006784| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008091| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007948| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007468| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007170| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006334| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006926| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007558| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007540| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006485| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5225 from 2002-07-01 to 2023-03-31\n",
      "NDAQ training\n",
      "Epoch[1/100] | loss train:0.073086| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012923| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011904| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014079| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014057| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010819| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011056| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009316| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008609| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10/100] | loss train:0.009633| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009527| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009393| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010112| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010567| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009924| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009507| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008907| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008434| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008697| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008949| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012084| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008925| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008582| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008758| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009287| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009073| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009616| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008933| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NTAP training\n",
      "Epoch[1/100] | loss train:0.056674| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019917| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.020354| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.018707| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.018852| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.017640| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.017652| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.016483| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.017440| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.017670| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.018642| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.017492| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.016644| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.017458| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.016614| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.016859| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.015666| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.016005| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.015491| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.015644| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.017468| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.017434| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.016708| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.014995| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.015567| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.015255| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.014550| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.014612| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.017252| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.014862| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.014625| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.016197| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.014238| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.015004| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.014612| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.015500| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.015018| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.016287| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.014780| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.015022| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.012704| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.013164| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.013165| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.013334| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.013006| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.012858| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.012798| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.012340| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.012868| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.013070| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.012571| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.012742| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.012463| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.012935| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.013130| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.012603| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.012674| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.012375| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5251 from 2002-05-23 to 2023-03-31\n",
      "NFLX training\n",
      "Epoch[1/100] | loss train:0.077038| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020225| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014762| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013758| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011822| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011344| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011570| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011305| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013381| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010099| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010656| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010905| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012251| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010718| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010739| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011381| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010438| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010108| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010431| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009885| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010273| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011452| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010445| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009346| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009323| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010545| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009521| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009367| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009526| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010281| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009734| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008758| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010253| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009130| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009145| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008629| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009685| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009189| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009732| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008205| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008294| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008545| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007918| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008428| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008057| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008027| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007678| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007974| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007305| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006946| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007101| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007472| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007282| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007655| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007635| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007750| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007267| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007193| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007802| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007709| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NWL training\n",
      "Epoch[1/100] | loss train:0.067814| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.021590| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017279| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015814| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013893| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014771| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013453| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013905| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016088| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013582| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014912| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013169| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012785| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013201| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013390| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012653| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013313| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012367| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011162| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012466| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011856| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012759| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012775| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012006| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011659| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[26/100] | loss train:0.012631| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012673| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011576| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.013240| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NEM training\n",
      "Epoch[1/100] | loss train:0.069114| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019649| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018685| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016414| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.017996| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016918| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.016323| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015155| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014699| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016089| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014980| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015328| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015800| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014530| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.015654| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.016055| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.015218| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014259| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.015525| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.015237| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014726| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.015232| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014258| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.014527| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014176| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.015307| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.014548| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013905| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.015178| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.014213| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.014337| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.014564| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.014550| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.014363| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.013798| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.014341| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.014092| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.013365| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013723| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.015174| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.012409| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.012133| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.011872| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.011680| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.011446| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.012101| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.011623| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.011940| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.011832| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.011974| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.011516| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.011403| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.011808| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.011586| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.011784| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.011460| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.011685| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.011925| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.011634| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.011313| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.011965| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.011664| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.012226| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.010921| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.011606| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.011192| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.011340| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.012045| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.012074| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.011634| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.011762| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.011410| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.011239| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.011448| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2464 from 2013-06-19 to 2023-03-31\n",
      "NWSA training\n",
      "Epoch[1/100] | loss train:0.060276| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.038107| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.023415| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012498| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.030372| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013569| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.023787| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012418| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.017965| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.018774| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011031| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010442| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008484| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008401| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010574| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011491| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009376| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009219| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.026890| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014526| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010952| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.106083| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.018411| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008864| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2464 from 2013-06-19 to 2023-03-31\n",
      "NWS training\n",
      "Epoch[1/100] | loss train:0.066736| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.029675| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014125| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012744| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012214| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.020623| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011778| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010720| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.019630| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.095511| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.043881| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.076932| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014743| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009103| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011654| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009061| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.015906| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011468| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.018286| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.018083| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.027835| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010668| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013554| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.017266| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014410| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011010| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NEE training\n",
      "Epoch[1/100] | loss train:0.068539| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015182| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012872| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010953| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011143| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010474| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010019| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011000| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009957| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010192| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011289| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008953| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009358| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009637| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009616| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011055| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008587| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009367| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009807| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008638| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009082| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009766| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008363| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009055| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008381| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010505| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008718| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010262| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008375| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008458| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008794| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008473| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NKE training\n",
      "Epoch[1/100] | loss train:0.058240| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018804| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013449| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016037| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014395| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011674| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011377| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010359| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011216| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010015| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010386| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010457| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012068| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010736| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011110| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009898| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011132| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010708| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010711| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010913| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010353| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009372| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009866| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010607| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010515| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010198| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009501| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009463| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009480| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008650| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010300| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008937| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010722| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008541| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009232| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009807| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009314| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008970| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008399| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007371| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007544| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007602| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007571| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007515| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006928| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007058| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007492| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007361| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007579| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007122| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007858| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007347| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007493| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007215| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007232| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NI training\n",
      "Epoch[1/100] | loss train:0.056945| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015201| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011424| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010961| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010134| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011236| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009356| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008785| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009505| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008916| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009255| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009139| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008797| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009175| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008428| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008680| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008187| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008188| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008628| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008435| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008266| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008302| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008473| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009094| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009748| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008047| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008860| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008349| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008387| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008410| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007627| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008711| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008286| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008304| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008236| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009038| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008221| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008094| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007207| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006565| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006991| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006711| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006764| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006828| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006670| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006412| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006805| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007141| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007161| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006647| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006765| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006889| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006870| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006864| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006875| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006747| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NDSN training\n",
      "Epoch[1/100] | loss train:0.061500| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012443| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015316| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010655| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011758| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009235| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009827| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010942| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009908| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008724| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010475| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010074| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009059| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010379| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009094| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009341| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008749| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009217| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010521| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009194| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NSC training\n",
      "Epoch[1/100] | loss train:0.049613| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013359| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013453| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011076| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013304| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011059| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010572| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012050| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010175| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010676| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010086| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009897| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010224| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009391| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009638| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009471| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009864| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010447| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009441| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009069| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009604| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010002| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009402| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008846| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009386| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[28/100] | loss train:0.010024| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011011| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008516| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009061| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009523| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008737| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008731| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008768| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008845| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009674| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008984| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008976| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008581| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NTRS training\n",
      "Epoch[1/100] | loss train:0.097660| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017983| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016057| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014172| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016135| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014801| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014431| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013890| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013667| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013591| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013086| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013603| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013445| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014785| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012322| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012501| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011920| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013083| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014170| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012760| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014508| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013824| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011902| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012698| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012219| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011985| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013468| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012142| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012759| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013061| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012845| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012493| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011602| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012281| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012063| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011498| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012623| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012178| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012382| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011552| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010479| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009840| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.010550| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009827| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009910| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009807| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009532| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010440| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009896| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009843| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009483| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009833| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009858| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009831| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009129| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009895| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009793| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009437| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009693| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009325| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.010312| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010047| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.010197| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.009719| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.009552| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NOC training\n",
      "Epoch[1/100] | loss train:0.064569| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014638| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013963| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011694| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012419| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011099| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010231| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010041| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010764| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009536| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010372| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011433| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009654| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009865| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010251| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009488| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009262| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009380| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010802| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009499| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010488| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008087| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009104| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009553| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009542| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009197| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009629| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008861| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008830| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010029| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009189| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008890| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2568 from 2013-01-18 to 2023-03-31\n",
      "NCLH training\n",
      "Epoch[1/100] | loss train:0.050207| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009876| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008942| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007308| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007942| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007116| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007250| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007819| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006733| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007430| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007195| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006637| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006555| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006831| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006577| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006291| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006031| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006252| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006557| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006803| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006562| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006130| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006064| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006299| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006637| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006331| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006190| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4866 from 2003-12-02 to 2023-03-31\n",
      "NRG training\n",
      "Epoch[1/100] | loss train:0.075283| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017567| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014390| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014345| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013814| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012191| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011961| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011947| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012699| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011728| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011941| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013960| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012285| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011672| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011868| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011119| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012536| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011321| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011097| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011397| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011256| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011264| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011194| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[24/100] | loss train:0.011525| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011867| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012583| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011040| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011050| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011462| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010745| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010835| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011053| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010811| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011362| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010875| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010524| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010707| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010998| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011334| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010560| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010153| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009431| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009949| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009171| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009229| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009406| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009262| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009001| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009137| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009130| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009276| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009254| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009429| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009149| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009172| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009355| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009433| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009055| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NUE training\n",
      "Epoch[1/100] | loss train:0.082312| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.025171| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017380| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016768| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.020303| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.019419| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.016610| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014318| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014649| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016400| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013311| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.016185| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014968| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.016027| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013520| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.015493| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013256| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.016434| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013333| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.015031| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012280| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013744| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013376| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012798| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014341| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013555| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.014463| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.014760| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012481| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012061| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.015223| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.013976| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.014999| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.013277| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.016195| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012412| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011937| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011779| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.013239| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.013535| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009514| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009645| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009836| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009891| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009606| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009778| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009412| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009809| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008899| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009624| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009350| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008756| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008988| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009888| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008738| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009150| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009331| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009315| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008854| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009509| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.009174| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.009318| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.009525| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.009115| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008839| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NVDA training\n",
      "Epoch[1/100] | loss train:0.084564| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019835| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019237| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016516| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.019233| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016267| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013536| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015705| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013882| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013240| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014712| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012805| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013300| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014122| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013127| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012708| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012833| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012853| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011995| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013062| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013918| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014251| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014174| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013544| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012311| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.014432| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.015022| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012457| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014506| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "NVR training\n",
      "Epoch[1/100] | loss train:0.062909| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014865| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013766| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015775| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012977| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011668| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011685| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012066| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011612| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011495| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010927| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010115| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010519| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012983| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009136| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010243| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009495| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009104| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011439| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010058| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009033| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010009| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010972| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008662| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009514| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009011| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009832| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010764| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009338| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008075| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009966| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009214| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/100] | loss train:0.009623| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009972| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009067| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010074| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009424| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009225| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008247| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007602| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007757| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007532| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007492| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007476| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007555| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007628| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007504| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007335| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007251| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007871| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007330| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007336| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007255| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007467| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007499| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007611| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007153| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007614| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007486| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007249| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007654| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007266| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007590| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006962| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007514| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007445| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007741| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007731| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007161| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007207| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.007377| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006992| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007367| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007459| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3185 from 2010-08-06 to 2023-03-31\n",
      "NXPI training\n",
      "Epoch[1/100] | loss train:0.081364| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013463| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009011| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009022| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009719| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007529| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007654| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008152| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008692| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008074| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008140| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007217| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008588| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008614| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008071| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005855| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006809| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007001| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006820| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007211| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007663| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006051| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007081| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007469| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006943| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006735| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ORLY training\n",
      "Epoch[1/100] | loss train:0.062678| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012514| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011514| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012894| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011816| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010995| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010408| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009819| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010862| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009653| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011877| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008966| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010635| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008605| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008520| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009742| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012538| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009029| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010771| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008859| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009722| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009274| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010175| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008554| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011228| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "OXY training\n",
      "Epoch[1/100] | loss train:0.063831| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015953| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013159| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011717| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012649| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012063| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011072| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011671| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011056| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011023| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012182| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011350| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010469| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011498| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011067| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010630| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010962| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010713| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010548| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011111| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010651| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010590| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010797| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ODFL training\n",
      "Epoch[1/100] | loss train:0.081681| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020821| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015838| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014973| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014639| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012934| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012369| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.016180| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011100| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011103| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011228| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011149| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011431| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013023| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012127| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010096| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008911| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012282| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012294| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010616| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010971| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010085| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011600| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010252| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010400| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011564| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010315| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "OMC training\n",
      "Epoch[1/100] | loss train:0.052727| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015828| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013557| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012620| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012582| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011629| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011304| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010686| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011222| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009950| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010592| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010630| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010134| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009780| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010381| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/100] | loss train:0.010235| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010439| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011153| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010517| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009777| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010572| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010251| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010013| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011025| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011548| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010661| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010486| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009965| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011062| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010812| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5766 from 2000-05-02 to 2023-03-31\n",
      "ON training\n",
      "Epoch[1/100] | loss train:0.088578| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.022682| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.020142| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.019495| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016797| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015859| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013137| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.019487| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.014511| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.015771| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.019233| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013797| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013526| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.014014| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.017904| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012364| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014400| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014100| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012542| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012778| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012519| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013571| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013241| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013951| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014380| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012879| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "OKE training\n",
      "Epoch[1/100] | loss train:0.051412| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013683| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015224| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011906| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010637| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009777| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010475| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012174| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009634| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010601| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009903| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009452| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010260| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010400| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010121| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009524| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011192| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009528| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009536| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010821| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009289| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009910| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009451| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010133| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008606| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009759| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010263| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010358| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009280| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009643| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009497| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010717| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009497| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008763| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ORCL training\n",
      "Epoch[1/100] | loss train:0.068462| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017261| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016894| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015471| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013013| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014999| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011975| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011983| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011716| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011589| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011308| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011167| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011712| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010739| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010639| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011030| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010957| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009951| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011058| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010078| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011590| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010314| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010186| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010632| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010477| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010721| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010761| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010257| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 463 from 2021-05-14 to 2023-03-31\n",
      "OGN training\n",
      "Epoch[1/100] | loss train:0.040262| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010082| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006755| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006592| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006281| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005098| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006022| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005890| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005949| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004864| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004504| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004699| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005130| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004844| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004792| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004579| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004400| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004427| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005075| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004313| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004949| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004746| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004855| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004197| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004202| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004460| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004725| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004419| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005621| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004856| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.004893| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005324| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005529| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005607| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 765 from 2020-03-19 to 2023-03-31\n",
      "OTIS training\n",
      "Epoch[1/100] | loss train:0.036058| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008399| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006074| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.004840| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.004868| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.003712| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004225| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.003516| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.003802| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.003450| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.003584| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.003256| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.003455| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.003199| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.002752| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.003360| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.003480| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003505| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003472| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003132| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003059| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.002961| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.002963| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[24/100] | loss train:0.003170| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.002822| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PCAR training\n",
      "Epoch[1/100] | loss train:0.069916| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016549| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011496| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012382| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012906| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011206| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010783| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011270| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011121| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010039| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011230| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010522| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010907| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010974| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010822| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010303| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011157| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009948| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011729| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009779| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010792| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009210| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010061| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010565| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009240| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009155| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008985| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009606| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009580| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009157| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010964| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009660| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009534| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009779| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010555| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009941| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008457| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009820| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007827| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007497| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007583| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007552| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007459| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007301| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007853| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007256| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007127| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007553| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007491| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007502| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007613| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007316| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008003| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007551| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007466| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007539| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007460| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5831 from 2000-01-28 to 2023-03-31\n",
      "PKG training\n",
      "Epoch[1/100] | loss train:0.077332| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013069| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012871| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011567| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011919| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009319| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010685| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009824| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010058| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009770| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009941| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009169| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010654| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009228| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008968| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009368| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009296| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008433| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008693| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008663| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009626| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009399| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008723| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009407| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4336 from 2006-01-03 to 2023-03-31\n",
      "PARA training\n",
      "Epoch[1/100] | loss train:0.074456| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017518| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013282| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012404| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011040| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011881| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011854| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009794| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012357| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010279| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010415| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010495| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010912| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010122| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010232| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011183| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009572| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010908| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011634| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010524| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009895| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009977| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009686| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010114| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009791| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008404| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011884| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011312| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009818| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008917| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010483| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009476| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009643| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009498| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008890| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009887| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PH training\n",
      "Epoch[1/100] | loss train:0.074099| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014368| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014175| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012300| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012794| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012781| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012933| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011668| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010812| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011207| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010996| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009771| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010630| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010042| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010596| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009526| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010750| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009469| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008872| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010631| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009645| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009656| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009967| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009398| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009680| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009918| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009090| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009943| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011362| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PAYX training\n",
      "Epoch[1/100] | loss train:0.059449| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016068| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015242| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013900| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011664| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6/100] | loss train:0.010958| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010299| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011260| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010967| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011166| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010272| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010333| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010343| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011221| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009294| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009544| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011923| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009590| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010017| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010637| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009837| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009955| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010296| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009764| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2257 from 2014-04-15 to 2023-03-31\n",
      "PAYC training\n",
      "Epoch[1/100] | loss train:0.040831| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008194| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007107| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006104| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005804| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007091| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004885| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005308| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004906| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005478| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004826| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004343| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005015| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004800| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005245| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004400| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004077| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004713| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005027| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004445| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004609| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003875| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.003926| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004522| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004536| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004353| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005062| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004339| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004322| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003721| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003880| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004687| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.004471| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004004| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004468| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.004071| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.003827| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.004164| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004137| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004357| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1940 from 2015-07-20 to 2023-03-31\n",
      "PYPL training\n",
      "Epoch[1/100] | loss train:0.071088| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008518| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006791| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005991| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005725| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005722| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.005534| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004587| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006138| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006858| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004501| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004203| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004443| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004430| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005340| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005099| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.004094| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004782| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003874| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.004375| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004609| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003946| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005464| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003831| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.003907| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003860| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004258| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004776| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.003670| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003570| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003923| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004185| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003638| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003740| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.004374| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003983| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004117| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.003997| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.003550| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.004517| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003609| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003331| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.003592| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003371| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.003078| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003012| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.003027| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.003012| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.002859| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.003249| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.002994| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.003095| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.003199| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.003204| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.003351| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.002986| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.003238| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.003147| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.003064| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PNR training\n",
      "Epoch[1/100] | loss train:0.093421| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016519| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015291| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014438| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012510| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011819| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013234| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012130| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011412| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012551| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010784| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011549| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012315| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013028| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011735| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011226| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012071| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010064| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011031| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011425| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010906| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010640| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010434| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009742| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010088| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011398| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012479| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009874| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009641| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010087| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011202| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010543| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009588| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010930| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011035| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010467| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009299| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008050| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008127| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007919| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/100] | loss train:0.007639| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008215| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007343| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007842| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008285| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008201| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007857| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007519| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007776| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007939| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007797| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007975| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008311| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PEP training\n",
      "Epoch[1/100] | loss train:0.051544| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013461| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014456| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010431| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010337| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010393| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011629| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009036| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009998| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010766| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010115| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009969| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009139| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010001| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011204| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009926| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009659| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009442| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PKI training\n",
      "Epoch[1/100] | loss train:0.060466| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016297| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016191| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011788| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012962| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011522| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012136| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012655| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008995| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010912| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010487| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010842| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010068| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011311| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010621| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009879| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010643| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010902| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PFE training\n",
      "Epoch[1/100] | loss train:0.067476| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015231| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013894| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013630| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014483| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011999| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012111| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013902| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012075| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011284| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013209| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011917| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011776| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012625| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010754| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011364| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010850| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010518| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010265| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011099| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009956| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010814| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011633| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009903| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009757| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009590| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010926| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010564| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011696| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010940| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013295| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010667| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010566| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009766| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010893| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010058| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PCG training\n",
      "Epoch[1/100] | loss train:0.062478| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015457| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014698| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014431| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011763| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012548| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011149| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010660| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011313| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011224| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010880| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010775| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010565| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010403| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010514| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010666| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009931| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009662| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010087| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009728| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009644| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009620| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010583| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008982| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010072| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010667| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009314| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010562| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009977| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009114| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009766| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010120| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009842| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009353| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3788 from 2008-03-17 to 2023-03-31\n",
      "PM training\n",
      "Epoch[1/100] | loss train:0.059585| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011141| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011049| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009020| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009390| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008636| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007973| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008914| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007901| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008592| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007459| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008096| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008070| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008267| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008219| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006941| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007411| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007466| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006892| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007883| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006892| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006309| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007101| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008473| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007217| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007390| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006259| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006826| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007444| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006695| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006945| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.006797| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007705| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007054| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007294| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006979| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.006870| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2761 from 2012-04-12 to 2023-03-31\n",
      "PSX training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.073572| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014831| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010849| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010989| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009002| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008992| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010150| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009767| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009149| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009374| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008250| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008540| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009326| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008471| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007845| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009240| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008029| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008640| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009366| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007350| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009005| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008100| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008656| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009166| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007917| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007892| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008366| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008129| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007772| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007957| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PNW training\n",
      "Epoch[1/100] | loss train:0.053223| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014373| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011995| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010175| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010899| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009849| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009215| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009334| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009703| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009743| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008354| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008874| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008876| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008341| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009105| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009277| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008828| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008809| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008879| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008680| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009375| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008208| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008062| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007875| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008795| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009203| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008609| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008617| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008651| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008271| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008181| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009093| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008085| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007913| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PXD training\n",
      "Epoch[1/100] | loss train:0.063728| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017320| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013703| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013278| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012879| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011955| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011667| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012226| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011069| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010904| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010859| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010576| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011820| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010534| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012971| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012074| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010578| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010861| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010760| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011040| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010361| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012379| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010300| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010486| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010806| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010653| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010488| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010755| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010091| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010997| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010186| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011576| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009828| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010688| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010140| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010647| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009985| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010090| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008628| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008631| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008739| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008396| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008887| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008270| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008218| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008540| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008612| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008360| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008056| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008252| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008277| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008239| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008503| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008194| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008767| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008257| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008602| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008569| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008640| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PNC training\n",
      "Epoch[1/100] | loss train:0.063035| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014972| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014717| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012960| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012218| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012155| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011758| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011710| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012300| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012151| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011921| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010410| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012837| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010023| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011715| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011413| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011116| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010415| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010617| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011117| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010895| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010469| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010583| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "POOL training\n",
      "Epoch[1/100] | loss train:0.070157| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016827| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013035| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012890| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015198| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012788| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011608| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011786| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013124| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/100] | loss train:0.011488| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010642| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010289| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011079| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011812| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011453| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010397| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009425| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010308| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011859| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010950| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010658| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009518| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010578| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011243| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009853| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010302| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012265| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PPG training\n",
      "Epoch[1/100] | loss train:0.060352| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015567| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010652| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010582| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011151| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010508| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010473| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009331| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010196| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010257| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009662| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009735| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010098| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009515| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008941| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008619| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008895| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009967| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008724| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008791| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010025| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008990| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008823| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009470| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009163| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009310| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PPL training\n",
      "Epoch[1/100] | loss train:0.065986| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015780| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011937| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011697| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012547| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012271| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010859| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011765| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010232| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010633| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010955| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011313| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011236| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010210| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011191| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010291| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010585| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009855| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010534| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009481| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009629| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010678| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009619| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010295| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010032| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009744| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009747| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010142| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009782| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009632| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5397 from 2001-10-23 to 2023-03-31\n",
      "PFG training\n",
      "Epoch[1/100] | loss train:0.068796| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017857| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014725| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015312| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011923| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013185| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013632| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012707| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012216| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011707| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015068| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013297| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014125| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011339| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011259| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013722| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011803| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010780| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011131| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012332| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011992| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011490| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011760| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011472| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011739| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012987| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011952| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010962| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PG training\n",
      "Epoch[1/100] | loss train:0.057849| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014362| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012923| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013943| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011289| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009769| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011779| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010853| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011002| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009860| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010553| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009694| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010718| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009831| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009612| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009073| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009786| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008464| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009498| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009023| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009700| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009645| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008964| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009109| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008391| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009069| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009603| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009802| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008585| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009979| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009350| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009138| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009022| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008407| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008205| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008388| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008783| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009091| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007955| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007142| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006946| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007238| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007026| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006813| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007012| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007041| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007217| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006918| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006941| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006849| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007036| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006996| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007009| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006570| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006778| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006649| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[59/100] | loss train:0.007430| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006912| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006531| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006625| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006735| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006870| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007199| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007066| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006687| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007020| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006697| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006537| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006766| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PGR training\n",
      "Epoch[1/100] | loss train:0.069184| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015368| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013890| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014601| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013275| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011300| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012746| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009505| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011740| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010851| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011399| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011043| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010628| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010138| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011687| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010332| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010651| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011780| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PLD training\n",
      "Epoch[1/100] | loss train:0.091471| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020061| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016625| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015068| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015087| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012958| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011838| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013338| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012545| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010478| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011702| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015929| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009820| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011059| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010418| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010367| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009837| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010225| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009600| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009679| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010629| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009242| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011521| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010976| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010537| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009912| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009940| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009872| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010668| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010103| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009408| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5361 from 2001-12-13 to 2023-03-31\n",
      "PRU training\n",
      "Epoch[1/100] | loss train:0.067336| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014582| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013163| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013938| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012832| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011240| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011028| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012671| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011423| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012074| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011966| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011234| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012102| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010789| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010229| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010405| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010335| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011035| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010701| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010490| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011686| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010577| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010838| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011204| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011237| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PEG training\n",
      "Epoch[1/100] | loss train:0.056644| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014080| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012849| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011003| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011514| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010920| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009614| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009279| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009425| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009799| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009563| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009383| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009611| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009990| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009424| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009839| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008820| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009760| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008607| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009163| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009370| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009535| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009075| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008320| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008704| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008939| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008258| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008692| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009029| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008601| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008996| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008146| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008928| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008541| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008274| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008366| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009193| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008365| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008302| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007464| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007118| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007078| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006799| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007336| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006970| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006854| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007113| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007034| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007081| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007028| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007080| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006855| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006657| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006857| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006655| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006729| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006912| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006846| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006653| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007119| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006997| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006695| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007102| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006981| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006978| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007229| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006805| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007385| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006977| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PTC training\n",
      "Epoch[1/100] | loss train:0.069114| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015123| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/100] | loss train:0.014286| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013357| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012738| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012496| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012319| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013255| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012239| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011893| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012164| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011459| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012652| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012779| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010780| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013040| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011829| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010907| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012236| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011507| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010604| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011121| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011085| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010809| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010055| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010822| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010731| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010742| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010718| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011563| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010980| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011773| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010794| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011087| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010697| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010905| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010413| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009118| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008961| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008551| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008959| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008342| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008204| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008856| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009034| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008240| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008756| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008836| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008311| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008528| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008761| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008773| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008183| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008470| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008490| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008557| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008334| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008587| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008202| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008169| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008659| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008998| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008960| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008694| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.008180| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008166| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.008364| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008608| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.008429| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.008680| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.008623| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.008296| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.008482| lr:0.001000\n",
      "Epoch[77/100] | loss train:0.008746| lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007984| lr:0.001000\n",
      "Epoch[79/100] | loss train:0.008550| lr:0.001000\n",
      "Epoch[80/100] | loss train:0.008619| lr:0.001000\n",
      "Epoch[81/100] | loss train:0.008149| lr:0.000100\n",
      "Epoch[82/100] | loss train:0.008453| lr:0.000100\n",
      "Epoch[83/100] | loss train:0.008626| lr:0.000100\n",
      "Epoch[84/100] | loss train:0.008290| lr:0.000100\n",
      "Epoch[85/100] | loss train:0.008084| lr:0.000100\n",
      "Epoch[86/100] | loss train:0.008402| lr:0.000100\n",
      "Epoch[87/100] | loss train:0.007911| lr:0.000100\n",
      "Epoch[88/100] | loss train:0.008504| lr:0.000100\n",
      "Epoch[89/100] | loss train:0.008540| lr:0.000100\n",
      "Epoch[90/100] | loss train:0.008051| lr:0.000100\n",
      "Epoch[91/100] | loss train:0.008335| lr:0.000100\n",
      "Epoch[92/100] | loss train:0.008538| lr:0.000100\n",
      "Epoch[93/100] | loss train:0.008487| lr:0.000100\n",
      "Epoch[94/100] | loss train:0.008118| lr:0.000100\n",
      "Epoch[95/100] | loss train:0.007943| lr:0.000100\n",
      "Epoch[96/100] | loss train:0.008236| lr:0.000100\n",
      "Epoch[97/100] | loss train:0.008114| lr:0.000100\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PSA training\n",
      "Epoch[1/100] | loss train:0.061091| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013314| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011906| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012466| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012302| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010521| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009676| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010624| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010799| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009686| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009433| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011162| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009135| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009008| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010181| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009933| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009733| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008763| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010331| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007947| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009507| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009681| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008285| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009339| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009360| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009905| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008683| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010572| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008687| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008484| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PHM training\n",
      "Epoch[1/100] | loss train:0.064090| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015542| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013580| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013049| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012555| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011536| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013958| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011986| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012265| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011943| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011879| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012196| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013539| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011285| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012823| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012346| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011472| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011575| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011079| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011477| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011882| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010795| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011643| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011814| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010612| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010911| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010115| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010892| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011094| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011594| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011642| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011556| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010721| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011067| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011040| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[38/100] | loss train:0.011066| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2076 from 2015-01-02 to 2023-03-31\n",
      "QRVO training\n",
      "Epoch[1/100] | loss train:0.059127| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009038| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008148| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007509| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006850| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006322| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006584| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005982| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008226| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006666| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005460| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007311| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006009| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006543| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005799| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006499| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006322| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006541| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005108| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005520| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005362| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.005644| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005688| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004878| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005557| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004974| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005893| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006889| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006115| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005925| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.004711| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004882| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005688| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005466| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005684| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005922| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.005902| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.005148| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.005001| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005178| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.004875| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "PWR training\n",
      "Epoch[1/100] | loss train:0.085865| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019451| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016837| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014931| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.018022| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.016648| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012728| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011684| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011783| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013815| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011450| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013083| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012021| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011819| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012023| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.015387| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012476| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012703| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012255| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013416| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010949| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013066| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011205| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012040| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011897| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011616| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011650| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011181| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012054| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.013393| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010573| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011254| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010795| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010329| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011723| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011348| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.013832| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010797| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010862| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010145| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008607| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008419| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008322| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008450| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007434| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007954| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008285| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008103| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007753| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008383| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008117| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008519| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007917| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008868| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008866| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "QCOM training\n",
      "Epoch[1/100] | loss train:0.095078| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016138| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016558| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014179| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014923| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014041| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012951| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013739| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013637| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013308| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012564| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012147| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011340| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012780| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012110| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011219| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010997| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012327| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012227| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014170| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011735| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012334| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012130| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011115| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012187| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012246| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012057| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "DGX training\n",
      "Epoch[1/100] | loss train:0.059282| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014087| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013409| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012023| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012108| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010722| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013054| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011681| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009576| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009514| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011721| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010443| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009586| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010350| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009382| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010517| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010185| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009035| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010148| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009441| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009835| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009464| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009292| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009458| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009057| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009844| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008694| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010208| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009350| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009433| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009358| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009499| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009376| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009124| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010253| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009379| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009634| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RL training\n",
      "Epoch[1/100] | loss train:0.077681| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015284| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014185| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011597| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011613| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012138| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011606| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011092| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011820| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011190| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010574| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011745| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010621| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010471| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010761| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011363| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011506| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011029| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011007| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010303| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010633| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011274| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010507| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010876| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010651| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010776| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010581| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011960| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010152| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010827| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010217| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010292| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010970| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010137| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011065| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009960| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010043| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010813| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010234| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009086| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008862| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009140| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008872| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008705| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008757| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008865| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008608| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008900| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008718| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008958| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009090| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008667| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008373| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009001| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008698| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008515| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008668| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008572| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008742| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008699| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008399| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008375| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008460| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RJF training\n",
      "Epoch[1/100] | loss train:0.081044| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017719| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016612| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014894| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015472| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011390| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012751| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011904| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011666| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011614| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009854| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012165| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012422| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013501| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010753| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011116| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011174| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010736| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010734| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010660| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011880| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RTX training\n",
      "Epoch[1/100] | loss train:0.069475| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013746| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012287| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011453| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011360| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010625| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011846| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011322| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010545| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012560| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011225| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009931| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011603| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010545| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009782| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010484| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009497| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010509| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010318| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010211| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009526| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010160| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009898| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010035| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009431| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009657| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009224| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010129| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009348| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009943| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009346| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009227| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009109| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009520| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009305| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009177| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007847| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007781| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007703| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007442| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007941| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007528| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007796| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007859| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007574| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007323| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007436| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007567| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007774| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007644| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007396| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007365| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008085| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007682| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007881| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007516| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "O training\n",
      "Epoch[1/100] | loss train:0.081742| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012863| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010930| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010872| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010860| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009571| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011129| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010172| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009683| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009741| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009075| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009838| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008941| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008747| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009373| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009392| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[18/100] | loss train:0.008370| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009190| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008817| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009793| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008641| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008655| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008650| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008872| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008449| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008918| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007778| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008570| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008104| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008750| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008113| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008184| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008248| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007989| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008531| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008012| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008486| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "REG training\n",
      "Epoch[1/100] | loss train:0.073292| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015307| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013036| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011662| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011601| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011225| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011546| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011955| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010981| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010664| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011263| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010397| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011244| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010137| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010102| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010506| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010569| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010171| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010529| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009948| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010747| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010220| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011011| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010463| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009734| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010537| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010675| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010468| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009894| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009487| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009849| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010538| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010112| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010231| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009979| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010337| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009822| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008725| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008218| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008410| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008170| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008405| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008473| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008194| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008308| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008101| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008388| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008027| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008244| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008510| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008412| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008245| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008209| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008280| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008199| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008109| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008279| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007999| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008428| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008436| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.008078| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008659| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008374| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008255| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.008341| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008194| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.008285| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.008170| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "REGN training\n",
      "Epoch[1/100] | loss train:0.069984| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014083| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012262| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010805| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010557| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010331| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009562| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010224| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010495| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010255| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010475| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009630| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010747| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009626| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009366| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010520| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009211| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009460| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008694| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008522| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009440| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009330| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008466| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009021| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009306| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008756| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009126| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009040| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008574| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008562| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008952| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008769| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008641| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RF training\n",
      "Epoch[1/100] | loss train:0.068200| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018056| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014520| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014987| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013316| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012467| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012361| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013061| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012407| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013480| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012632| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012894| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012954| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011822| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012082| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011650| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011730| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012508| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011773| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012200| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012531| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011502| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012237| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011221| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011256| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011666| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011548| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011019| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011635| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011320| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011843| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[32/100] | loss train:0.011066| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011076| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010979| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011962| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010962| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010708| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011353| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011245| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.010180| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009030| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009499| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009474| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009504| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009504| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009741| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009375| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009983| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009515| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009146| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009477| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RSG training\n",
      "Epoch[1/100] | loss train:0.060281| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013423| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011604| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014676| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010313| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009209| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009655| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010887| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010957| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010279| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009348| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010148| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011291| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010464| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009864| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RMD training\n",
      "Epoch[1/100] | loss train:0.078670| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015639| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016517| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012818| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014180| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011211| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011689| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011106| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011264| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010345| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011617| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010119| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010849| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011622| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010913| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010649| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009856| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010224| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009123| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009141| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012037| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010617| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009311| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010222| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009689| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010461| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010628| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009960| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009952| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5891 from 1999-11-01 to 2023-03-31\n",
      "RHI training\n",
      "Epoch[1/100] | loss train:0.068119| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020849| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019454| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014328| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014577| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012984| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013332| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013059| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012644| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013961| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013296| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012708| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015088| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009957| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013166| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012692| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014302| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012389| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011520| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011724| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012774| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012106| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012777| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010967| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ROK training\n",
      "Epoch[1/100] | loss train:0.058271| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014574| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012693| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012161| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012521| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013351| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011079| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010522| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010194| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010844| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009710| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010088| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010648| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010158| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011284| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010859| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010918| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010467| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010402| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010030| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009907| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ROL training\n",
      "Epoch[1/100] | loss train:0.056070| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012939| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013087| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010248| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010016| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010879| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009601| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009443| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009164| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008879| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009307| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009389| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009589| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009853| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008771| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009031| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008740| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008954| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008846| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008954| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008638| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008575| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008710| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008983| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008880| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008376| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008247| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008629| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008924| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008567| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008099| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007960| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009051| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008797| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009580| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008414| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008791| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007400| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007404| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006845| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006848| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006560| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007246| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007025| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006988| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006859| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[50/100] | loss train:0.006813| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006816| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006702| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007036| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006841| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006930| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ROP training\n",
      "Epoch[1/100] | loss train:0.066542| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013452| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011847| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011794| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011208| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008666| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009595| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010135| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009904| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009188| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009041| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008828| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008604| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009101| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009220| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009369| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008567| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008665| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008528| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008354| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009072| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008548| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008277| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008757| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008965| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008515| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008244| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008185| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008200| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008480| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.007904| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008348| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.007893| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.007518| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008737| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008157| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008242| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.007564| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007246| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007202| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006954| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006552| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006657| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006542| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006351| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006730| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006652| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006245| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006519| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006557| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006550| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006765| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006398| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006427| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006448| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006383| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006716| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006404| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ROST training\n",
      "Epoch[1/100] | loss train:0.052323| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013108| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012264| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010792| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011120| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012205| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009910| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012927| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009660| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010406| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009986| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009371| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009976| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009070| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010025| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008747| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009170| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009036| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008900| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009059| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009193| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008893| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009068| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009581| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008520| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009005| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009386| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008648| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009011| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008673| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008730| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008754| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008438| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009885| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008549| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008982| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009293| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008559| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009013| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009088| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007790| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007468| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006783| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007315| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007188| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007071| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007170| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007397| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007679| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007250| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007744| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007147| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007057| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "RCL training\n",
      "Epoch[1/100] | loss train:0.063193| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016957| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015720| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013799| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012021| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011909| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012732| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011902| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011289| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012363| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012634| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011682| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011233| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012543| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011776| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011588| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011439| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010740| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011057| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012053| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011143| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012178| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010647| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010739| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011357| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010371| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011007| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011036| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010576| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010984| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010621| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010844| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010566| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010962| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010593| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010440| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5591 from 2001-01-02 to 2023-03-31\n",
      "SPGI training\n",
      "Epoch[1/100] | loss train:0.081764| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012608| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014098| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010629| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011406| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010092| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/100] | loss train:0.010150| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010770| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009749| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009107| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008035| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011238| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009551| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008937| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010696| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009051| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011010| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009417| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009698| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008232| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009780| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4727 from 2004-06-23 to 2023-03-31\n",
      "CRM training\n",
      "Epoch[1/100] | loss train:0.056421| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014327| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011359| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011038| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010938| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011394| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009105| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010470| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009392| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008926| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010134| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008698| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008924| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009426| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008732| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010464| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009039| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009283| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010419| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008809| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008456| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008686| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009484| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007954| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008714| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008439| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008852| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008656| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008756| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008523| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008660| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008053| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.007998| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008101| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SBAC training\n",
      "Epoch[1/100] | loss train:0.064701| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016313| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013500| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011382| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012597| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011735| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011891| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009167| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010798| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012939| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011803| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009689| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009900| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009397| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010880| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010297| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009410| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010698| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SLB training\n",
      "Epoch[1/100] | loss train:0.074863| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018611| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018161| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014615| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014563| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013639| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013961| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013946| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012442| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012650| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012304| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011818| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011961| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012630| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012880| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012005| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012544| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012099| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011848| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012588| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013971| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012780| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5111 from 2002-12-11 to 2023-03-31\n",
      "STX training\n",
      "Epoch[1/100] | loss train:0.067029| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016958| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.019153| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012109| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012801| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010641| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013334| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010839| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014867| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012632| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011529| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011043| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012388| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010250| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010145| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010877| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011060| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009923| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010475| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010626| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010424| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009482| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009615| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011531| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009828| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009870| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010585| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009704| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011258| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010613| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010348| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010713| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SEE training\n",
      "Epoch[1/100] | loss train:0.085054| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015339| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015996| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013488| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012357| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014300| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012074| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012919| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011350| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012583| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012570| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012998| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011102| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012873| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012833| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012152| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011666| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011985| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011965| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011074| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010821| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010912| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010623| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010897| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011101| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SRE training\n",
      "Epoch[1/100] | loss train:0.073625| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013970| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013120| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012218| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011805| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009635| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011131| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010358| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010187| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011478| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/100] | loss train:0.009798| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009491| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010265| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009860| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008846| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009867| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010067| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009779| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010006| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009038| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009353| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008423| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009289| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009845| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009405| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009855| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008956| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008667| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009044| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009373| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010004| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009017| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2706 from 2012-06-29 to 2023-03-31\n",
      "NOW training\n",
      "Epoch[1/100] | loss train:0.055144| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008848| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007070| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007138| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007508| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005924| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006026| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005976| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006173| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005451| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005225| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005443| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005733| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006047| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.005264| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004938| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005257| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005934| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005236| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005124| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004772| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004718| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005029| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004935| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005188| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005764| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005675| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005390| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005817| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004430| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005163| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005075| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005006| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.005293| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.005039| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.005081| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.004658| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.004821| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.004750| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.005480| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SHW training\n",
      "Epoch[1/100] | loss train:0.071541| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015086| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012893| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012661| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011229| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011298| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009875| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010537| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010421| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010413| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010722| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009938| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010305| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010302| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009479| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010385| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009124| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009993| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011042| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009381| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009746| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008996| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009817| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009755| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009675| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009648| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009241| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008630| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010579| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009106| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010368| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010380| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009169| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009172| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009366| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010709| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009763| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010063| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SPG training\n",
      "Epoch[1/100] | loss train:0.069288| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013356| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012706| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012713| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012197| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010660| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009803| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010012| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009701| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009561| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009251| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010142| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009253| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009355| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009864| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009139| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009319| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010240| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008819| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008739| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009451| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008613| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008925| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008693| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009003| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009211| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009329| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009260| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009325| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008623| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009307| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009059| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SWKS training\n",
      "Epoch[1/100] | loss train:0.081125| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013961| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015096| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012517| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012771| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012093| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011319| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011233| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012983| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011180| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012165| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010214| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011621| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011342| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011704| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012192| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012153| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010693| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011041| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010729| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011838| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009992| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011547| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011263| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011804| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010646| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010101| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[28/100] | loss train:0.009934| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010373| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009845| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010020| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010060| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011577| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010264| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010284| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010222| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009933| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009750| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010335| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011106| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008212| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008588| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008202| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007804| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008031| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008418| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008419| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008010| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008667| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007781| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008129| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007987| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008076| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008196| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008123| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008638| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008158| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008107| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008018| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008324| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SJM training\n",
      "Epoch[1/100] | loss train:0.058045| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013397| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010304| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010579| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010155| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009341| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010243| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009951| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009036| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009697| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008957| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009003| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009261| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009047| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009094| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009300| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008537| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008204| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008138| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008857| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009050| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008806| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008655| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009241| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008815| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008964| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008517| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008792| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008176| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SNA training\n",
      "Epoch[1/100] | loss train:0.062125| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014365| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012507| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011432| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010972| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011124| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010707| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009884| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010256| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010210| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010275| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009992| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009606| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011182| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008778| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009146| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010172| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009921| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009115| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009548| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009163| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009132| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009482| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009891| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009320| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2019 from 2015-03-26 to 2023-03-31\n",
      "SEDG training\n",
      "Epoch[1/100] | loss train:0.044498| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015321| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009922| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007041| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005414| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008543| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012915| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007616| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006606| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005511| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009464| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007234| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007060| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007009| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006049| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SO training\n",
      "Epoch[1/100] | loss train:0.057347| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014635| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012463| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012729| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011246| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011436| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012273| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010443| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011331| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011300| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010399| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010561| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009535| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009758| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009515| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010624| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008732| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008408| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009194| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010030| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009896| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010001| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008770| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010158| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010141| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008729| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009313| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "LUV training\n",
      "Epoch[1/100] | loss train:0.055332| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012984| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012289| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011272| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010341| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011713| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010545| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009946| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010049| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010613| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010182| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010142| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009673| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009801| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010109| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009524| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009765| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009606| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010372| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009599| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009871| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010281| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SWK training\n",
      "Epoch[1/100] | loss train:0.058153| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014157| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013551| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011374| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011805| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012117| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009840| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010387| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011748| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011499| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010039| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009963| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011882| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011697| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010091| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010328| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009868| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SBUX training\n",
      "Epoch[1/100] | loss train:0.055314| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014223| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013909| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011108| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011013| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011024| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012003| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010479| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010201| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010093| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010473| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010696| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011247| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010013| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010083| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010484| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010021| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009509| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009097| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010305| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010003| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010078| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009450| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009025| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010394| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009477| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010469| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009039| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008553| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009830| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010176| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008852| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009200| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009240| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008724| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009687| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009028| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010266| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008717| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "STT training\n",
      "Epoch[1/100] | loss train:0.071471| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019983| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017128| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017222| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.018944| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.015610| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015461| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015012| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016160| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.016247| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014987| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.015727| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014757| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013728| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014035| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.016835| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014101| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014972| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.014581| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014247| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013683| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.014088| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014588| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.014351| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.014006| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013653| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013915| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.014634| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014026| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.014015| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.013632| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.014197| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012911| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.014165| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.014837| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.013525| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.014358| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.014090| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.014589| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.014316| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.012424| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.011829| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.012019| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.011954| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.011884| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.011678| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.011336| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.012282| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.011757| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.011483| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.011671| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.011515| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.011888| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.011754| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.011704| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.011572| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.011842| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "STLD training\n",
      "Epoch[1/100] | loss train:0.106351| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020876| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017124| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013503| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014689| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013190| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015770| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014914| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012807| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012819| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013568| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014508| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015507| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.015065| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013192| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011524| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012390| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012816| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012810| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011610| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011859| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012020| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011645| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.015080| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013202| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.016709| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "STE training\n",
      "Epoch[1/100] | loss train:0.068732| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015377| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014053| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012005| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010988| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011988| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011188| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010503| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010242| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010071| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010689| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008664| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009453| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009561| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010380| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009049| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010314| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010217| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009778| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009367| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/100] | loss train:0.009205| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009476| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SYK training\n",
      "Epoch[1/100] | loss train:0.082162| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015606| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012201| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010897| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011861| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010366| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011441| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010542| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010064| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010889| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011000| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010617| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009193| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010076| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011128| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009570| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009733| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009613| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010814| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009572| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008996| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009213| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010028| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009668| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009019| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009137| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009184| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009219| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009108| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009376| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009962| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009466| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008883| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010386| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009338| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009180| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009597| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007588| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007462| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007456| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007403| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007107| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007146| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007527| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007295| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007207| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007249| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007290| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007314| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007235| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007223| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007056| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007184| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007294| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007134| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007087| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006892| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007275| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006981| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007508| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007200| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007255| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007386| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007105| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007121| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007272| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007157| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2183 from 2014-07-31 to 2023-03-31\n",
      "SYF training\n",
      "Epoch[1/100] | loss train:0.063501| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013981| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010893| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010933| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011242| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011215| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010684| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008872| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008982| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009744| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009616| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009306| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010519| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010431| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009858| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009404| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009527| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SNPS training\n",
      "Epoch[1/100] | loss train:0.074670| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019085| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016564| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013902| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013812| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010925| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015373| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013550| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011390| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013114| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011639| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012872| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011402| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009692| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011075| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010853| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009721| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011655| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012921| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010842| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011414| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011842| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009470| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012100| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011448| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010985| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010128| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010538| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010703| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010257| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011168| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009750| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009897| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012012| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010162| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010186| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009909| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "SYY training\n",
      "Epoch[1/100] | loss train:0.073052| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015744| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013004| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012112| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013239| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013447| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010443| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010835| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012379| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010443| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010075| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009635| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011315| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009498| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010041| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010917| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009859| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009534| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009843| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009383| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009471| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008984| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009557| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010432| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009400| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009483| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009706| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009629| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010047| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009007| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009504| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009600| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4017 from 2007-04-19 to 2023-03-31\n",
      "TMUS training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.045101| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011326| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008875| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008676| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007781| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008673| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009252| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007245| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007934| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007178| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008527| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007265| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007396| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006711| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007355| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006781| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007535| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008934| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006095| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007447| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006748| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008289| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.006604| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006177| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006254| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006707| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006737| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006749| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006607| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TROW training\n",
      "Epoch[1/100] | loss train:0.071460| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014355| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016339| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013474| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014433| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012894| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011997| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011542| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011762| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011712| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011231| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011195| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011255| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013011| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011031| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011237| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011445| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012052| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010589| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010598| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010320| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011319| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010295| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011121| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011360| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012527| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011688| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012336| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011582| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010331| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011542| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010975| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TTWO training\n",
      "Epoch[1/100] | loss train:0.074708| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015074| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012556| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011272| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012236| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012688| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009980| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011644| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010219| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011186| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011064| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010510| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010907| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010057| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009757| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010337| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009783| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009769| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010710| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010909| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010241| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010558| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009372| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009545| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010480| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010731| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009727| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009862| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009699| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010951| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009332| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009413| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009439| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010861| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008888| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009122| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009321| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010848| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009790| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007907| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008152| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007996| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007662| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007214| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007547| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007575| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007827| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007396| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007032| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007250| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007621| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007301| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007584| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007709| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007428| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007206| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007626| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007210| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007272| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5656 from 2000-10-06 to 2023-03-31\n",
      "TPR training\n",
      "Epoch[1/100] | loss train:0.071268| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017113| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016690| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014991| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013950| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014295| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012980| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013544| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015075| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013298| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012986| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013108| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014442| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011628| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012791| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013336| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012632| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012108| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013860| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013059| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011135| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012572| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011425| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013015| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012313| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012637| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011423| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012382| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011576| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011547| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012154| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3100 from 2010-12-07 to 2023-03-31\n",
      "TRGP training\n",
      "Epoch[1/100] | loss train:0.049449| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010627| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010557| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011379| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009262| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008386| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010729| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008583| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/100] | loss train:0.009084| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009007| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008238| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008092| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009564| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007705| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006850| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007498| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007988| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007168| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008621| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007380| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007455| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007664| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007979| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007790| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008238| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TGT training\n",
      "Epoch[1/100] | loss train:0.073058| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015873| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016723| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012935| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016073| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014750| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015679| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011913| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013157| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012564| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011460| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011292| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012676| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010849| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013952| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011575| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011247| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013852| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012126| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012961| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011269| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011500| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013100| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012170| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3978 from 2007-06-14 to 2023-03-31\n",
      "TEL training\n",
      "Epoch[1/100] | loss train:0.061545| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011883| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009132| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008442| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007611| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008333| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008153| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008262| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008199| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007086| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008271| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007320| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007138| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006947| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007706| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006456| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007545| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006129| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006349| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006585| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006356| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006737| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007281| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.006831| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.006903| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.006192| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006996| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006483| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5876 from 1999-11-23 to 2023-03-31\n",
      "TDY training\n",
      "Epoch[1/100] | loss train:0.058344| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014715| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013676| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012352| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010819| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011072| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012346| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011110| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010560| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010687| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010304| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011070| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010463| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010740| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010770| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010139| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009552| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010110| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009605| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009891| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010919| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011046| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009658| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011001| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009839| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010015| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TFX training\n",
      "Epoch[1/100] | loss train:0.079890| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017791| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014720| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012042| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011358| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013102| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010099| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011474| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010290| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010785| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010902| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010285| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010742| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010287| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010027| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009391| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010348| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011118| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010282| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010353| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010266| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010091| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009079| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010038| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009647| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009911| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009952| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009570| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009085| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009535| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009437| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010539| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009918| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TER training\n",
      "Epoch[1/100] | loss train:0.073103| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017987| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.018371| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014526| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013892| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013270| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014563| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014612| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015425| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013447| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014761| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012364| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.015340| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013067| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012507| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013957| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012468| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012154| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013645| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011315| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013848| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011786| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012442| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011609| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012441| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013338| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011970| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013157| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012258| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012393| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 3212 from 2010-06-29 to 2023-03-31\n",
      "TSLA training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.059402| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011577| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009994| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009460| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009512| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008355| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007957| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008899| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.007882| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007692| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009580| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008610| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009078| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006809| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009755| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007007| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007330| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007463| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008242| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008336| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.006153| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008660| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007249| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007104| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007218| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007104| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.006554| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007943| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006854| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005819| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006949| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007966| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.006890| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.006716| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.006466| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.006983| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007528| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.007095| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008797| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.006970| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TXN training\n",
      "Epoch[1/100] | loss train:0.069450| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016509| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013283| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014409| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012205| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010061| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011294| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010801| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011166| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010897| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010063| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010491| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011108| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010366| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011503| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010166| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010386| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010630| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009555| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TXT training\n",
      "Epoch[1/100] | loss train:0.066469| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017952| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014377| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013658| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013612| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013475| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012674| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012218| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012442| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011835| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012799| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013715| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011965| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013520| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012878| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011818| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012209| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011925| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012198| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012377| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012732| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012476| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011809| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012165| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011756| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010891| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012196| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011744| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011090| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011194| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011385| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010793| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012130| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010900| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010998| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011283| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011191| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011182| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.011155| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011128| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009772| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.009736| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009285| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.009381| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.009164| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.009139| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009171| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.009841| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.009219| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.009341| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.009071| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.009078| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.009348| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.009304| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009310| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.009605| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.009573| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.009106| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.009420| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.009383| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.009291| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TMO training\n",
      "Epoch[1/100] | loss train:0.083069| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016881| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.017654| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012794| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012935| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011464| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013664| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010309| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011132| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010819| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010905| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010272| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010989| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010451| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010268| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011182| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009535| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009269| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009747| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009701| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010570| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009465| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009554| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009458| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009520| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009895| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009151| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009333| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009181| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008880| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009679| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008904| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009077| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008752| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009644| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008698| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009236| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008966| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009216| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008406| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007830| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/100] | loss train:0.008331| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006947| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007049| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007480| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007496| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006928| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007447| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007398| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007158| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007101| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007523| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006918| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007386| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007237| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007428| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007177| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006823| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007528| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007306| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006909| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006930| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007569| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007564| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.006416| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007363| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007202| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006992| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006717| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.006857| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.006667| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.006855| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.006760| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007248| lr:0.001000\n",
      "Epoch[76/100] | loss train:0.006615| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TJX training\n",
      "Epoch[1/100] | loss train:0.079103| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014186| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011128| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010347| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011307| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010722| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010207| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011798| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010267| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009642| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009040| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010222| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009665| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008573| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008894| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008717| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009192| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009885| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008719| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008599| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008539| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009501| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008578| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009370| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008520| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009188| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008261| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009562| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008556| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008574| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008980| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008715| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008670| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008631| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009244| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008458| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008982| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TSCO training\n",
      "Epoch[1/100] | loss train:0.067604| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014169| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013866| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013000| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013712| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010521| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010889| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011817| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010899| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011464| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010954| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010133| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011256| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011092| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011689| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012071| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010048| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011239| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009322| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009416| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010603| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010183| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010377| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009561| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011507| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009475| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009115| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011000| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010775| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009419| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009764| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009378| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009937| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009738| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009382| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010543| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010479| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TT training\n",
      "Epoch[1/100] | loss train:0.082453| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015333| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012332| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012757| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013868| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010726| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010200| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010879| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009958| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010738| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010560| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013292| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009184| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011826| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010356| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009661| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011966| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009410| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009496| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010301| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010174| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008815| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009457| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008914| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009614| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010447| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009843| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008347| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010310| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009666| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008353| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009517| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008839| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009496| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008378| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 4292 from 2006-03-15 to 2023-03-31\n",
      "TDG training\n",
      "Epoch[1/100] | loss train:0.047415| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009936| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.009492| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008360| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.008520| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.008740| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009452| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006615| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008572| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007898| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007977| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.007527| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/100] | loss train:0.008764| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007851| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007557| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007509| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.007590| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007623| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TRV training\n",
      "Epoch[1/100] | loss train:0.075146| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014975| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013905| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012016| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010994| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010904| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010274| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010934| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009635| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009474| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.008929| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009537| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009965| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009932| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009736| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009447| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009655| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010051| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009379| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008826| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009488| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009737| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009407| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009865| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009273| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009264| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010289| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009994| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009343| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009178| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TRMB training\n",
      "Epoch[1/100] | loss train:0.085065| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.020064| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015356| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.017682| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014038| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013467| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013848| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.015642| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.016669| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013879| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011300| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013885| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013899| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013258| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011798| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011303| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011524| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012348| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010654| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.012323| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012499| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011853| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.012762| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.012224| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012778| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011784| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011342| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.012566| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011423| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TFC training\n",
      "Epoch[1/100] | loss train:0.062432| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016124| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014975| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.016114| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012767| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014856| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012385| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013448| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012718| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012182| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012853| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012565| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013184| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012871| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012474| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013112| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012898| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013428| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012733| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013049| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TYL training\n",
      "Epoch[1/100] | loss train:0.060452| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013692| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011937| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015506| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013990| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011892| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011611| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012363| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011623| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010286| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010051| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009356| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010984| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010175| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010150| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011025| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010100| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009039| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009193| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010548| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010822| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010316| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009400| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010296| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009440| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010213| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010115| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009381| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "TSN training\n",
      "Epoch[1/100] | loss train:0.064443| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012970| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013485| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011023| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010461| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010869| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010093| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010552| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010171| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009620| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010255| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010413| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009520| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011212| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009908| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009129| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010248| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009957| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010414| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010599| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009184| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009880| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008797| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009617| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009791| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009785| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009326| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009871| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009649| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009709| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009538| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008703| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008287| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008776| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008943| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009541| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008952| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007808| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007690| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007968| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008057| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007818| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007589| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/100] | loss train:0.007684| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007750| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007572| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007436| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007463| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007304| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007637| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007519| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007609| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007677| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007578| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007558| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007631| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007454| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007642| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007568| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "USB training\n",
      "Epoch[1/100] | loss train:0.078811| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014166| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014750| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011592| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011026| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011281| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011347| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010755| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011108| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011378| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011219| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011374| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011069| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009742| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010413| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011110| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010190| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010074| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010665| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011643| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009275| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010305| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009799| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009604| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010687| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010396| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010090| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010382| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009797| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "UDR training\n",
      "Epoch[1/100] | loss train:0.067890| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013980| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014325| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012141| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013444| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010989| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010786| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012030| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010177| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010208| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011199| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009550| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010784| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009646| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010350| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010685| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010530| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009392| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010074| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009613| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009842| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010033| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009811| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009679| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009751| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010080| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009389| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009717| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009360| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010156| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009436| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008755| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009024| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008814| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009186| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009489| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008512| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009576| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010007| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007703| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007895| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007178| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007581| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007218| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007585| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007041| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007009| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007132| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007488| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007410| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007319| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007616| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006933| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007518| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007388| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007333| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006770| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007447| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007678| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007001| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007562| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007484| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007050| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007057| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007038| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007109| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007311| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3885 from 2007-10-25 to 2023-03-31\n",
      "ULTA training\n",
      "Epoch[1/100] | loss train:0.076522| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012202| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011350| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009529| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009297| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010696| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007622| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008419| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.008991| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.007727| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.007597| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008964| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.007254| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.007089| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.007166| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.007637| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009084| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.007178| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006593| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.007293| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.007390| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007443| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.007732| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.007444| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.007306| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.007437| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010474| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007371| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008011| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "UNP training\n",
      "Epoch[1/100] | loss train:0.064054| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013918| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012502| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011109| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010612| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009841| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011542| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010646| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009246| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010187| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009513| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008943| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009357| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009276| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009230| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010416| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010041| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/100] | loss train:0.009532| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008716| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009804| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008733| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009036| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009160| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008827| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009638| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008666| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009007| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008343| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008531| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008591| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009177| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009251| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009186| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009290| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009236| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.009322| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008263| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008333| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007339| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007044| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007323| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006905| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006615| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006640| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006651| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006730| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007237| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006765| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007119| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006761| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006764| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006997| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006977| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4318 from 2006-02-06 to 2023-03-31\n",
      "UAL training\n",
      "Epoch[1/100] | loss train:0.069769| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015336| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014472| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011259| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010511| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013021| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010601| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010152| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010100| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010292| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011286| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009515| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009886| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010148| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010374| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010649| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009649| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010276| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009414| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009652| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009555| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009395| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009146| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009805| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009291| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009449| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008541| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008958| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010251| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009324| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009670| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009222| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008919| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008998| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008632| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007799| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008323| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007604| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007971| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008046| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007606| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007884| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008012| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007866| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007707| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007835| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007972| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007270| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007909| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007950| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007833| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007747| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008057| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007792| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007698| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007966| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007682| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007671| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5885 from 1999-11-10 to 2023-03-31\n",
      "UPS training\n",
      "Epoch[1/100] | loss train:0.063030| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015457| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015126| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015712| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011530| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011284| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012181| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.013408| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012151| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011461| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010528| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010264| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010286| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010101| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010495| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011838| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009578| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010239| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010719| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009624| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010970| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010421| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010404| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010934| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011862| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009543| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009381| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010452| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010320| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010969| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010000| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009949| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009939| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010082| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010084| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009759| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010086| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "URI training\n",
      "Epoch[1/100] | loss train:0.064649| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013913| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013452| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011799| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011761| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013435| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011858| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011771| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012196| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011287| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010217| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010567| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012120| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011876| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009775| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010964| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011932| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011109| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012740| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010363| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009800| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010732| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010870| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010344| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009953| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "UNH training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] | loss train:0.052163| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013877| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013202| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013438| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012130| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009833| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011129| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010636| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011422| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010335| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009866| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010340| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010508| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010203| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010478| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010485| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "UHS training\n",
      "Epoch[1/100] | loss train:0.070426| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014456| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013278| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011758| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011158| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010215| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011132| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009931| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010713| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009646| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009770| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009686| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009399| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008981| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009374| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009537| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009309| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009284| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009470| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009131| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009984| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009176| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009124| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008893| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009248| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010159| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008712| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009202| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009483| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009934| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009124| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008809| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009257| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008976| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009417| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009148| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008778| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "VLO training\n",
      "Epoch[1/100] | loss train:0.087400| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017431| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016207| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013861| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014183| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014729| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011278| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012559| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012365| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012426| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011663| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012795| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013150| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012419| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011818| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012621| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011191| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010628| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011862| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011074| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010742| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012731| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010429| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011753| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011564| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011288| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010491| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013585| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014274| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011582| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011420| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012331| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010988| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "VTR training\n",
      "Epoch[1/100] | loss train:0.069169| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013364| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011676| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011651| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010815| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011276| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009483| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010131| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009825| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009669| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009718| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009503| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010262| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009451| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009740| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008644| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008851| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009849| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009186| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009441| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009126| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009157| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009431| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008668| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009634| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009376| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "VRSN training\n",
      "Epoch[1/100] | loss train:0.062574| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013818| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013315| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012208| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012736| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010762| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012894| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010596| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010342| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010397| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011363| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010193| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010053| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010061| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010355| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010837| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011154| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009719| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010089| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011932| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010074| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010026| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009448| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010084| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009567| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010898| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009734| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009873| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009975| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009546| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010093| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009281| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010120| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009338| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008225| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008253| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008234| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008591| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008283| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008289| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007752| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008191| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008024| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[50/100] | loss train:0.008507| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007648| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007960| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007879| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008193| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007976| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008161| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008307| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007968| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008415| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007949| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007679| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3394 from 2009-10-07 to 2023-03-31\n",
      "VRSK training\n",
      "Epoch[1/100] | loss train:0.070145| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010459| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008435| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.008631| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007661| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006746| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.007414| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.007215| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006915| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006011| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006160| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006020| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006239| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006318| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006258| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.006006| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005864| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.006236| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005621| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.006247| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005723| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.006318| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005196| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005974| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005936| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005427| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005924| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005525| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.006202| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.005972| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005608| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005489| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.005825| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "VZ training\n",
      "Epoch[1/100] | loss train:0.077640| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015835| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013222| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012902| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010652| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011107| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010373| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010972| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011012| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010541| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009232| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010129| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009844| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010068| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010144| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008829| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009805| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009770| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010598| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008958| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009364| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008819| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008841| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009314| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009621| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009722| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008946| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009439| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009705| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009653| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008953| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009362| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "VRTX training\n",
      "Epoch[1/100] | loss train:0.076977| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014677| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013691| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014413| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013122| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014639| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011212| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010584| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012539| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011728| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010479| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011770| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011049| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010445| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011139| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010170| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009832| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012122| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010461| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010199| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011146| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009957| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011845| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010940| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010255| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009607| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010277| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009678| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009281| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009361| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010034| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010044| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009628| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010069| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011540| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010087| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009316| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010313| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009575| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "VFC training\n",
      "Epoch[1/100] | loss train:0.075889| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015548| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012279| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011886| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010962| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010476| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009509| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010959| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009934| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009731| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010032| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009626| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010487| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010073| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009367| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009497| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009220| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009123| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009483| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008607| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009671| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010373| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009365| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008780| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008937| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008232| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008848| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008662| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008795| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008850| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008867| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008543| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008713| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009116| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008840| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 597 from 2020-11-16 to 2023-03-31\n",
      "VTRS training\n",
      "Epoch[1/100] | loss train:0.042449| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.012611| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008625| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006908| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.004622| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004304| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004786| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/100] | loss train:0.004588| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004001| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004150| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004183| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004127| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004009| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.003775| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.003809| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.003694| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.003532| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003536| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003700| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003809| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003777| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003417| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.003757| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003957| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004397| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003982| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.003680| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.003392| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.002907| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004067| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.004141| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004160| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003780| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.003560| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.003658| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003880| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.003438| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.003430| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.003542| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1373 from 2017-10-17 to 2023-03-31\n",
      "VICI training\n",
      "Epoch[1/100] | loss train:0.047625| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.008640| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006518| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.005366| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.004678| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.004836| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004272| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.004657| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004473| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004508| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005068| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.004528| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004317| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004368| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004791| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004667| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.003522| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.003749| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.003863| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003809| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.003984| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.003498| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004316| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.003528| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.003794| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.003264| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.003419| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.003762| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.003803| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.003999| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.003738| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.004116| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.003931| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.004182| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.003327| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.003091| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.003258| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.003566| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.003523| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.003572| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.003100| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.003091| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.003082| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.003176| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.003165| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.003107| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.002992| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.002922| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.002934| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.002845| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.003147| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.003096| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.002843| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.002943| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.003040| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.002978| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.002868| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.002922| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.002866| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.002566| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.002993| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.002712| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.002892| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.002754| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.002526| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.002808| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.002916| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.002937| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.002731| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.002991| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.002802| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.002933| lr:0.001000\n",
      "Epoch[73/100] | loss train:0.002628| lr:0.001000\n",
      "Epoch[74/100] | loss train:0.002905| lr:0.001000\n",
      "Epoch[75/100] | loss train:0.003136| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 3786 from 2008-03-19 to 2023-03-31\n",
      "V training\n",
      "Epoch[1/100] | loss train:0.051338| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.010500| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008656| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.007147| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006483| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.006401| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006340| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.006575| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.006119| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.005471| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.005852| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.006326| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.006221| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.005999| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006876| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005477| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.006290| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005933| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.006090| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005809| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "VMC training\n",
      "Epoch[1/100] | loss train:0.068076| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016947| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013535| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012824| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012545| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012185| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013184| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012554| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011069| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011502| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011861| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010855| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012131| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011673| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011944| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010732| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011378| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010696| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010273| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011556| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010850| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010011| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010700| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009964| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009685| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010125| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010548| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009663| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011000| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010032| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010604| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009835| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009889| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009748| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009548| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009896| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/100] | loss train:0.009662| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010002| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010155| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008842| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007991| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007846| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008392| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008356| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007911| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007903| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007919| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008094| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008416| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008073| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008469| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008091| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WAB training\n",
      "Epoch[1/100] | loss train:0.063909| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013568| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011258| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010230| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010333| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009464| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009820| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009282| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009492| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009806| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009523| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010328| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009264| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009790| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008865| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008667| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009063| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009152| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008444| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008653| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008666| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008591| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009040| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008764| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008486| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008059| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008380| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008922| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008537| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008639| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008357| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008349| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008998| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008522| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009283| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008476| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WBA training\n",
      "Epoch[1/100] | loss train:0.072449| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014356| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012611| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012549| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012312| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011362| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012033| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010920| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011325| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011021| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011783| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010559| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012287| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010253| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.012054| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010651| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010622| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010752| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010214| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011213| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010958| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011084| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010122| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011203| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010962| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010380| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010329| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010463| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010723| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010546| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010456| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010172| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011731| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WMT training\n",
      "Epoch[1/100] | loss train:0.055928| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015408| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011269| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010330| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010981| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011493| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010555| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011424| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010263| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009749| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009823| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011035| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009622| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009507| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009776| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010218| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010354| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009452| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009607| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009274| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009375| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009652| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009144| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009233| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008912| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010745| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009107| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008742| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008768| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008549| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009343| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009065| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.009036| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008764| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009363| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009181| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009058| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008942| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008387| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010132| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007751| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007252| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007508| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006972| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007306| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007421| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007541| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007247| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007076| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007124| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007338| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007215| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006863| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007003| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007258| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007210| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006928| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007286| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006847| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007140| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007224| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006463| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007207| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007323| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007386| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007148| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006973| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.006957| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.006938| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.006811| lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007227| lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007043| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 4462 from 2005-07-08 to 2023-03-31\n",
      "WBD training\n",
      "Epoch[1/100] | loss train:0.063586| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017281| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016853| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/100] | loss train:0.017321| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015701| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013937| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.015244| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012338| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011745| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013497| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.013395| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013217| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012264| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011886| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011841| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.013336| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012106| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013137| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011894| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WM training\n",
      "Epoch[1/100] | loss train:0.059915| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014547| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013340| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010017| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011000| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011183| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009684| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009461| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009022| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009190| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011450| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009139| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008796| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010400| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008419| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009688| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008705| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.009349| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009089| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009237| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008914| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009594| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009559| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008400| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009892| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008987| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007959| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.007977| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008192| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008386| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009024| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007838| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008792| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008373| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009280| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008691| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007766| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008050| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008339| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008847| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.006729| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007123| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006980| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006726| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006523| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006779| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.006582| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006689| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006770| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007112| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006628| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006615| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006382| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006468| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006362| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006555| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006688| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006465| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006402| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007102| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.006538| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.006689| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.006667| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.006536| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006757| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WAT training\n",
      "Epoch[1/100] | loss train:0.067812| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015713| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013566| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014302| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012859| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014248| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010588| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012934| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011676| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010623| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011924| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011514| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012386| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011194| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010596| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010143| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011042| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011472| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011611| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011349| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009494| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011659| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010568| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010806| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009374| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009926| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009422| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011186| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009980| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010020| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010267| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010286| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009554| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011066| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009736| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WEC training\n",
      "Epoch[1/100] | loss train:0.075592| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014213| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012630| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.009862| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009875| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011274| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010553| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.009029| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010178| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008828| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010074| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008837| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008483| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008498| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009006| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008430| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009165| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008732| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.008928| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008469| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008940| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009004| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008994| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008784| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008770| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008252| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008698| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008181| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.007898| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008499| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008092| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.007719| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008682| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008212| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008549| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008319| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.007937| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.008495| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.008079| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.008146| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007234| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.006738| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.006868| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.006641| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006484| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006993| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/100] | loss train:0.006814| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.006595| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006752| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.006382| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.006685| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.006789| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.006645| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.006864| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.006830| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.006758| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006748| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.006725| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.006692| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.006619| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WFC training\n",
      "Epoch[1/100] | loss train:0.071379| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016320| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012754| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012619| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011815| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012330| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011774| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012057| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011380| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010892| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011390| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011762| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011353| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011105| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011153| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010771| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011609| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010991| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011093| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010875| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010303| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010262| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010796| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010232| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009917| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010841| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010497| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010081| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010465| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010354| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009575| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010680| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010212| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010205| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010239| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010059| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009987| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010323| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010409| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009494| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009099| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008764| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.009038| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008615| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008557| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008828| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.009060| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008755| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008672| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008734| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008773| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008913| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008705| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008787| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.009010| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5585 from 2001-01-02 to 2023-03-31\n",
      "WELL training\n",
      "Epoch[1/100] | loss train:0.061005| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.013585| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.010755| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011160| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010621| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010751| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010768| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010320| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009334| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009734| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010671| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010146| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009031| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011772| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008895| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009268| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009543| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010216| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009086| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008889| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009379| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008555| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009401| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009605| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008486| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009015| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008898| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009153| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.009452| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008780| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009295| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.009708| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008905| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.009425| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009494| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WST training\n",
      "Epoch[1/100] | loss train:0.073376| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017770| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012996| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014691| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013381| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012668| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011318| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011863| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011964| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014295| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012647| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010359| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009963| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011411| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010922| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009766| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010686| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012726| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010073| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010069| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009529| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011026| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009973| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011429| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009142| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012222| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010205| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009816| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011039| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010010| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010238| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011119| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008411| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011845| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009650| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.009253| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009731| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011120| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009710| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010237| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007801| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007604| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007718| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.007747| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.007659| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007788| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007294| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007127| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.006845| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007312| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007419| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007602| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007290| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007027| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007022| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007746| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.006695| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.007320| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007608| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[60/100] | loss train:0.007059| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007822| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007692| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007605| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007583| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.006963| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.007232| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.006739| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WDC training\n",
      "Epoch[1/100] | loss train:0.066017| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017967| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015494| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013162| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012560| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012015| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011563| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011678| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010761| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012866| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012124| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011287| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011361| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010952| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010836| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011589| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010536| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011845| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011144| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011065| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010726| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011925| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.010504| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010408| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010243| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010741| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010723| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010763| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011007| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010111| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011301| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010696| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010122| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010698| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011006| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010525| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.009442| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010430| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009623| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.010169| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.008976| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008681| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008856| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008438| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008433| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.008799| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.008593| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008673| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008617| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008721| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.008522| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.008654| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.008363| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.008762| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008578| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.008476| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.008810| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008361| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008484| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.008241| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.008383| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.008355| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.008559| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.009094| lr:0.001000\n",
      "Epoch[65/100] | loss train:0.008734| lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008544| lr:0.001000\n",
      "Epoch[67/100] | loss train:0.008359| lr:0.001000\n",
      "Epoch[68/100] | loss train:0.008816| lr:0.001000\n",
      "Epoch[69/100] | loss train:0.008784| lr:0.001000\n",
      "Epoch[70/100] | loss train:0.008420| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 1957 from 2015-06-24 to 2023-03-31\n",
      "WRK training\n",
      "Epoch[1/100] | loss train:0.069483| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016051| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012474| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014680| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012624| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012054| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014149| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014568| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.015314| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.013705| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010907| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011472| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010688| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010500| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010318| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010745| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.013193| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012921| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012782| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011333| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.025186| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010546| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.023121| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.019088| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010470| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WY training\n",
      "Epoch[1/100] | loss train:0.075035| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015566| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014249| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012334| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.013134| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.011999| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013052| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012589| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011897| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012212| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012283| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012743| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011985| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012158| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.013056| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011026| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012899| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011771| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.010914| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011051| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010459| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011741| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011142| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011345| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012669| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010917| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011261| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011137| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011879| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.010240| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012257| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011024| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011221| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011398| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.011655| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011861| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.011286| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012298| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.010374| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.011522| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WHR training\n",
      "Epoch[1/100] | loss train:0.072240| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015445| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.014587| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014681| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011979| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010245| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011503| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012069| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011430| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011455| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010457| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010577| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010339| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011692| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010662| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011293| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "WMB training\n",
      "Epoch[1/100] | loss train:0.060916| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2/100] | loss train:0.017145| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015196| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015076| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014378| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012847| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.013697| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.012728| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011636| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012198| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.012856| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012479| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012614| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012483| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010921| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011086| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011947| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012111| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012084| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.011596| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.011910| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010772| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011382| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011637| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011397| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012069| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011776| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011210| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.011814| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011420| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011284| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011002| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 1823 from 2016-01-05 to 2023-03-31\n",
      "WTW training\n",
      "Epoch[1/100] | loss train:0.067019| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.030463| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.007436| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.019900| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.005621| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.025267| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.008054| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.022777| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.021088| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006978| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004995| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005057| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011489| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.013352| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014415| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005306| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.020141| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005202| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013792| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013063| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004345| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.009471| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005012| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.009381| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005181| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008510| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.011175| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.006818| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.021909| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.017250| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.006722| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "GWW training\n",
      "Epoch[1/100] | loss train:0.068265| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.019721| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.016531| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.013046| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011870| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010526| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012207| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011787| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.011854| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010618| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011067| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011426| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012999| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011248| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010789| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010046| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012540| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.011324| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011356| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010736| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.010241| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011847| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009863| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010661| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011664| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009844| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009553| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.009921| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010609| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011520| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.010287| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010183| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010227| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010009| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.009344| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.010097| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.010477| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.010081| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.009479| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.009680| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.007880| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.007755| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.007395| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008149| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.006979| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.006903| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007391| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.007610| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.007354| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.007262| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007049| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007120| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007320| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007132| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.007130| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007073| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 5143 from 2002-10-25 to 2023-03-31\n",
      "WYNN training\n",
      "Epoch[1/100] | loss train:0.077034| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017233| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.013971| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015153| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.014551| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012246| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012410| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011562| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012673| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012459| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.011786| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.012031| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.012952| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011640| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011242| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.012012| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.011334| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012537| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.011404| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.010530| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012378| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.011231| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.011666| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010778| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012045| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010182| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.010278| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.011163| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010720| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011201| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011027| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.011192| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.011747| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.010823| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012257| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011642| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "XEL training\n",
      "Epoch[1/100] | loss train:0.052554| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.014159| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012126| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011618| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.010061| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009671| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011107| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008821| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/100] | loss train:0.009489| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.008537| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009903| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.009427| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.009026| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010059| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.009262| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009154| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008695| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008845| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009527| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008186| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009947| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.008422| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009627| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008127| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.009069| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008524| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.007947| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008978| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008101| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009617| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008666| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008569| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.008193| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.008579| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.008525| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.008763| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.008298| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 2885 from 2011-10-13 to 2023-03-31\n",
      "XYL training\n",
      "Epoch[1/100] | loss train:0.037001| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.007688| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.008012| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006581| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.007704| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.007159| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.006290| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008300| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.005721| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.006404| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.006225| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005327| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.005678| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.006105| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.006545| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.005682| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005769| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.005345| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.005457| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.005033| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.005580| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004964| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.005359| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.005963| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.005071| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.005327| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.005180| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.005009| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.005660| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.006054| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.005065| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.005456| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "YUM training\n",
      "Epoch[1/100] | loss train:0.050837| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.011983| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.011534| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.010183| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.009485| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.009845| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.009744| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.008963| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.009506| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.009911| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.009141| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.008949| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.008366| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.008754| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.008939| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.009077| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.008898| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.008842| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.007986| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.008158| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.008145| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.007948| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.008540| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.008831| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.008289| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.008035| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.008585| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.008200| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.008843| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.008614| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.008335| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.008165| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ZBRA training\n",
      "Epoch[1/100] | loss train:0.101486| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.017973| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.021227| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014065| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.016714| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014757| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.011957| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014186| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.012531| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.012385| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.015824| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.013703| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.014723| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.011377| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.011619| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.011979| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.012484| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010654| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.012232| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013179| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.012852| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013187| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014216| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.011556| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012867| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.009982| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013496| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010089| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010830| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.011179| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.011747| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012153| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.013758| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011426| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.010578| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.011563| lr:0.010000\n",
      "Early stopping.\n",
      "Number data points 5456 from 2001-07-25 to 2023-03-31\n",
      "ZBH training\n",
      "Epoch[1/100] | loss train:0.066016| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015692| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012711| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.012828| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.012095| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.012688| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012126| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.011362| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010595| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.011462| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010344| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.010942| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.011573| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.010010| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010145| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.010744| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.009819| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.010258| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009785| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009591| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009848| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010115| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009447| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010156| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.010031| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.010550| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.009573| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.010917| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.010262| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.009608| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.009650| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.010029| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.010321| lr:0.010000\n",
      "Early stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5892 from 1999-11-01 to 2023-03-31\n",
      "ZION training\n",
      "Epoch[1/100] | loss train:0.070256| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.016726| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.015300| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.014830| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.015252| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.013425| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.014977| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.014787| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.013546| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.014386| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.014236| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.014198| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.013357| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.012813| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014623| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.014279| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.014267| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.013484| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013663| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.013496| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.013046| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.012748| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.013394| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013698| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013432| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.012675| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.012419| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013246| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.012240| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.012775| lr:0.010000\n",
      "Epoch[31/100] | loss train:0.012773| lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012746| lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012650| lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012320| lr:0.010000\n",
      "Epoch[35/100] | loss train:0.012421| lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012509| lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012897| lr:0.010000\n",
      "Epoch[38/100] | loss train:0.012118| lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012600| lr:0.010000\n",
      "Epoch[40/100] | loss train:0.012569| lr:0.010000\n",
      "Epoch[41/100] | loss train:0.011010| lr:0.001000\n",
      "Epoch[42/100] | loss train:0.011065| lr:0.001000\n",
      "Epoch[43/100] | loss train:0.010936| lr:0.001000\n",
      "Epoch[44/100] | loss train:0.011230| lr:0.001000\n",
      "Epoch[45/100] | loss train:0.010937| lr:0.001000\n",
      "Epoch[46/100] | loss train:0.010776| lr:0.001000\n",
      "Epoch[47/100] | loss train:0.010977| lr:0.001000\n",
      "Epoch[48/100] | loss train:0.010652| lr:0.001000\n",
      "Epoch[49/100] | loss train:0.010715| lr:0.001000\n",
      "Epoch[50/100] | loss train:0.010880| lr:0.001000\n",
      "Epoch[51/100] | loss train:0.010659| lr:0.001000\n",
      "Epoch[52/100] | loss train:0.010485| lr:0.001000\n",
      "Epoch[53/100] | loss train:0.010450| lr:0.001000\n",
      "Epoch[54/100] | loss train:0.010229| lr:0.001000\n",
      "Epoch[55/100] | loss train:0.010623| lr:0.001000\n",
      "Epoch[56/100] | loss train:0.010729| lr:0.001000\n",
      "Epoch[57/100] | loss train:0.010459| lr:0.001000\n",
      "Epoch[58/100] | loss train:0.010687| lr:0.001000\n",
      "Epoch[59/100] | loss train:0.010587| lr:0.001000\n",
      "Epoch[60/100] | loss train:0.010278| lr:0.001000\n",
      "Epoch[61/100] | loss train:0.010720| lr:0.001000\n",
      "Epoch[62/100] | loss train:0.010737| lr:0.001000\n",
      "Epoch[63/100] | loss train:0.010460| lr:0.001000\n",
      "Epoch[64/100] | loss train:0.010237| lr:0.001000\n",
      "Early stopping.\n",
      "Number data points 2559 from 2013-02-01 to 2023-03-31\n",
      "ZTS training\n",
      "Epoch[1/100] | loss train:0.061221| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.009315| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.006678| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.006630| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.006218| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.005753| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.004474| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.005938| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.004442| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.004774| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.004435| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.005199| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.004582| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.004119| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.004138| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.004172| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.005131| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.004323| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.004905| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.003992| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.004333| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.004051| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.004196| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.004451| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.004161| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.004287| lr:0.010000\n",
      "Epoch[27/100] | loss train:0.004020| lr:0.010000\n",
      "Epoch[28/100] | loss train:0.004471| lr:0.010000\n",
      "Epoch[29/100] | loss train:0.004102| lr:0.010000\n",
      "Epoch[30/100] | loss train:0.004166| lr:0.010000\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# API call limit per minute\n",
    "limit = 5\n",
    "\n",
    "check = time.time()\n",
    "\n",
    "for i,ticker in enumerate(tickers):\n",
    "    \n",
    "    # account for API calls per minute\n",
    "    if not i + 1 % limit:\n",
    "        sleep = check + 60 - time.time()\n",
    "        if sleep > 0:\n",
    "            time.sleep(sleep)\n",
    "        check = time.time()      \n",
    "    \n",
    "    date_data, close_price_data, num_data_points, display_date_range = get_data(config, ticker)\n",
    "    \n",
    "    scaler = Normalization()\n",
    "    normalized_close_price_data = scaler.fit_transform(close_price_data)\n",
    "    \n",
    "    data_x, data_x_unseen = prepare_data_x(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    data_y = prepare_data_y(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    \n",
    "    split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "    model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    n_epochs_stop = config[\"training\"][\"epoch_stop\"]\n",
    "    \n",
    "    print('{} training'.format(ticker))\n",
    "    for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "        loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch[{}/{}] | loss train:{:.6f}| lr:{:.6f}'\n",
    "                  .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, lr_train))\n",
    "        if loss_train < best_loss:\n",
    "            best_loss = loss_train\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    torch.tensor(data_x_unseen)\n",
    "    x = torch.tensor(data_x_unseen).float().to(config[\"training\"][\"device\"]).unsqueeze(0).unsqueeze(2) # this is the data type and shape required, [batch, sequence, feature]\n",
    "    prediction = model(x)\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "    prediction[0] = scaler.inverse_transform(prediction[0])\n",
    "    \n",
    "    df = pd.DataFrame([[next_day,prediction[0]]], columns = ['date','close price']) \n",
    "    path = \"csv/TimeSeries/\" + ticker + \"_predict.csv\"\n",
    "    df.to_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41321d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished in 12856.391211271286 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Program finished in {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17768cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(columns = ['date','ticker','close price'])\n",
    "\n",
    "for ticker in tickers:\n",
    "    path = \"csv/TimeSeries/\" + ticker + \"_predict.csv\"\n",
    "    df1 = pd.read_csv(path)\n",
    "    data = {'date': [df1['date'][0]],\n",
    "            'ticker': [ticker],\n",
    "            'close price': [df1['close price'][0]]}\n",
    "    df2 = pd.DataFrame(data)\n",
    "    predictions = pd.concat([predictions,df2], ignore_index=True)\n",
    "    \n",
    "predictions.to_csv(\"csv/predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
