{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37e13e9",
   "metadata": {},
   "source": [
    "This notebook explores a machine learning algorithm to predict the stock prices of SPY, the S&P 500 ETF, and is intended to utilize functions that can be easily translated to a python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01bcfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: alpha_vantage in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from alpha_vantage) (2.21.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from alpha_vantage) (3.8.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->alpha_vantage) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->alpha_vantage) (3.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->alpha_vantage) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->alpha_vantage) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->alpha_vantage) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->alpha_vantage) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->alpha_vantage) (4.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->alpha_vantage) (2022.12.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->alpha_vantage) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->alpha_vantage) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->alpha_vantage) (3.0.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn) (1.23.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_market_calendars in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (4.1.4)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas_market_calendars) (1.5.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas_market_calendars) (2022.7.1)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas_market_calendars) (4.2.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas_market_calendars) (2.8.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
      "Requirement already satisfied: pyluach in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.23.5)\n",
      "Requirement already satisfied: korean-lunar-calendar in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\john bernardin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install alpha_vantage\n",
    "!pip install scikit-learn\n",
    "!pip install pandas_market_calendars\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26129c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas_market_calendars as mcal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb7b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file (placing here for now, some fields will change on later impplementations)\n",
    "\n",
    "config = {\n",
    "    \"alpha_vantage\": {\n",
    "        \"key\": \"2JMCN347HZ3BU9RC\", \n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"key_adjusted_close\": \"5. adjusted close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 30,\n",
    "        \"train_split_size\": 1,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"xticks_interval\": 90, # show a date every 90 days\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since for now we are only using close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\",\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epoch\": 50,\n",
    "        \"epoch_stop\": 10,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c35333f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "# tickers = np.array(sp500[0]['Symbol'])\n",
    "# if ('BF.B' in tickers)\n",
    "#     tickers[tickers.index('BF.B')] = 'BF-B'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tickers = ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ATVI', 'ADM', 'ADBE', 'ADP',\n",
    "       'AAP', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE',\n",
    "       'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN',\n",
    "       'AMCR', 'AMD', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK',\n",
    "       'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA',\n",
    "       'AAPL', 'AMAT', 'APTV', 'ACGL', 'ANET', 'AJG', 'AIZ', 'T', 'ATO',\n",
    "       'ADSK', 'AZO', 'AVB', 'AVY', 'BKR', 'BALL', 'BAC', 'BBWI', 'BAX',\n",
    "       'BDX', 'WRB', 'BRK.B', 'BBY', 'BIO', 'TECH', 'BIIB', 'BLK', 'BK',\n",
    "       'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO',\n",
    "       'BF-B', 'BG', 'CHRW', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH',\n",
    "       'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE',\n",
    "       'CNC', 'CNP', 'CDAY', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG',\n",
    "       'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX',\n",
    "       'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP',\n",
    "       'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CTVA', 'CSGP', 'COST',\n",
    "       'CTRA', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA',\n",
    "       'DE', 'DAL', 'XRAY', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DISH',\n",
    "       'DIS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DTE', 'DUK', 'DD',\n",
    "       'DXC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV',\n",
    "       'LLY', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EQT', 'EFX', 'EQIX',\n",
    "       'EQR', 'ESS', 'EL', 'ETSY', 'RE', 'EVRG', 'ES', 'EXC', 'EXPE',\n",
    "       'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FICO', 'FAST', 'FRT', 'FDX',\n",
    "       'FITB', 'FRC', 'FSLR', 'FE', 'FIS', 'FISV', 'FLT', 'FMC', 'F',\n",
    "       'FTNT', 'FTV', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GEHC',\n",
    "       'GEN', 'GNRC', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GL', 'GPN',\n",
    "       'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'PEAK', 'HSIC', 'HSY', 'HES',\n",
    "       'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ',\n",
    "       'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'ILMN', 'INCY',\n",
    "       'IR', 'PODD', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG',\n",
    "       'IVZ', 'INVH', 'IQV', 'IRM', 'JBHT', 'JKHY', 'J', 'JNJ', 'JCI',\n",
    "       'JPM', 'JNPR', 'K', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI',\n",
    "       'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS',\n",
    "       'LEN', 'LNC', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB',\n",
    "       'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH',\n",
    "       'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM',\n",
    "       'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ',\n",
    "       'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP',\n",
    "       'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'NDSN',\n",
    "       'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI',\n",
    "       'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OGN', 'OTIS',\n",
    "       'PCAR', 'PKG', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PNR', 'PEP',\n",
    "       'PKI', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'POOL',\n",
    "       'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC',\n",
    "       'PSA', 'PHM', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX',\n",
    "       'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP',\n",
    "       'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE',\n",
    "       'NOW', 'SHW', 'SPG', 'SWKS', 'SJM', 'SNA', 'SEDG', 'SO', 'LUV',\n",
    "       'SWK', 'SBUX', 'STT', 'STLD', 'STE', 'SYK', 'SYF', 'SNPS', 'SYY',\n",
    "       'TMUS', 'TROW', 'TTWO', 'TPR', 'TRGP', 'TGT', 'TEL', 'TDY', 'TFX',\n",
    "       'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG',\n",
    "       'TRV', 'TRMB', 'TFC', 'TYL', 'TSN', 'USB', 'UDR', 'ULTA', 'UNP',\n",
    "       'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VRSN', 'VRSK',\n",
    "       'VZ', 'VRTX', 'VFC', 'VTRS', 'VICI', 'V', 'VMC', 'WAB', 'WBA',\n",
    "       'WMT', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC',\n",
    "       'WRK', 'WY', 'WHR', 'WMB', 'WTW', 'GWW', 'WYNN', 'XEL', 'XYL',\n",
    "       'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474520c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to account for API limitations (on the weekend)\n",
    "\n",
    "# index = tickers.index('L')\n",
    "# tickers = tickers[(index+1):]\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5cf266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-04-12'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "next_weeks = mcal.date_range(mcal.get_calendar('NYSE').schedule(start_date=today, end_date=(today+relativedelta(months=1))), frequency='1D')\n",
    "next_weeks = [date.strftime('%Y-%m-%d') for date in next_weeks]\n",
    "next_weeks = next_weeks[1:]\n",
    "next_day = next_weeks[0]\n",
    "next_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77bcc8",
   "metadata": {},
   "source": [
    "# Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671e4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from the configuration file\n",
    "def get_data(config, ticker):\n",
    "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"]) \n",
    "    data, meta_data = ts.get_daily_adjusted(ticker, outputsize=config[\"alpha_vantage\"][\"outputsize\"])\n",
    "\n",
    "    date_data = [date for date in data.keys()]\n",
    "    date_data.reverse()\n",
    "\n",
    "    close_price_data = [float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]]) for date in data.keys()]\n",
    "    close_price_data.reverse()\n",
    "    close_price_data = np.array(close_price_data)\n",
    "\n",
    "    num_data_points = len(date_data)\n",
    "    display_date_range = \"from \" + date_data[0] + \" to \" + date_data[num_data_points-1]\n",
    "    print(\"Number data points\", num_data_points, display_date_range)\n",
    "\n",
    "    return date_data, close_price_data, num_data_points, display_date_range\n",
    "\n",
    "class Normalization():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "    \n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    # use the next day as label\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "# Class to prepare data for training and LSTM model\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # right now we have only 1 feature, so we need to convert `x` into [batch, sequence, features]\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "# neural network model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        \n",
    "        # layer 2\n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "    \n",
    "# function for training LSTM model\n",
    "def run_epoch(dataloader, is_training=False):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        y = y.to(config[\"training\"][\"device\"])\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += (loss.detach().item() / batchsize)\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    return epoch_loss, lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab08e8",
   "metadata": {},
   "source": [
    "## Predict the Next Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9665520a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MMM training\n",
      "Epoch[1/50] | loss train:0.069969| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014881| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012386| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011412| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012905| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010018| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009381| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011195| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009559| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009274| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009353| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009010| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009488| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010056| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009801| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008470| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009286| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008665| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009389| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009271| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009296| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008552| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008575| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009081| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008686| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008849| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008283| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008673| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009368| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008819| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008384| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008364| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008452| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008895| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008824| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008455| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008176| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008567| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008930| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007668| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006942| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007124| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007012| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006963| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007031| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007410| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006864| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007027| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007174| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AOS training\n",
      "Epoch[1/50] | loss train:0.064365| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012315| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011003| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009493| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009920| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009360| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009184| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010312| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008999| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009976| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008687| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009591| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008944| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009502| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009492| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010101| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009028| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009499| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009356| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008313| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008464| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008910| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009329| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008719| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008506| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008316| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008630| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008686| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008480| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008685| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009107| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007739| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008823| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008509| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008298| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007927| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008511| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008390| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008431| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007762| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007523| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007600| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006930| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007194| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007067| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006958| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006987| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006665| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007101| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006743| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ABT training\n",
      "Epoch[1/50] | loss train:0.062306| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014970| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016383| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010785| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012543| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010553| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009927| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011108| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011524| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010292| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010151| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010060| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009323| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008891| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009485| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009907| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009936| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010115| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008912| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008960| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009474| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008663| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008769| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009545| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009085| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009945| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008384| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009977| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008373| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008903| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009466| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009227| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008545| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008771| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010442| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008051| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008851| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007857| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007221| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007368| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006878| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007179| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007297| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007111| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007082| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007253| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006576| lr:0.001000\n",
      "Number data points 2586 from 2013-01-02 to 2023-04-11\n",
      "ABBV training\n",
      "Epoch[1/50] | loss train:0.055775| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.007770| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007152| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006655| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005150| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005071| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006078| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006147| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005547| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005407| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005608| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005264| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005404| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005717| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005164| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004925| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/50] | loss train:0.004369| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005590| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004902| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004978| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005248| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004427| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004493| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004554| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004792| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004584| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004565| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004891| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004160| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004206| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004554| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005025| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006256| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004713| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004068| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004165| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004092| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004758| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004532| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003958| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003973| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003857| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003492| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003487| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003602| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003874| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003567| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003545| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003527| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003205| lr:0.001000\n",
      "Number data points 5466 from 2001-07-19 to 2023-04-11\n",
      "ACN training\n",
      "Epoch[1/50] | loss train:0.063398| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016019| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013482| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013421| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010971| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011332| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011685| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010031| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011821| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010407| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011041| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011335| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014117| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010881| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010375| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010970| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010108| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010957| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009836| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010156| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009516| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008945| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009491| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009759| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010713| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009310| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010622| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009651| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009678| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009524| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009022| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009355| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009199| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010160| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008233| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009540| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009101| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009875| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008915| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007242| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008118| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007589| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006946| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007862| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006730| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007104| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007090| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006802| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007349| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ATVI training\n",
      "Epoch[1/50] | loss train:0.068563| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013162| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011038| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011866| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010486| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010071| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009346| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009967| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010402| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012133| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008502| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009216| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009234| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008824| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008629| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009172| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009072| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008709| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009175| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008506| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009159| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009231| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008985| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008470| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008526| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008732| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008365| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009386| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009408| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008099| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008782| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009019| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009077| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008206| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008280| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007933| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007774| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007430| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007275| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007257| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007151| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006836| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006914| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006865| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007030| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006803| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ADM training\n",
      "Epoch[1/50] | loss train:0.059999| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014955| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014864| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014288| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013494| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012022| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012618| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012660| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012047| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013707| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010875| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013189| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010901| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011024| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011793| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011130| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010559| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010705| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010896| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012086| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012321| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010567| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011590| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011561| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009967| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012314| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009959| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010232| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012121| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010744| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009674| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009563| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010159| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/50] | loss train:0.009487| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009826| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011619| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011795| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010426| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010437| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008591| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007959| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007753| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007804| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007855| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007344| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007720| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007374| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007159| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007696| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ADBE training\n",
      "Epoch[1/50] | loss train:0.069146| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015342| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013493| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015853| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013027| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011368| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011682| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012129| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012090| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012430| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010492| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010764| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012415| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009886| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011576| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011214| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010774| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012355| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010388| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011117| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011432| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010054| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011181| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009720| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010680| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010486| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009979| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011092| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010266| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010754| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010151| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010954| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011066| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011346| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011092| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009919| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010115| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009276| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010299| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008282| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008737| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008190| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007867| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008055| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007423| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007553| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008059| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007596| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007359| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ADP training\n",
      "Epoch[1/50] | loss train:0.060683| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014981| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012667| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012104| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011989| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010416| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012079| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011005| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009574| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010542| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010680| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010248| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010701| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010000| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009612| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009492| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009658| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011033| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010501| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009973| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009740| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008700| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009239| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010816| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009839| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009290| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008489| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008385| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008820| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009149| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009068| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009653| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009015| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009556| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009261| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008653| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008159| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008756| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007972| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009083| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007954| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006815| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007102| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007180| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007453| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007097| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007145| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007230| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007132| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006690| lr:0.001000\n",
      "Number data points 5377 from 2001-11-29 to 2023-04-11\n",
      "AAP training\n",
      "Epoch[1/50] | loss train:0.065224| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015058| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012576| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010914| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010268| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011222| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010586| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009480| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009017| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008620| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009139| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009548| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009803| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009311| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010258| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009252| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008791| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009631| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010213| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008960| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009325| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008730| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008681| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009095| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008219| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008607| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008586| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009153| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008921| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008490| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008908| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008294| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008535| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008755| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008411| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008936| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008421| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007649| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007163| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006993| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007247| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007245| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007023| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007170| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006999| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007041| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AES training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] | loss train:0.070596| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020252| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018669| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016366| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.016298| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015387| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015738| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.016795| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.016089| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015515| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014696| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013947| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014115| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015557| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014628| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014674| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013351| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014555| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.015536| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013454| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.015425| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014438| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013427| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013338| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013852| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013974| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013429| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.015246| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014168| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013744| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013664| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013237| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012713| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.014983| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012906| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.014307| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.014486| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011813| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013724| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013366| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011541| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011051| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011664| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010932| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011125| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010075| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010726| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010165| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010055| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010193| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AFL training\n",
      "Epoch[1/50] | loss train:0.074471| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020034| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013907| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013660| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013707| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012021| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010751| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012412| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011700| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010827| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011001| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011012| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010154| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013567| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011166| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010706| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010241| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011288| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010467| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010227| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010292| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010407| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009563| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011534| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008823| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011149| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011393| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009658| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010322| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010707| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009461| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009550| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009434| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010064| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009887| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009402| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009020| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010009| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009942| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007952| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007868| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007655| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007616| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008143| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008126| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007859| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007769| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007585| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007443| lr:0.001000\n",
      "Number data points 5885 from 1999-11-18 to 2023-04-11\n",
      "A training\n",
      "Epoch[1/50] | loss train:0.074984| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019011| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017097| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014955| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012461| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015439| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011975| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013028| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011503| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012335| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012238| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011496| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011978| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013036| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011971| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012025| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011298| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011429| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010797| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012551| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010619| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011703| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010877| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010545| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011935| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011337| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009946| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009964| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010722| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011266| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011818| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011016| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010092| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010419| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010020| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011005| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011227| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011044| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009653| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011334| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008237| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008462| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008403| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008335| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007761| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008292| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008310| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008165| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008228| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007604| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "APD training\n",
      "Epoch[1/50] | loss train:0.056765| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016095| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011668| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010856| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011046| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011725| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010807| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010430| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009929| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010356| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010442| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010315| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008956| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010644| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009480| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009717| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009263| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/50] | loss train:0.010817| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010205| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008902| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009480| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009244| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009539| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008605| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011356| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008857| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008659| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009042| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008776| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009506| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009256| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008724| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009174| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009263| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009633| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008372| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009365| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007616| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007495| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007625| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007254| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007183| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007170| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007258| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007204| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007362| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007315| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AKAM training\n",
      "Epoch[1/50] | loss train:0.074077| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.024279| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018511| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.018595| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.021247| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.021774| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.018022| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.020525| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.019092| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.020387| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.019333| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.023394| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.016582| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.019276| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.022915| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.020420| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.017326| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.017264| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.025364| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.017877| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.020606| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.017335| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.018623| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.021483| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.020086| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.017763| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.020808| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.018084| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.016114| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.018361| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.016861| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.016394| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.016338| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.016902| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.017345| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.017731| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.017833| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.017411| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.016584| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.018778| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.013330| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.012090| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.013205| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.015213| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.012544| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011229| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.012557| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.012640| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011790| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011139| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ALK training\n",
      "Epoch[1/50] | loss train:0.066993| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014417| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011323| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011101| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010196| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010946| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010041| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010008| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009527| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009883| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009597| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010397| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009653| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009759| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009299| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010011| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009718| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009620| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011148| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008708| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010302| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008898| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008755| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009721| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009097| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009539| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009134| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009222| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008931| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009692| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008596| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009478| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008529| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007831| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007375| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008008| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007889| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008073| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007755| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007722| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007490| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ALB training\n",
      "Epoch[1/50] | loss train:0.100794| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019760| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018813| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016830| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.017807| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016302| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013502| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014508| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.017705| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014986| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.015553| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014368| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013157| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012607| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013645| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013106| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014437| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011836| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013025| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014015| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012089| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012097| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012304| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.015055| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012271| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011486| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011393| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013613| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011716| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011552| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013518| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011330| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012512| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012629| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012692| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011925| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/50] | loss train:0.012092| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011743| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012198| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010623| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009189| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008798| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008462| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009107| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008875| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009068| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008702| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009611| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008344| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008823| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ARE training\n",
      "Epoch[1/50] | loss train:0.073691| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015300| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012806| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011672| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011283| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011891| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012609| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011001| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013058| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009713| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010122| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011650| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010161| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010261| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009313| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009216| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009143| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010115| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010406| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010500| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009302| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010712| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009735| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010246| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010443| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009266| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009880| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009982| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009303| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009760| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009216| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008800| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009423| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010508| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009337| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009128| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008210| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007747| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007517| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007554| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007424| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007342| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007585| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007463| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007301| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007636| lr:0.001000\n",
      "Number data points 5584 from 2001-01-30 to 2023-04-11\n",
      "ALGN training\n",
      "Epoch[1/50] | loss train:0.080499| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014826| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015372| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014748| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012351| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015575| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013361| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013986| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011970| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011051| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011884| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012598| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011966| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010475| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010391| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013859| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010148| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011851| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012309| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010923| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011814| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011434| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012257| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010628| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011388| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011065| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011454| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010547| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010691| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010189| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010001| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011568| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009373| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010822| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011477| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010873| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010150| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010321| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008211| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007778| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008324| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007633| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007942| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007395| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007679| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007249| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007531| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007755| lr:0.001000\n",
      "Number data points 2364 from 2013-11-18 to 2023-04-11\n",
      "ALLE training\n",
      "Epoch[1/50] | loss train:0.046180| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010120| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007054| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007753| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006844| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005722| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006776| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006970| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005790| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005857| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005746| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005612| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005924| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005982| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006473| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005627| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005011| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005491| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005494| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006010| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006111| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005849| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005336| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005806| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006967| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006103| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005509| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005455| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005618| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005470| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005817| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005099| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005563| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005094| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005343| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005996| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005207| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005848| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005473| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004600| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004469| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004537| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004405| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004404| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004341| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004611| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004540| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004682| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004227| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004523| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LNT training\n",
      "Epoch[1/50] | loss train:0.063316| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013627| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/50] | loss train:0.013081| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012274| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010209| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010128| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008749| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008746| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008939| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009425| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008632| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009221| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008778| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008367| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008847| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008925| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008558| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009018| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009024| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009510| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008510| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008338| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008976| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008529| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008604| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007938| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008825| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007687| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008665| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009346| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008217| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008537| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008199| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008131| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007947| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007561| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007941| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008946| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006876| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006997| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006867| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006593| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006583| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006333| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006714| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006536| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006698| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006610| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ALL training\n",
      "Epoch[1/50] | loss train:0.077254| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016780| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012909| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012512| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012451| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010451| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009867| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009511| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011015| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011250| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008910| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010022| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011199| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009838| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009611| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009132| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009112| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010308| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008946| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009335| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010106| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008865| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008571| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009820| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009318| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009640| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009028| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008477| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008608| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009270| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009633| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009149| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008684| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008613| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008949| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008592| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008577| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007122| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007275| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007120| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007480| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006920| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006755| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006960| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007620| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007078| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006841| lr:0.001000\n",
      "Number data points 4693 from 2004-08-19 to 2023-04-11\n",
      "GOOGL training\n",
      "Epoch[1/50] | loss train:0.078653| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013359| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011536| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010322| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008739| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008711| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009139| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008725| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007990| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009195| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008708| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008491| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009575| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009278| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006549| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008478| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008956| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007399| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008954| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007896| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008674| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008410| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007211| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007989| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007637| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007392| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009119| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007864| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007563| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006806| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007877| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008804| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006922| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008334| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007122| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007514| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007275| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006457| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005796| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005567| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005858| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005633| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005828| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005983| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006098| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005685| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005801| lr:0.001000\n",
      "Number data points 2276 from 2014-03-27 to 2023-04-11\n",
      "GOOG training\n",
      "Epoch[1/50] | loss train:0.049076| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012127| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006404| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006940| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006018| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006211| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005465| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010031| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010099| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006052| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004860| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006358| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005498| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007074| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005308| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006599| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006097| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004392| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006462| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005930| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/50] | loss train:0.007135| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008632| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004695| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005671| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004340| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005662| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005446| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005581| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005407| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005980| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004819| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007753| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005668| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004991| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004630| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005645| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004342| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005505| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006212| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005003| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004491| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003834| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004241| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004967| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004061| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003509| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005604| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004291| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004013| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005028| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MO training\n",
      "Epoch[1/50] | loss train:0.079631| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013266| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011279| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008801| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009697| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008440| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008660| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008828| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008434| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008627| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008179| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008575| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008194| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008841| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007753| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007846| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008597| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008346| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007632| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007864| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008446| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007885| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008510| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008023| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007524| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007794| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007684| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008006| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007892| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007152| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008068| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008115| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008561| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008217| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007666| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007772| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007594| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008370| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007060| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006827| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006654| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006536| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006273| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006696| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006643| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006410| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006548| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006607| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AMZN training\n",
      "Epoch[1/50] | loss train:0.073977| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016199| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011416| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014955| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012405| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011564| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011278| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010437| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011199| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009055| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010743| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012223| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009320| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010138| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010018| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010824| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010002| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009342| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009612| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009012| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008864| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009342| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010541| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008939| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010046| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008608| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009528| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010036| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008893| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008562| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009739| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009820| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008906| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010278| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009611| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008804| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009057| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008260| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007258| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007057| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007409| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007040| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007630| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007650| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007210| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007251| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007095| lr:0.001000\n",
      "Number data points 966 from 2019-06-11 to 2023-04-11\n",
      "AMCR training\n",
      "Epoch[1/50] | loss train:0.051607| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011350| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008842| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007050| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007306| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007313| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006754| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007057| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006842| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008387| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006963| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006847| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006683| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006582| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007599| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006899| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007479| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006671| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006622| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006140| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007270| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005724| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006221| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006845| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007026| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006286| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006804| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006356| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005718| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005861| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006262| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005982| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006188| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006206| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006017| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005895| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007761| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007914| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/50] | loss train:0.006601| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006570| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005946| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005288| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005503| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005375| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005275| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005472| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005564| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005529| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005766| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005506| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AMD training\n",
      "Epoch[1/50] | loss train:0.069579| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018325| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.020372| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015467| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.020054| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015604| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013743| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013981| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014833| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015161| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013423| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012627| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013468| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012572| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011734| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013552| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011581| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.016370| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013058| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012650| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013160| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012105| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012574| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011947| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012049| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012053| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011952| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012781| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012808| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013405| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012658| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012839| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012404| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011770| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013456| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011521| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013906| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013411| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013092| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012642| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010282| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009583| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010623| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009581| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010942| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009535| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009706| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009005| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009616| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008776| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AEE training\n",
      "Epoch[1/50] | loss train:0.067341| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016678| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012819| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012370| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010834| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011351| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010945| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011225| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010687| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009172| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009653| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009557| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011558| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010251| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010057| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009944| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010056| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009968| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009797| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009463| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009800| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009742| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009101| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008633| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008517| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008962| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009821| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009483| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008351| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009650| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009117| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008790| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008927| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009132| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009098| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008560| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007615| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007221| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007374| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007149| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007148| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007109| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007418| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007070| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007216| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007301| lr:0.001000\n",
      "Number data points 4414 from 2005-09-27 to 2023-04-11\n",
      "AAL training\n",
      "Epoch[1/50] | loss train:0.078108| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014350| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012220| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010729| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010720| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009733| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010598| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009422| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009705| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009728| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009060| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010061| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009530| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010029| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010146| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009663| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008451| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010158| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008631| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010184| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009312| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009762| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009034| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009028| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009118| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009178| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009661| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008818| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008934| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008535| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009561| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008561| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008976| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008568| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009824| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009131| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008803| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008051| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007742| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007561| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007845| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007939| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007622| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007544| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007593| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007709| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007770| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AEP training\n",
      "Epoch[1/50] | loss train:0.053297| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012757| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012271| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011247| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/50] | loss train:0.011140| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009871| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011301| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009870| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010181| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009016| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009752| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010156| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009422| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009215| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009373| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009752| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009532| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009140| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010685| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008589| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008895| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008587| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009267| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008806| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008538| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009318| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009152| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008670| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009197| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008555| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008992| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008299| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008839| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008342| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007257| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007029| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007160| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007082| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006956| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007406| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007274| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007126| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007390| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007004| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AXP training\n",
      "Epoch[1/50] | loss train:0.064084| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014762| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013654| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012258| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014498| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013086| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011763| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010348| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013509| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012016| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010934| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011466| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010780| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011851| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010759| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010384| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012404| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012781| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009920| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010841| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010845| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011276| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010624| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010792| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009850| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010060| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010536| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010648| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011098| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010042| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009707| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010569| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010368| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010910| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010855| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010717| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010667| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010381| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010899| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008861| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008472| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008439| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008164| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008019| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007964| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007938| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007977| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008568| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008409| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AIG training\n",
      "Epoch[1/50] | loss train:0.046014| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012648| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010802| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010576| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011447| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009239| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010400| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009926| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008875| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008604| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009784| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008522| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008508| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008344| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008969| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008697| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008910| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008853| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008451| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008234| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008918| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008307| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008370| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008347| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008084| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008903| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008461| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009197| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007577| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007787| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008620| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008098| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007532| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008806| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008063| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008434| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008234| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008620| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007501| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008379| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007139| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007128| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006819| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006937| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007069| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006945| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007043| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006731| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007053| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006965| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AMT training\n",
      "Epoch[1/50] | loss train:0.058547| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013423| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012335| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010759| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011417| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010498| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009575| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012230| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010075| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009020| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009104| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009930| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008894| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009132| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008926| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009185| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009743| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009138| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009216| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/50] | loss train:0.009074| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009430| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008611| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008676| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008376| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008753| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008220| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009480| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009006| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009230| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008787| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009528| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008474| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008093| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008373| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008261| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008791| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007475| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007431| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007050| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006746| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006807| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006878| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007099| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007233| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006739| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007204| lr:0.001000\n",
      "Number data points 3768 from 2008-04-23 to 2023-04-11\n",
      "AWK training\n",
      "Epoch[1/50] | loss train:0.060836| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010039| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008724| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008806| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007561| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008003| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007572| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006415| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006997| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006533| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007053| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006597| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006742| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006336| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005899| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006567| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006478| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006561| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005694| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007089| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006339| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005855| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005917| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005134| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005592| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006681| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006027| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006051| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006088| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005768| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006295| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006031| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005755| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006282| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006013| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006766| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006149| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005891| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006331| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005555| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005104| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004947| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004733| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004555| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004646| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004957| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004747| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004606| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004496| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004797| lr:0.001000\n",
      "Number data points 4422 from 2005-09-15 to 2023-04-11\n",
      "AMP training\n",
      "Epoch[1/50] | loss train:0.058714| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011413| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010415| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012760| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010158| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010975| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008026| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010292| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010336| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008225| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009300| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009347| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007367| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007876| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007792| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008433| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008939| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008036| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007844| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007214| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008074| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009066| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008248| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007154| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008085| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007670| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007202| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007296| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008358| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007272| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008307| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007837| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007106| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007484| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007652| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007438| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007390| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008353| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005848| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006164| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005769| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005933| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005466| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006060| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005706| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006219| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005541| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006107| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ABC training\n",
      "Epoch[1/50] | loss train:0.050737| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013829| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010434| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011069| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010291| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010936| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009654| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012704| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010612| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009578| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010438| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009045| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009441| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008935| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009673| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010250| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008885| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009692| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009519| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008327| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008417| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008716| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008844| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008984| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009099| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009330| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008759| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008504| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008670| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008497| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008645| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008453| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008546| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008507| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008593| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007752| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/50] | loss train:0.007092| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006923| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006995| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006926| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007038| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006976| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006920| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007080| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006917| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006978| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AME training\n",
      "Epoch[1/50] | loss train:0.057118| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012529| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011027| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013551| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011435| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010105| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009732| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010246| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008926| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010283| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009465| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009060| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009806| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008793| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009880| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008818| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008836| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008636| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008875| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009220| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008415| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008600| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008975| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008218| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009525| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008659| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009016| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008421| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008786| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008558| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009010| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008373| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009075| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008098| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007298| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007149| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007172| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006986| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006885| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006657| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006573| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006901| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006606| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006786| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AMGN training\n",
      "Epoch[1/50] | loss train:0.071710| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012885| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012359| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011444| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011236| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010731| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009994| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010647| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010017| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010180| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009289| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010832| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010291| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009672| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010252| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008610| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009286| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010194| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008883| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009864| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009797| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008460| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009455| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010496| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008785| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008235| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008971| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009219| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008656| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009290| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008408| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009142| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008988| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008552| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007959| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009507| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007742| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007221| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007415| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007188| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007689| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007381| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007002| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007371| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007240| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007685| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "APH training\n",
      "Epoch[1/50] | loss train:0.064706| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015747| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011914| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011642| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011533| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012321| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011673| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010400| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009545| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009508| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009126| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008977| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009366| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009490| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008956| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009275| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008606| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008753| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009940| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009240| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009017| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009492| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008419| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008457| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008451| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008910| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009357| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008585| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008348| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008843| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008679| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007527| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007035| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007175| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006942| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006538| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007106| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006701| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006653| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006874| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006993| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ADI training\n",
      "Epoch[1/50] | loss train:0.088580| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016772| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014156| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012560| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013038| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011846| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/50] | loss train:0.010978| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012718| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011762| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011127| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011624| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010779| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012618| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011784| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010757| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011514| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011587| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012306| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010271| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011471| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010094| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011194| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011449| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011460| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009517| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010490| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010055| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010643| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010887| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010505| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012483| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010320| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011322| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009953| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011388| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011155| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010576| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010061| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011098| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009913| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008527| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007990| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008466| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008338| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008599| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007701| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008576| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008185| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008255| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008190| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ANSS training\n",
      "Epoch[1/50] | loss train:0.067689| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015813| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013592| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013053| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012886| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010141| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011485| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012026| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011154| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011404| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010401| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009508| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009795| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010436| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010824| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011255| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010836| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011258| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012944| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009619| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009710| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011054| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011196| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011254| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010625| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009450| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009698| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009079| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010664| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009944| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009863| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010549| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010357| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010985| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008878| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009708| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009085| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010001| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010117| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008491| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007732| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007743| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008130| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007936| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008282| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007725| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007794| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008138| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007901| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AON training\n",
      "Epoch[1/50] | loss train:0.079696| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015419| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012180| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011895| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011735| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009446| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010206| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010787| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010398| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011385| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009791| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010768| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010360| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010262| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010158| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009727| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010408| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009395| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008964| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009332| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008991| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009225| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009278| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008663| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009163| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010269| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009069| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008230| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008079| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009644| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008999| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009207| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008745| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008764| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008430| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009073| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008827| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008268| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007356| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006987| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006822| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006845| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006439| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006926| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006758| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006978| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006529| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006749| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "APA training\n",
      "Epoch[1/50] | loss train:0.088368| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017388| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015299| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014437| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015501| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013334| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014362| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011628| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012546| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012374| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013796| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012344| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013111| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012765| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013124| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012395| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012348| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011640| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013255| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012838| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012044| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012734| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012543| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012499| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[25/50] | loss train:0.011889| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011719| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012119| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011954| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011845| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011680| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010705| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011705| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011808| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011285| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012280| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011557| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011187| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011389| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011544| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011301| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009789| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009805| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009561| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009693| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009849| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009554| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009750| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009956| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010207| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009317| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AAPL training\n",
      "Epoch[1/50] | loss train:0.063124| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015398| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011584| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012948| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011945| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010709| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013057| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012250| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011425| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009991| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012633| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011543| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009798| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010486| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010717| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010294| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009699| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009776| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010909| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009962| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010393| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008899| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008616| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009093| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011916| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009227| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009833| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009195| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008651| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009836| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009560| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009237| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010757| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009345| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009441| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008717| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009573| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007656| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007654| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007912| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008042| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007441| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007315| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007218| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007599| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006943| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AMAT training\n",
      "Epoch[1/50] | loss train:0.082630| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.023691| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013780| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.019351| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015607| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013374| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013642| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014669| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014343| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013948| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012655| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013755| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013734| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011491| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012213| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012542| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011756| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011711| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013014| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013736| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011876| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013700| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012341| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011725| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012291| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012012| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012351| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011852| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012075| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012153| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012293| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011281| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011924| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.014039| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010529| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010468| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010641| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010442| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010923| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011979| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010206| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009866| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008807| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009016| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009066| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009074| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009121| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008999| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008627| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009346| lr:0.001000\n",
      "Number data points 2866 from 2011-11-17 to 2023-04-11\n",
      "APTV training\n",
      "Epoch[1/50] | loss train:0.056969| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012399| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011360| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010630| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010099| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009523| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008269| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007239| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007521| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008203| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008512| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006427| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008196| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006448| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007934| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008275| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007817| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006833| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007392| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006649| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009411| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006698| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006569| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006567| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007908| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008744| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007271| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006730| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006904| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008132| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007273| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007721| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007422| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007126| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006511| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006162| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006809| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006604| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006899| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006106| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[42/50] | loss train:0.006256| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005736| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005883| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005458| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005298| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005784| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005414| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005244| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005421| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ACGL training\n",
      "Epoch[1/50] | loss train:0.080201| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013615| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013063| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012643| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011680| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013097| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012299| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010252| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010914| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010317| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010976| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009436| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010122| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010854| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009800| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008900| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009258| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010570| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010976| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010135| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010309| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010126| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009649| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009538| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009202| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009929| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010170| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009027| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009859| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010075| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009407| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009468| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009062| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009008| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008740| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009959| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007859| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007649| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007591| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007146| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006963| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006841| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006878| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007198| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007321| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007101| lr:0.001000\n",
      "Number data points 2227 from 2014-06-06 to 2023-04-11\n",
      "ANET training\n",
      "Epoch[1/50] | loss train:0.064593| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009884| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009208| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007890| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006673| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006585| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005825| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005411| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007543| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006840| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004795| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004640| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004986| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007301| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005699| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004687| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005162| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004603| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006960| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004475| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005699| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004560| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005392| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005101| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004391| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004748| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005033| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004674| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004369| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004164| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005032| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005948| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004720| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004235| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004219| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005820| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006761| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005083| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005090| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005234| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004348| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004078| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004212| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003952| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003798| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003719| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003810| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003578| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003787| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003637| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AJG training\n",
      "Epoch[1/50] | loss train:0.076028| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015952| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012304| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012386| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013891| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010505| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013241| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010586| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012217| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011112| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011422| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009686| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013915| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010739| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011187| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010196| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011286| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009611| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010201| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010644| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009835| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010639| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011246| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008692| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010293| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008967| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009445| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009282| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009880| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008911| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009879| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010631| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009120| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009835| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008379| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009296| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009334| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007434| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006857| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007706| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006895| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006726| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006793| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007338| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006340| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006446| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006433| lr:0.001000\n",
      "Number data points 4828 from 2004-02-05 to 2023-04-11\n",
      "AIZ training\n",
      "Epoch[1/50] | loss train:0.053587| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015392| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011210| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010598| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008654| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008778| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008699| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/50] | loss train:0.011238| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009255| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008383| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009221| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009007| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008895| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008878| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008988| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008625| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009012| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008001| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008637| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007739| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009862| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007932| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008891| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008008| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007806| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008846| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008615| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007799| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007965| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008690| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008771| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008068| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008019| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007818| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007773| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007610| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007442| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007856| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007907| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007822| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006854| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006381| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006084| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006407| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006068| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006018| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006584| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006048| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006168| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006632| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "T training\n",
      "Epoch[1/50] | loss train:0.067821| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015287| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013251| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012304| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012099| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012048| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012408| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011689| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011616| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012015| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010974| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011741| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010372| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010386| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011100| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011553| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011107| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011117| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009956| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010117| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010253| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010295| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010108| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010448| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010029| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010339| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009286| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010252| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009788| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009710| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010466| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009557| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009469| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009684| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009381| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010120| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009812| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008755| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008415| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008166| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008376| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008179| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008444| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008470| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008437| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008010| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008200| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ATO training\n",
      "Epoch[1/50] | loss train:0.053576| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013288| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011527| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011053| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012832| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008349| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009671| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010535| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008341| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009057| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009985| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008290| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009014| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009026| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008609| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010808| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008851| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008313| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008682| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008478| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009043| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008512| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009537| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008056| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008341| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008160| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008253| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008137| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008391| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008626| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007974| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008286| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009104| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008623| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008347| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008250| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007866| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008718| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008607| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006983| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006777| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006545| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006851| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006669| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006742| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006778| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006396| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006536| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006684| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ADSK training\n",
      "Epoch[1/50] | loss train:0.070392| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014820| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011812| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013311| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011957| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013319| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012471| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011089| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011299| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011125| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011150| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013194| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010186| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010592| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013575| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010674| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010781| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010560| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010397| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009786| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011803| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009047| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010375| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011084| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010681| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[26/50] | loss train:0.009898| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011770| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009793| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011036| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011784| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010393| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009805| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009702| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010134| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010407| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009222| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009693| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010523| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010106| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008579| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008412| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008152| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007497| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008133| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007768| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007310| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007439| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007547| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007816| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007728| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AZO training\n",
      "Epoch[1/50] | loss train:0.082348| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014398| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012537| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012715| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011475| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012780| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013284| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012119| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010570| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009976| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009945| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012214| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010809| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009322| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011287| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010102| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008563| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008573| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010786| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009952| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010333| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009221| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009322| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010205| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010368| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009830| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009610| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009700| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009344| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010622| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009133| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009877| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009013| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008843| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010322| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008543| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007147| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007704| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007587| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007399| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007188| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007339| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006379| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007074| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007064| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006735| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AVB training\n",
      "Epoch[1/50] | loss train:0.072401| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014020| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011535| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012459| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010789| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011396| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010498| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009810| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012045| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009356| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010449| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010184| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009619| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010062| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009502| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008884| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009171| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009456| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009183| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009839| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009199| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009034| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009328| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010010| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009509| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008194| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008936| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009509| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009254| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009163| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008593| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008770| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009056| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009013| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007731| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007419| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007271| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007372| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007693| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006768| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007283| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007502| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007106| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007189| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "AVY training\n",
      "Epoch[1/50] | loss train:0.075920| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015162| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014785| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012574| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013973| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010967| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011062| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010843| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011252| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011340| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010283| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011871| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011009| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009887| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010375| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010069| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009262| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009888| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008852| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010684| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011678| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008956| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010006| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010495| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009964| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009379| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009249| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008958| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009768| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010249| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009904| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008770| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010144| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009772| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009110| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009429| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009179| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007164| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007597| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007448| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[44/50] | loss train:0.007191| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007246| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007304| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007062| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007348| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006770| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006900| lr:0.001000\n",
      "Number data points 1452 from 2017-07-05 to 2023-04-11\n",
      "BKR training\n",
      "Epoch[1/50] | loss train:0.045363| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014207| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010579| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010592| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009359| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009099| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008536| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008234| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007081| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007676| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009435| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008187| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008496| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008102| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007104| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009503| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008658| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009890| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007396| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008654| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008749| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008936| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007467| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008029| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007757| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007376| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007307| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006967| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006599| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006983| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007606| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008075| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008721| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006970| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006820| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007129| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007301| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007422| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006729| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006411| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006047| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006790| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006178| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006429| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005761| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006436| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006966| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005889| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005919| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BALL training\n",
      "Epoch[1/50] | loss train:0.083471| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014500| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013904| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012966| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010900| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011328| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010679| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009822| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010313| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010695| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011151| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009903| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010241| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010013| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011122| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010376| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008737| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010882| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009835| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010229| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010085| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009359| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010289| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009472| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010138| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009091| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009429| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010634| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009306| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009173| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009342| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009486| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010161| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009022| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009320| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009278| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009073| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009518| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007540| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007392| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007220| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008232| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007737| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007094| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007390| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007898| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007508| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007428| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BAC training\n",
      "Epoch[1/50] | loss train:0.058395| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016566| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015649| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014000| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014206| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014283| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014467| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012877| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012517| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011833| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012918| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012764| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013241| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012412| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012434| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012781| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012475| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012363| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012252| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012409| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011971| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012047| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010666| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011074| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011708| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012048| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011919| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012083| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011607| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013189| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011191| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011354| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011952| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011427| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011008| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011670| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011854| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011814| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010998| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011239| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009817| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009858| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009555| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009879| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010008| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009410| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009314| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009338| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009554| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009329| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BBWI training\n",
      "Epoch[1/50] | loss train:0.071046| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016373| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012253| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012572| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013541| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011718| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011888| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011793| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010687| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10/50] | loss train:0.012267| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010575| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010223| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010883| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011779| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011596| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010501| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010925| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011451| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010861| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010645| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011107| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010501| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010811| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010029| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009923| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010999| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010629| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011274| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011330| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010745| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010314| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011211| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010204| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010515| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010174| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010293| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010858| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010483| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008845| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008793| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008753| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008246| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008085| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008737| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008169| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009082| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008413| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008230| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BAX training\n",
      "Epoch[1/50] | loss train:0.078974| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015056| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015112| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012000| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011230| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010440| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009917| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009178| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009687| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011010| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009942| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009604| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009643| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010174| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010166| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008988| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009193| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010141| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009573| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008713| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009114| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009907| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010009| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009722| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008220| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008797| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009004| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009334| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009775| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009599| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008832| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008761| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009247| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007745| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007508| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007405| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007325| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007189| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007401| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007290| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007351| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007879| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007333| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BDX training\n",
      "Epoch[1/50] | loss train:0.048980| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012827| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013474| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009365| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009841| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010777| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009564| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009077| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008995| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009542| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009532| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008500| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009160| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008350| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008814| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008758| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008721| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009304| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008771| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008635| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008557| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008638| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008134| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007866| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008867| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008350| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008584| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008812| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008181| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007936| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008528| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008542| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008440| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007953| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007441| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008032| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008299| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007814| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008063| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007771| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007043| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007092| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006954| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006823| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006853| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006618| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006841| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006536| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006739| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006537| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WRB training\n",
      "Epoch[1/50] | loss train:0.081986| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014631| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013427| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014013| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012376| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010435| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010843| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009735| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011805| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012308| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010384| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011848| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009537| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009398| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010370| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009585| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008955| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010004| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009556| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009891| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009154| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009305| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008436| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009300| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009478| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009139| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008763| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[28/50] | loss train:0.009705| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010413| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009968| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008637| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008690| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009257| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010143| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009202| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008150| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008537| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009637| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007641| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007344| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007130| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007661| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006366| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007067| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006981| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007013| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006918| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007815| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BRK.B training\n",
      "Epoch[1/50] | loss train:0.067667| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013503| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013642| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012820| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010455| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010080| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009490| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009732| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010300| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010528| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009552| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009550| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009405| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009972| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009107| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010351| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008669| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009096| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009187| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009765| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008048| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008670| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008081| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008695| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008935| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009141| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008757| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008021| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008858| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007840| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009806| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008896| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008501| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007359| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006946| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006588| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006865| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006435| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006924| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006649| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006879| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006493| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006866| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BBY training\n",
      "Epoch[1/50] | loss train:0.069732| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018651| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015320| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013615| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013482| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014011| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012254| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011963| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014293| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013130| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011232| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012196| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012158| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011565| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012875| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012988| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011353| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011631| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011954| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011773| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011722| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012625| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010742| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011880| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011965| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011782| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010829| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011791| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011627| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011243| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011257| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010813| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013832| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011433| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010825| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011951| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011312| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010003| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012673| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010311| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009589| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008750| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008739| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008499| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008869| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008949| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008763| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008915| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009157| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008803| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BIO training\n",
      "Epoch[1/50] | loss train:0.076375| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016353| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014693| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013662| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015348| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012750| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015207| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010933| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.015256| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012596| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011371| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011220| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011005| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009831| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010684| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012139| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012305| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012001| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010400| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010665| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010935| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011481| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009284| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010232| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010221| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010070| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010294| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010858| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010182| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009859| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011201| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010633| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009243| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009216| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009771| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010302| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010426| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008048| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007795| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007384| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008010| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007752| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[46/50] | loss train:0.007207| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007098| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007381| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007049| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007638| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TECH training\n",
      "Epoch[1/50] | loss train:0.083500| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017672| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013487| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015149| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013984| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012347| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012950| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011910| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012685| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010528| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010963| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012458| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011353| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012143| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010046| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012892| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010765| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011012| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010541| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012652| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009936| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011918| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010038| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010881| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011649| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010146| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010146| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011685| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009585| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011181| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010090| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010247| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009866| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010510| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009988| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011651| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009578| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010244| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007788| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007860| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008305| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007756| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007551| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007970| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008619| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007765| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007785| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007586| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BIIB training\n",
      "Epoch[1/50] | loss train:0.058828| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014834| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012551| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011695| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011794| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011773| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012183| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011888| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010766| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011755| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010938| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011359| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011703| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010950| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011505| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010644| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011527| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011773| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010798| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011012| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010334| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011398| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010639| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010834| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010409| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011013| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010946| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010844| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010827| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009959| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010108| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010473| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010638| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010182| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010784| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010567| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010873| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010180| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010144| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009017| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008955| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008565| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009019| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008843| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008613| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009008| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009162| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008823| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008751| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BLK training\n",
      "Epoch[1/50] | loss train:0.057312| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014698| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011517| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012177| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010666| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010912| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010370| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011342| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012309| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010447| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010923| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010845| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010292| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009466| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009324| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010109| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011368| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009981| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012580| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010108| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009849| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009474| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009100| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009708| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010080| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008787| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009425| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009829| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009522| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009316| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009259| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009043| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009643| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008959| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008968| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009161| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009334| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009274| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008121| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007338| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007158| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006865| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007253| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007685| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007309| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007646| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007299| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007315| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BK training\n",
      "Epoch[1/50] | loss train:0.071192| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019296| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016718| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016139| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014543| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.017007| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014902| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.016349| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014044| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015155| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.015142| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/50] | loss train:0.016304| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014339| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014755| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014690| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013338| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.016023| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014456| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014272| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014699| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.015287| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013994| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.015319| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013239| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.014302| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014038| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013031| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013965| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014140| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013702| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013640| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.014152| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013496| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013676| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013061| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013624| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013784| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013703| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013967| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013189| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011602| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011904| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011715| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011578| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011764| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011198| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011587| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011361| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011650| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011064| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BA training\n",
      "Epoch[1/50] | loss train:0.067946| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015095| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015026| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015550| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014019| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011761| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013285| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013688| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011427| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012885| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011958| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012284| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011135| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011173| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011693| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011283| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011821| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010703| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011213| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012534| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011351| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010435| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010745| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011472| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011454| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011786| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010272| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011444| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010713| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010203| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010037| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010906| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010268| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011279| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010453| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010122| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010032| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009081| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009177| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008263| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008802| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008611| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008271| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008508| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008326| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007846| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008052| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BKNG training\n",
      "Epoch[1/50] | loss train:0.073959| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014038| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011787| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011290| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011303| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011840| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009532| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010541| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009602| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009249| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010783| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009430| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009180| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009413| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009996| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008949| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009632| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009166| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008796| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008269| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009910| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009187| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009581| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009517| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009295| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009363| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009067| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008874| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008425| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008647| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008727| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008618| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008178| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009317| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008344| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008968| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008951| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007648| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007345| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007151| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007335| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007302| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007198| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007513| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007207| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007070| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007294| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BWA training\n",
      "Epoch[1/50] | loss train:0.061965| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015376| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012224| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011080| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011279| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010200| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010498| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012119| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011044| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010792| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010785| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010933| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010485| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010831| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010266| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009775| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009699| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010626| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010271| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010904| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009932| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009604| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010220| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010166| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009753| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009988| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010074| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[30/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009671| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010072| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009351| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010107| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009063| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009432| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009431| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009914| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009789| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008391| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008382| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008111| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008246| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008270| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007848| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008323| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007905| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007533| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007721| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BXP training\n",
      "Epoch[1/50] | loss train:0.061341| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013404| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012540| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011831| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011604| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011374| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011066| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010733| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010435| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010764| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010578| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011122| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010182| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010975| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009971| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010304| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010705| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009984| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010352| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009893| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010351| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010429| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009625| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010135| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009992| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009676| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009695| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010590| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010094| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009973| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010288| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009595| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009895| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009926| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010003| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010054| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010143| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009360| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008838| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008181| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008245| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008254| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008272| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008525| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007997| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008179| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008115| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008298| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BSX training\n",
      "Epoch[1/50] | loss train:0.067187| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014042| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012027| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013482| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010762| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010430| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012328| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010312| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010764| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010868| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011880| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009994| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010411| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010395| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009859| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010766| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010180| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010437| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009743| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009654| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010327| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009446| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010076| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009463| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009704| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010233| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010483| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009065| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009235| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009099| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009232| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010260| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010251| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009453| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009245| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009482| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008426| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008102| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007932| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007890| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007999| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007464| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008072| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007942| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007826| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007966| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BMY training\n",
      "Epoch[1/50] | loss train:0.065715| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013856| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012428| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012210| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010698| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010468| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009609| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011030| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010235| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010423| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010065| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010785| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009420| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011014| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009344| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010084| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010287| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009056| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009867| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009156| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008945| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010086| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008908| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009198| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009546| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009580| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008818| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009690| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008967| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008590| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009314| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009165| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009055| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009132| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007691| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008095| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007504| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007823| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007665| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007882| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007427| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[48/50] | loss train:0.007393| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007551| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007824| lr:0.001000\n",
      "Number data points 3443 from 2009-08-06 to 2023-04-11\n",
      "AVGO training\n",
      "Epoch[1/50] | loss train:0.057526| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010123| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009239| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007382| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006601| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008926| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008179| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005863| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006920| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006847| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008286| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006366| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006004| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006936| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007309| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006270| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006512| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008743| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006954| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005572| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007279| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005782| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006568| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006600| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006391| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006890| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006341| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006336| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005869| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005830| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006576| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006416| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006945| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005798| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007189| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005800| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005778| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005596| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005779| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005471| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004831| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005133| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004887| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005118| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004647| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004746| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004971| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004474| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004597| lr:0.001000\n",
      "Number data points 4035 from 2007-04-02 to 2023-04-11\n",
      "BR training\n",
      "Epoch[1/50] | loss train:0.055724| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011245| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008324| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007435| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007104| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007173| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007723| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006268| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007127| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006542| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007215| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006629| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005811| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006009| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006931| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006477| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006934| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005961| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006811| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006378| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006348| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006551| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006110| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005983| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006284| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005682| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006641| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006428| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006121| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005615| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006016| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006873| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006464| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005711| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005568| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006183| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005937| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006204| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005525| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006252| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005352| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005076| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004894| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004948| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004670| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004989| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004735| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004664| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004726| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004753| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BRO training\n",
      "Epoch[1/50] | loss train:0.058109| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015297| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010654| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012005| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012792| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012873| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010289| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010811| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011186| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010238| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009861| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009902| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011437| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009769| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009419| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009655| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009172| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010805| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010272| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009414| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009403| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009495| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008729| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009470| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008729| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009591| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008614| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008929| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008610| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008860| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008580| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008410| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009568| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009484| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008184| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007243| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007514| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007576| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007318| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006531| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007284| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007294| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006499| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006753| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007045| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BF-B training\n",
      "Epoch[1/50] | loss train:0.054966| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014179| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011145| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010349| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013078| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010033| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011666| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010175| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009611| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011000| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008954| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009368| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[14/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008840| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009228| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009678| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009144| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008851| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009004| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009373| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009628| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008764| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008914| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009113| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009269| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008467| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008931| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008070| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008286| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008267| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007867| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008841| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008617| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008541| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008466| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008772| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007590| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007370| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007222| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006836| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007028| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006819| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007008| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006777| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006944| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006844| lr:0.001000\n",
      "Number data points 5456 from 2001-08-02 to 2023-04-11\n",
      "BG training\n",
      "Epoch[1/50] | loss train:0.069367| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017087| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017337| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015022| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014127| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014555| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013486| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012513| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013838| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014697| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014785| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012539| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013021| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013127| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012117| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012610| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012081| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014075| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012598| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011981| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013871| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012803| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012153| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011963| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012306| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012968| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011634| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013347| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013242| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012050| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011743| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012451| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011537| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012522| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011869| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012207| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011876| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011124| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012435| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012136| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010180| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010692| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010128| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009521| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009588| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009614| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009458| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009908| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009206| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009752| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CHRW training\n",
      "Epoch[1/50] | loss train:0.052188| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014842| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012788| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012813| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011462| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010934| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011896| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010806| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011234| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010725| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010240| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011235| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009724| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010677| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011247| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009434| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010656| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009566| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010348| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008836| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009254| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010620| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009328| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009130| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009034| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009018| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010167| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009626| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009456| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009482| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009670| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009017| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009365| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010007| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008790| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009369| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008802| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007940| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007980| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007934| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007238| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007713| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007377| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007655| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007727| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007541| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007586| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CDNS training\n",
      "Epoch[1/50] | loss train:0.078782| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018081| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016663| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014121| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013254| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015703| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013549| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012625| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011915| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011721| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013217| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010728| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012996| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011459| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011435| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011951| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011407| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010938| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011270| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009989| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010143| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010946| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011447| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010501| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012506| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010011| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011965| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009975| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011599| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010135| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010518| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[32/50] | loss train:0.011183| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010596| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011274| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011831| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009255| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011498| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010784| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010246| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009491| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008342| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008579| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008224| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008152| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008570| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008284| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007498| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008187| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007788| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007903| lr:0.001000\n",
      "Number data points 2153 from 2014-09-22 to 2023-04-11\n",
      "CZR training\n",
      "Epoch[1/50] | loss train:0.056722| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012811| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009892| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011067| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009332| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008338| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007028| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008934| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006669| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006911| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005771| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005835| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006372| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006773| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007876| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008388| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006090| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005684| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005946| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007608| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008129| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006355| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006795| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006965| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006280| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006020| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007977| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008864| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008617| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007001| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005102| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005258| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005097| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008160| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006085| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006173| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005854| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004775| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005746| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005417| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005017| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004954| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004242| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004911| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005764| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004932| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004894| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004497| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004118| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004576| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CPT training\n",
      "Epoch[1/50] | loss train:0.064381| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014155| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013534| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011502| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011155| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009797| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010014| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010566| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010494| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011478| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010814| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010847| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010510| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010344| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010091| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008778| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009938| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009740| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009230| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010953| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009615| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010468| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009536| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009048| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008040| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009879| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008922| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009591| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008916| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009136| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010043| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009283| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010308| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008602| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008140| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008269| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007941| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007135| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006941| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007308| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006833| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007001| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007273| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007176| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007042| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CPB training\n",
      "Epoch[1/50] | loss train:0.059717| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016032| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012328| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012243| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010696| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012714| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010272| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012341| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011855| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010733| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011915| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010935| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010596| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010397| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010169| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009711| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009720| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010121| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009820| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011084| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009774| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010384| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010264| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009569| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009314| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009610| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009951| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009803| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010776| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010112| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009537| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009498| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009682| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009793| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009473| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009751| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008211| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008439| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008277| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007701| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008287| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008470| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007801| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008593| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008258| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[50/50] | loss train:0.008304| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "COF training\n",
      "Epoch[1/50] | loss train:0.086991| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.022128| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.019146| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016193| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015870| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016001| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.016519| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.016249| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014319| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.016352| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014351| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013720| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014834| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015486| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013970| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013260| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013477| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013088| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013723| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014123| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014563| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013425| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.014292| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013383| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.014430| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012626| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013476| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013461| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013621| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013497| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011798| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013751| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013905| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013692| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012484| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013808| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013973| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.014901| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013690| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011773| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010592| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010380| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010697| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010289| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009939| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010442| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010085| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010486| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009296| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010563| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CAH training\n",
      "Epoch[1/50] | loss train:0.075073| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017386| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015614| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013635| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013578| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014037| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013988| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012102| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012924| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012325| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012332| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011918| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012115| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012139| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012466| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012376| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012449| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011423| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011904| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013470| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013057| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011590| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012192| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012275| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012808| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012017| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011101| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011257| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011991| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011707| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010957| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012211| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010835| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010938| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011572| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012087| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011482| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010792| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011067| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012184| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009694| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009625| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009455| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009769| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009148| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008891| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009084| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009460| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009021| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009305| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "KMX training\n",
      "Epoch[1/50] | loss train:0.083921| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015544| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015820| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014092| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012297| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012619| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011744| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012308| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012035| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013913| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012322| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011873| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013595| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011294| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010314| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012466| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010590| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012237| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011282| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010796| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011387| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011843| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012081| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009648| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010456| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010898| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010673| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010397| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010389| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010938| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010325| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011383| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010289| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009973| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010943| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010872| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010120| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010165| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009662| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009855| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008528| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008528| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008442| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008097| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007881| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007754| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007764| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008086| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008041| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007808| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CCL training\n",
      "Epoch[1/50] | loss train:0.074301| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018305| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016523| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015739| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014919| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.018645| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015294| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013724| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012537| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014284| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012532| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012960| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014541| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012870| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.015362| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[16/50] | loss train:0.012639| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013496| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012632| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013208| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012790| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012462| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012691| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013602| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014080| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012439| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012004| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012947| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011645| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012984| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012614| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012693| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012066| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012927| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012689| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013655| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012199| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012279| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012894| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011798| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011742| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010386| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010208| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009947| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010317| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010017| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010200| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010064| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009754| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010100| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009682| lr:0.001000\n",
      "Number data points 771 from 2020-03-19 to 2023-04-11\n",
      "CARR training\n",
      "Epoch[1/50] | loss train:0.031013| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.006372| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.004237| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.003896| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.003957| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.003126| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.003410| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.003020| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.002739| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.003073| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.003358| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.002885| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.002790| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.002639| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.002866| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.003272| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.002944| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.003407| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.003283| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.002210| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.002540| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.002231| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.002507| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.002250| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.002630| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.002636| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.002960| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003046| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.002843| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003186| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003020| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.002799| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.002546| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.002974| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.002863| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.002482| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.002667| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.002773| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003040| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003333| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.002020| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.002149| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.002180| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.002138| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.001840| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002069| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002251| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.001923| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.002082| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.001809| lr:0.001000\n",
      "Number data points 2189 from 2014-07-31 to 2023-04-11\n",
      "CTLT training\n",
      "Epoch[1/50] | loss train:0.046887| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010041| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007900| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006615| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005528| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005644| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005049| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006327| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.004797| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004714| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005275| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004797| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004241| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004392| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005133| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004642| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004552| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004592| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004268| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004581| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005407| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004167| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004144| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004781| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004221| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003963| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004423| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004292| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004588| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004099| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003973| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004016| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004067| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004315| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004163| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004486| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003879| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004013| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004566| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004427| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003488| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003543| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003535| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003301| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003752| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003443| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003648| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003305| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003323| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003487| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CAT training\n",
      "Epoch[1/50] | loss train:0.065977| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017551| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014582| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013489| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013668| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013423| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011166| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011532| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013400| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011510| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010561| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011016| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011047| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009910| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010228| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011308| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010692| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010185| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012000| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010296| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009735| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009765| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010245| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010799| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010604| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011424| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009500| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010007| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009608| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010195| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010257| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009375| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009035| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[34/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010499| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010283| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009221| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011083| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009649| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008071| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007573| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008045| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007573| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007706| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007110| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007674| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007201| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007984| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007350| lr:0.001000\n",
      "Number data points 3228 from 2010-06-15 to 2023-04-11\n",
      "CBOE training\n",
      "Epoch[1/50] | loss train:0.048457| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009140| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007488| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006702| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006264| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006033| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006685| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005947| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005428| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006063| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005331| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005157| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006114| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005876| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005352| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005869| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005112| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005451| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005597| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005302| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005419| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005490| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005231| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005251| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005323| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005299| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005493| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005062| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004874| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005138| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005284| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005003| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004863| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005336| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004963| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004630| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005135| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005088| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004934| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004827| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004362| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004475| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004155| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004130| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004157| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004210| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004245| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004200| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004362| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004220| lr:0.001000\n",
      "Number data points 4741 from 2004-06-10 to 2023-04-11\n",
      "CBRE training\n",
      "Epoch[1/50] | loss train:0.061465| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013705| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014177| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011394| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010313| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010786| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009897| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009907| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009811| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008749| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009222| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009760| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010089| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009419| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009746| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008826| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010307| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011180| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008457| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009333| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008506| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008335| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008265| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008058| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008442| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008007| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008977| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008442| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008930| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008977| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008421| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008496| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008057| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008288| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009511| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008189| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007287| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006812| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006789| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007060| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006228| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006590| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006217| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006560| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006577| lr:0.001000\n",
      "Number data points 2464 from 2013-06-27 to 2023-04-11\n",
      "CDW training\n",
      "Epoch[1/50] | loss train:0.061553| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011209| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011202| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008143| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013775| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010673| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006934| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005662| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.004977| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008670| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005195| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011046| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006393| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005488| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005203| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005242| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006892| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006143| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005565| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004338| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011292| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005712| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005190| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007496| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012995| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004424| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.018433| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.026259| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008057| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.016704| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005213| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005046| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004395| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.016116| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006672| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005167| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004647| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007665| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005404| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007465| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005518| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.015368| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005478| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003606| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004371| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004506| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003486| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005730| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005969| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004216| lr:0.001000\n",
      "Number data points 4586 from 2005-01-21 to 2023-04-11\n",
      "CE training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] | loss train:0.051412| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012865| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010507| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010814| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010683| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009562| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011810| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009036| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010052| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011811| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010657| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008818| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009719| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009172| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009254| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009106| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008880| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007935| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008395| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008502| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009315| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009194| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008351| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010141| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008172| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008679| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008043| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007631| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008839| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009880| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007855| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008337| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008630| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008305| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007994| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008208| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006644| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006795| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006360| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006358| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007537| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007083| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006552| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006584| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006597| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006151| lr:0.001000\n",
      "Number data points 5367 from 2001-12-13 to 2023-04-11\n",
      "CNC training\n",
      "Epoch[1/50] | loss train:0.052756| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012269| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010816| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011226| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009605| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009593| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009293| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009294| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009434| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009148| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009305| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008956| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009446| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008213| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009406| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008581| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008968| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009495| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008915| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008686| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008665| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007768| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008586| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008226| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008260| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008422| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008587| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008528| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008188| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008081| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008009| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008005| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009524| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008135| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008121| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008135| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008523| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008134| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008035| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007086| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006836| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007136| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006795| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006963| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006687| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006828| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006785| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006761| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007133| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CNP training\n",
      "Epoch[1/50] | loss train:0.062604| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013518| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011397| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011566| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012056| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011007| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010555| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010784| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010103| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010180| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009693| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009811| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011033| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010575| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009473| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010168| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010331| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009734| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009443| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009166| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009937| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009805| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008556| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009654| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009614| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009495| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009436| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010044| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008524| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010095| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008760| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009365| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008832| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008565| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008593| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007693| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008085| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007758| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007636| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007376| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007543| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007418| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007220| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007216| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007573| lr:0.001000\n",
      "Number data points 1248 from 2018-04-26 to 2023-04-11\n",
      "CDAY training\n",
      "Epoch[1/50] | loss train:0.049147| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.021724| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.082111| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010195| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005997| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008911| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.021824| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.032831| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008872| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007908| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.023056| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005746| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004716| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.026729| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011597| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007205| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013470| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008199| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/50] | loss train:0.008131| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.036995| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008194| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012776| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009662| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006857| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011805| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009283| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006277| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005573| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005471| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010644| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009205| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.015483| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007631| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013394| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005336| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010394| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005838| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005248| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013844| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007265| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005859| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004314| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005256| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.024984| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010579| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009472| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007199| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009297| lr:0.001000\n",
      "Number data points 4446 from 2005-08-11 to 2023-04-11\n",
      "CF training\n",
      "Epoch[1/50] | loss train:0.065091| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015166| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013211| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011520| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011884| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014102| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011717| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011331| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012572| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012160| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013307| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011367| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010478| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009454| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010748| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011702| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009449| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009679| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010739| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009593| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009550| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009672| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010284| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009384| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010411| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011149| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009374| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009898| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010044| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008865| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009863| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009789| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010734| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009967| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009294| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010899| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010921| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009660| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007346| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007864| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007017| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007356| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007120| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007103| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007564| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007266| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006599| lr:0.001000\n",
      "Number data points 5735 from 2000-06-23 to 2023-04-11\n",
      "CRL training\n",
      "Epoch[1/50] | loss train:0.091918| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020808| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016249| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011748| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.019285| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015349| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012484| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010611| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013427| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011532| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.017042| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011259| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011781| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010907| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.015606| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013542| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014022| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011099| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013223| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012978| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014235| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011893| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.016953| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012454| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010246| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012837| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012653| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011358| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011711| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009685| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010288| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012493| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009627| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011435| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011500| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012180| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011022| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010723| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011414| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009857| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008640| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008831| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009085| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008057| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008597| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007796| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008846| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007426| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008057| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SCHW training\n",
      "Epoch[1/50] | loss train:0.081948| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018552| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014426| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014245| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013941| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014102| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012699| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012520| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014773| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010707| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011262| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010243| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012901| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010726| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011964| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011102| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011784| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011059| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012261| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013778| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011856| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010899| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012600| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012044| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012794| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010587| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011293| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011871| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010916| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011309| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010442| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010742| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011737| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011041| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010298| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/50] | loss train:0.010452| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010453| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011379| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010113| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009438| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008420| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009187| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008297| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009105| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007928| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008438| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008151| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008330| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008539| lr:0.001000\n",
      "Number data points 3339 from 2010-01-05 to 2023-04-11\n",
      "CHTR training\n",
      "Epoch[1/50] | loss train:0.037136| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010057| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007977| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006984| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007393| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007923| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005715| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005626| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008255| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005039| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006543| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007277| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005146| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006316| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007136| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006386| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005712| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005582| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005927| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007170| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006474| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005345| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005671| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005825| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005836| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005223| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005814| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005638| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005925| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006286| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005570| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005472| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005374| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005484| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005367| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005479| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005329| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005159| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005815| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005612| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004700| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004650| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004447| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004675| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004394| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004505| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004477| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004377| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004444| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004501| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CVX training\n",
      "Epoch[1/50] | loss train:0.069236| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014642| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015235| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012219| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012086| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011674| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015005| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012917| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012237| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011476| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013265| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011424| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011053| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011206| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012543| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010520| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013435| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012054| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010029| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010318| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011509| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010395| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010244| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011852| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010086| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009978| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010856| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011722| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011529| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009681| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010890| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010617| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010403| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010359| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009772| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012008| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010122| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009369| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010656| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011534| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008275| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008426| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007950| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008317| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007759| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008002| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007895| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008211| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008161| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007878| lr:0.001000\n",
      "Number data points 4331 from 2006-01-26 to 2023-04-11\n",
      "CMG training\n",
      "Epoch[1/50] | loss train:0.066553| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011337| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012946| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011822| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010607| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010308| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010111| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008390| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009470| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010102| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009157| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009113| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010160| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008796| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008435| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008286| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007475| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008052| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007843| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008848| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007790| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008230| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008551| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008045| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007735| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008253| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008375| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007449| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009011| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008167| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007806| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009549| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008270| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007258| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007992| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007096| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006614| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007236| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007473| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006584| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006469| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006580| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006287| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006196| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006164| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006392| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006096| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006272| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006189| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CB training\n",
      "Epoch[1/50] | loss train:0.058051| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014480| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/50] | loss train:0.012992| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012146| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012464| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011501| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011103| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010564| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011488| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012880| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010808| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009568| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010893| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009947| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009179| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009289| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011012| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009801| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009436| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010538| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010766| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008598| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008864| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009214| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010269| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009220| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009519| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009044| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008407| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009919| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009265| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009167| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009339| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009677| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009093| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008756| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008717| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008440| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007969| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007152| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007842| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007994| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007435| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007133| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007792| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007362| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007694| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CHD training\n",
      "Epoch[1/50] | loss train:0.074760| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011987| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012380| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010249| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009563| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010507| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010105| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009277| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010588| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008167| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009179| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008802| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009599| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008358| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008632| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008750| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008545| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008044| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008541| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008264| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008647| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008360| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010058| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008552| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008152| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007945| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009107| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008404| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009447| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008012| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008951| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008073| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008396| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008405| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008748| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008119| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008619| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007881| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007061| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007507| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006785| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006573| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006383| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006627| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006768| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006598| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006771| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006597| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CI training\n",
      "Epoch[1/50] | loss train:0.075614| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014128| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013828| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013727| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011295| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012876| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011381| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011298| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010346| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010925| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012319| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011370| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010033| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012174| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009418| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010771| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010464| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009841| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010092| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009955| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010292| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009677| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009824| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010189| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008659| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010113| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010088| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010589| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008884| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009334| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010302| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009018| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009459| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009699| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009503| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009926| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007682| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007674| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007949| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007615| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007807| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007490| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007773| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007437| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007418| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007684| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CINF training\n",
      "Epoch[1/50] | loss train:0.071861| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017435| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014630| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013356| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012086| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012113| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011748| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011758| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010687| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010459| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012249| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012744| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010122| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011418| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009652| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010015| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011092| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010413| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010377| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011166| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/50] | loss train:0.010827| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010216| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010574| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009290| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009813| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010013| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010149| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009681| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010602| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009574| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010200| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010331| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009978| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010081| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010859| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009029| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008164| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007984| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008429| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008002| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007923| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007906| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008162| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007491| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007771| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007706| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CTAS training\n",
      "Epoch[1/50] | loss train:0.055373| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012591| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012866| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012003| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012101| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012608| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010286| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011411| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010437| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011300| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010129| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011104| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010962| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010015| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010731| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011853| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009091| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010022| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009664| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009148| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009331| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009435| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008990| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009535| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009540| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009142| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009670| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009214| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009382| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009078| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009011| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008699| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008833| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009176| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008123| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008036| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007033| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007260| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007558| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007006| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006895| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006466| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007739| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006689| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006817| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007220| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CSCO training\n",
      "Epoch[1/50] | loss train:0.062899| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017326| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016247| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013911| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013258| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012599| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012789| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012749| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011728| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013629| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011853| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011481| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012690| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012175| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011789| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010739| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013063| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011979| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011984| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011973| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011277| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012399| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011438| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010991| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011327| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011439| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011437| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010810| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011346| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011834| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011448| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010870| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011901| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011020| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011945| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012565| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011602| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010879| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010852| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011275| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009787| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009822| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009322| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009218| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009203| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009351| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008656| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009395| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009132| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009179| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "C training\n",
      "Epoch[1/50] | loss train:0.059086| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011540| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010094| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010161| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010313| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009198| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009926| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008490| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009060| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009126| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009003| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010245| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008895| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008944| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009250| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008112| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009200| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008958| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008372| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008794| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008440| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009153| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009099| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008399| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008481| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008677| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008035| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009245| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008305| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008846| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009059| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008406| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008026| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008035| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008607| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008201| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008038| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/50] | loss train:0.008271| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008509| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006812| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007066| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007005| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007043| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006989| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006931| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007360| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006964| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007354| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007395| lr:0.001000\n",
      "Number data points 2151 from 2014-09-24 to 2023-04-11\n",
      "CFG training\n",
      "Epoch[1/50] | loss train:0.052320| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011901| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012349| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010712| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008431| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008723| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008567| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008276| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008244| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007968| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006649| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008311| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008544| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009412| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007567| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008862| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008642| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008425| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007570| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007448| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010153| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007325| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010696| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009069| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007330| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007202| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007914| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008702| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008472| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008567| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007948| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009777| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008922| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008234| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008347| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007082| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006754| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006852| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006810| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006096| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005669| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006006| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005213| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006159| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006138| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005253| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005658| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005862| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CLX training\n",
      "Epoch[1/50] | loss train:0.064383| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011932| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010498| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009881| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010347| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010020| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010013| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010902| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009375| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010182| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008848| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009841| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009368| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009751| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009733| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009115| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009859| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009179| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008846| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008521| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009245| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008833| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010025| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008433| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008893| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009275| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008610| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008510| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008855| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009129| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008547| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008811| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007910| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008619| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007616| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007473| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007216| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006890| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007342| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006745| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006928| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007238| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006888| lr:0.001000\n",
      "Number data points 5120 from 2002-12-06 to 2023-04-11\n",
      "CME training\n",
      "Epoch[1/50] | loss train:0.060567| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012791| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011332| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008460| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009517| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010216| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009145| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007758| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008689| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008435| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009130| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008433| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008713| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008693| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008301| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008104| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008680| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008697| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009506| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007236| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008084| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008200| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008214| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008070| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007382| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007560| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008300| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008184| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008868| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007898| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007867| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007852| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007864| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008094| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007268| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007621| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008671| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007931| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006498| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006743| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006588| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006672| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006361| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006485| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005880| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006728| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006364| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006397| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CMS training\n",
      "Epoch[1/50] | loss train:0.071757| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013647| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012920| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011209| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011401| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009097| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009873| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010116| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009434| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009357| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009509| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009824| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008698| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009004| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008871| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009070| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008937| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008977| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008118| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007883| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008856| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008160| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008600| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009037| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008556| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008454| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008397| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008492| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009188| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008501| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008325| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008663| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008212| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008074| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008937| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007680| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007030| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007000| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007321| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006774| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007057| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006851| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006950| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006908| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006780| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "KO training\n",
      "Epoch[1/50] | loss train:0.055605| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014942| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015208| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011031| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011147| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010432| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010517| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009559| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009674| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008958| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009973| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010431| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009882| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009310| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009502| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009443| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009144| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008790| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009058| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009431| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008745| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009730| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009532| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008764| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009824| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008598| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009980| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009080| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008241| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008239| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009463| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008672| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008818| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008065| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009089| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008281| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008551| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008695| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007690| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006870| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006649| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007009| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006753| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006802| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007199| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006494| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007024| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006879| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CTSH training\n",
      "Epoch[1/50] | loss train:0.058422| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013632| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011054| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010675| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010017| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010103| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010563| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010496| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008867| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009602| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009507| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009702| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009585| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009428| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010003| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008779| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008321| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008804| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009188| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009068| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008937| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008435| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008279| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008466| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008387| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008422| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008154| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008990| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009030| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008281| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008394| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008606| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008432| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008767| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008533| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008492| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008606| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008507| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007330| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007233| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007300| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006866| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007286| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007112| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007148| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007079| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007257| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007340| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CL training\n",
      "Epoch[1/50] | loss train:0.055468| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013824| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010366| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011607| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009999| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009739| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009263| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008264| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009041| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008877| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008247| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008829| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008522| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008362| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008494| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008404| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008312| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008775| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009031| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/50] | loss train:0.008200| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008211| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008422| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008168| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008110| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008124| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007979| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008133| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008544| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007964| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008273| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007916| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007888| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007444| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008868| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007510| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008344| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007454| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006803| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007072| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006552| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006751| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006933| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006925| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006632| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006605| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007074| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CMCSA training\n",
      "Epoch[1/50] | loss train:0.065084| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013920| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013382| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011079| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009880| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011000| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010240| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011494| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009287| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010608| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009827| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009080| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009157| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010493| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010185| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009421| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009097| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010339| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009765| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009900| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009415| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009109| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009683| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009979| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008786| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010149| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009181| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008581| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009754| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009353| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008009| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008827| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008636| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008476| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008223| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008381| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009119| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008019| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007390| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007578| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007302| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007276| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007495| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007241| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007671| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007133| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007205| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006753| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CMA training\n",
      "Epoch[1/50] | loss train:0.062865| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019669| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016303| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015006| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015554| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014120| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013637| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013216| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.015599| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014350| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012461| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013877| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012960| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012780| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013579| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012811| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012388| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013169| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012285| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012789| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013901| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011088| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012695| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013425| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011720| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013066| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012134| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011829| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012099| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011668| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012051| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012006| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011763| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012940| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011380| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012408| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011104| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012736| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011744| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011298| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010495| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010252| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009391| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009587| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009853| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009536| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009228| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009252| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009846| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009456| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CAG training\n",
      "Epoch[1/50] | loss train:0.077959| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014214| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012452| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012259| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010957| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009554| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010745| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010006| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009776| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010389| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009307| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009514| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009422| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009586| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009445| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009570| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009875| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009461| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009559| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009344| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009145| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009069| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009213| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008918| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009360| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009085| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008366| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008886| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008846| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008968| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009032| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009030| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009053| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008591| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008235| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/50] | loss train:0.007587| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007967| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007650| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007634| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007694| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007351| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007467| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007810| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007127| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007306| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "COP training\n",
      "Epoch[1/50] | loss train:0.082072| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020265| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.021613| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015452| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.018397| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015931| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.018935| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014861| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013572| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015580| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013628| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014280| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012212| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011983| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014896| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014031| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012940| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013403| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012424| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013992| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013763| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013176| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013930| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012481| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013779| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012314| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014643| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012925| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012905| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013579| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011948| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011440| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012895| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012299| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013437| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012360| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012013| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010896| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.014280| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011630| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010087| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009456| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009457| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009287| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008665| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008438| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008799| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008261| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008536| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008200| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ED training\n",
      "Epoch[1/50] | loss train:0.069780| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013487| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012631| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011220| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010518| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009345| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010294| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010830| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009520| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008676| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008972| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010439| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008868| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008603| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009045| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010026| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010484| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008668| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008997| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009068| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008830| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008346| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009249| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008912| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009067| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009067| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009160| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008471| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008885| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008514| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008136| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008240| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008623| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008104| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008406| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008712| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007269| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007047| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007103| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006850| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006815| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006829| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006719| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007081| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006745| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "STZ training\n",
      "Epoch[1/50] | loss train:0.063367| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011807| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009723| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009418| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009005| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009396| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008492| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009513| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008256| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008281| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008828| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008820| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008123| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008453| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008729| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007846| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007869| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009452| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008220| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007964| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008709| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008708| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008074| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007798| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007837| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008208| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007766| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008128| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007527| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007898| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009020| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008425| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007844| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007697| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007838| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008117| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007681| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008798| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007222| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006639| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006791| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006532| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006613| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006575| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006472| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006659| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006462| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006673| lr:0.001000\n",
      "Number data points 298 from 2022-02-02 to 2023-04-11\n",
      "CEG training\n",
      "Epoch[1/50] | loss train:0.038030| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014088| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008004| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007191| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004649| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004049| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/50] | loss train:0.004265| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006634| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006858| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005646| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.003230| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.003237| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.003554| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.003545| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.002166| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.002330| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.002994| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.003761| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.001964| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006296| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.002927| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004255| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003456| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004426| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006878| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004739| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.003823| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.002475| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004141| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003263| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.002522| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.002656| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003983| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005050| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.002728| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003090| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004335| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003223| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.002387| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003053| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.002550| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.002654| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.002203| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.002273| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.002264| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002566| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002522| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.002207| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.001805| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003431| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "COO training\n",
      "Epoch[1/50] | loss train:0.067764| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014574| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010918| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012595| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010264| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010692| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010570| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010252| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009961| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009972| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010935| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010221| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009076| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008645| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008243| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009123| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009506| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009574| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009421| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008480| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010723| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008201| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008876| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009396| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009250| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008007| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008700| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009428| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008540| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008477| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008007| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008794| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008565| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008630| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008145| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008736| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007141| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007235| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007055| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007179| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007124| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007391| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006750| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006749| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006923| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006645| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CPRT training\n",
      "Epoch[1/50] | loss train:0.061040| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013971| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013086| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010915| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011150| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010543| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010483| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011377| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009427| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011725| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008473| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009881| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010154| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008890| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009309| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010400| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008788| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008734| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008931| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010733| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010211| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009864| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009392| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010210| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009146| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008839| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008399| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008580| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008224| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009606| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008253| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008746| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007725| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008563| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008004| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007288| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007023| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006992| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006638| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007430| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006968| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006610| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007107| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006517| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006478| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GLW training\n",
      "Epoch[1/50] | loss train:0.071280| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.023445| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.024910| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.022229| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.020293| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.021671| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.017327| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.017087| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.020618| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.017106| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.016826| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.015769| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.019213| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.018571| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.018709| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.015606| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.016976| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.015966| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.017830| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.017751| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.016161| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.016799| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.016879| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.018144| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[25/50] | loss train:0.015554| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.017537| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014698| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.016154| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.015515| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.015652| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.016425| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.017334| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.015489| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.015782| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.016574| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.016762| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.016787| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.015361| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.014857| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.014797| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.014161| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.012662| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.012706| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.012483| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011923| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011879| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.012070| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.012204| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.012165| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011903| lr:0.001000\n",
      "Number data points 977 from 2019-05-24 to 2023-04-11\n",
      "CTVA training\n",
      "Epoch[1/50] | loss train:0.042505| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008482| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.005603| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.004096| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.003981| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.003810| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.003135| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.003345| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.003069| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.003021| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.002771| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.002868| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005041| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.003554| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.002712| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.002286| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.002328| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.002618| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.002566| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.002766| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.002649| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.002648| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.002794| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.002251| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.002418| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.002324| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.002394| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.002249| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.002535| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.002544| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.002444| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.002256| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.002241| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.002270| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.002187| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.002168| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.002121| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.002382| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.002857| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.002526| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.002119| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.001823| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.001976| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.002212| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.001998| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002016| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002004| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.001890| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.001855| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.001960| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CSGP training\n",
      "Epoch[1/50] | loss train:0.066260| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013946| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012281| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013582| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011555| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013619| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010498| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011285| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009478| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010168| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009788| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011276| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009982| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011315| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010850| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008644| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010575| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009349| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009896| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008742| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010018| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009200| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011256| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009260| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009249| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009530| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009737| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010035| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009270| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009831| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009967| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008580| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008360| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009054| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009280| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010646| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008687| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009293| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007080| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007543| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006946| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007177| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007229| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007134| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007248| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007608| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007398| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006821| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "COST training\n",
      "Epoch[1/50] | loss train:0.068588| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016907| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013123| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013151| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010997| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011110| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012189| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010050| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011988| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012160| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009711| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009388| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009901| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011098| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011963| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010265| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010000| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010065| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011256| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009627| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008922| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010528| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009740| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009919| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009193| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009975| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009079| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009159| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009720| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009522| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008975| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008639| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009213| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009156| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009638| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009075| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009423| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007717| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007454| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/50] | loss train:0.007058| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007185| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007199| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006901| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007251| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007359| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007493| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007653| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CTRA training\n",
      "Epoch[1/50] | loss train:0.056579| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015989| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013793| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012277| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011109| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012261| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011018| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012073| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011108| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010531| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010286| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010657| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010909| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011608| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010509| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009477| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010826| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009900| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010704| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010936| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010821| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010269| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010524| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009844| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009452| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010408| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010641| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010200| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009715| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010945| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009741| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010545| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010024| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009407| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010510| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010258| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010274| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009797| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010031| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008655| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008488| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008459| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008210| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008190| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008567| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008322| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008507| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008252| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008400| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CCI training\n",
      "Epoch[1/50] | loss train:0.055225| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012939| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012363| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010445| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011268| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009919| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010387| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010699| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010901| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010562| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009514| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009628| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009685| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009372| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009710| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009174| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010103| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009399| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008788| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008962| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010461| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009238| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008386| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009034| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008590| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009831| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009063| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009075| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009821| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008879| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008862| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009522| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008458| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009601| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008679| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007713| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007943| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006996| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007014| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007196| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006651| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006736| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007013| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006823| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007174| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CSX training\n",
      "Epoch[1/50] | loss train:0.059808| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013303| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013037| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012424| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011055| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010342| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010820| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011947| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010110| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009620| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010114| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009086| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008953| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011331| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009961| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010016| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009733| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009102| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010228| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010533| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008622| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009072| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009294| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009917| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009823| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009347| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009726| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009837| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008262| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009782| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009053| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008509| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010361| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009133| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008875| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008713| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008790| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009651| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007866| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007474| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007095| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007565| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007113| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007187| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007273| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007010| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007478| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007301| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CMI training\n",
      "Epoch[1/50] | loss train:0.074736| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016258| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013631| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011300| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012281| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010004| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010919| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010307| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009663| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009646| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010768| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010220| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009460| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010052| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009689| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010258| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009372| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008825| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008763| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009076| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008860| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009149| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009776| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009238| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009356| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009347| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009059| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009053| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009386| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008983| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009408| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008770| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009208| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008094| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009547| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007521| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007346| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007177| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007457| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007172| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007405| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007028| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006838| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006952| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007018| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "CVS training\n",
      "Epoch[1/50] | loss train:0.056746| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012558| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011023| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010937| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011916| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010757| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011153| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011061| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009223| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008923| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009989| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009626| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009159| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010026| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009329| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009572| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009243| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009549| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009641| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009160| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009345| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009089| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009320| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009460| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010075| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009282| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008323| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009408| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008604| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008901| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009042| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008818| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009479| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009293| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009161| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009067| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008120| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007879| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007486| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007284| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007378| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007420| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007299| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007239| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007195| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007436| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007165| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DHI training\n",
      "Epoch[1/50] | loss train:0.066872| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016010| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014568| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013095| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013054| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013823| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012846| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012933| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013156| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010611| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011733| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010897| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011254| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010530| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011761| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010187| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010685| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011993| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011456| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010576| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010499| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010966| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011536| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011332| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010789| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010890| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010072| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010121| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011472| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010009| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010213| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010996| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010078| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010665| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010902| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012143| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011100| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009951| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010091| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008123| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008060| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008067| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008356| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007843| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008012| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007711| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007710| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007578| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008313| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DHR training\n",
      "Epoch[1/50] | loss train:0.070639| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019318| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014688| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.017310| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010992| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010722| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012503| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012500| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011281| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010896| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010072| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012305| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009523| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010554| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009101| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009564| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009823| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009431| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009213| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009120| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011316| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009603| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008862| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011033| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009136| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/50] | loss train:0.009760| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010544| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010101| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008396| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009557| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009461| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009621| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009702| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008730| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009177| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009002| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009916| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008060| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008011| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007473| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006946| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007551| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007452| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007213| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007306| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007045| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006935| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DRI training\n",
      "Epoch[1/50] | loss train:0.065916| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014817| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013336| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011352| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011471| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010847| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014364| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010487| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010174| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010101| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008951| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011189| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009822| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011181| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010416| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009408| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010710| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009780| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009161| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009269| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008944| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010532| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009932| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009864| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010445| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009549| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010330| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009312| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009132| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009611| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010283| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008814| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008912| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009154| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009774| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009207| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009967| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010597| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009612| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008117| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007637| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007564| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007143| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006979| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007388| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007361| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007631| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007136| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007656| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DVA training\n",
      "Epoch[1/50] | loss train:0.056849| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013374| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012031| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013182| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012729| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012461| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011686| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010957| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010329| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010950| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010967| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010456| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010438| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010458| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010301| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011401| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011620| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011312| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009810| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010027| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009516| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010383| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010240| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010364| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009892| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010129| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009284| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010035| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010172| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009650| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009678| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009862| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009621| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009186| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009259| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009324| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008549| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008103| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007575| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007281| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007535| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007396| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007644| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007521| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007671| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007672| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007568| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DE training\n",
      "Epoch[1/50] | loss train:0.058058| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016250| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011357| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014298| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010606| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011597| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010669| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010601| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011362| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010138| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010122| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010728| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010670| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009880| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010823| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009626| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010018| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011710| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010249| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010084| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010761| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010771| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009097| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009216| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009768| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011471| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012275| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008707| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009950| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008938| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009272| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010206| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008857| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009145| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009157| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008786| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008987| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009290| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009343| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007893| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007612| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007416| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007412| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/50] | loss train:0.007628| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007626| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007314| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007030| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006820| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006886| lr:0.001000\n",
      "Number data points 4013 from 2007-05-03 to 2023-04-11\n",
      "DAL training\n",
      "Epoch[1/50] | loss train:0.057843| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014534| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012058| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009002| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009382| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008797| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008307| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008435| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008681| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008450| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008177| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008326| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008534| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008168| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008302| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008142| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007594| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007458| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008127| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007607| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009086| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007907| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008352| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007911| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007517| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007973| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008284| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007630| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007411| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007010| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008074| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007492| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008640| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007877| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007161| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007215| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007159| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007918| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008220| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007879| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006394| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006933| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006930| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007041| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006419| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006272| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006165| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006362| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006348| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006693| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "XRAY training\n",
      "Epoch[1/50] | loss train:0.067001| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015299| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014437| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012702| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012331| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012469| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010602| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011368| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011133| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011249| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010354| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012227| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011134| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010878| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011673| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010268| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011113| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011208| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010662| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010608| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010802| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010712| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010251| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009627| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010531| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010526| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010343| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009833| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011290| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010686| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009969| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010283| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010753| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010371| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009977| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009536| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010192| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010531| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009957| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008906| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008857| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008450| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008607| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008678| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008625| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008216| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008439| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008800| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008404| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DVN training\n",
      "Epoch[1/50] | loss train:0.069781| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020089| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018209| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016744| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015845| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015986| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.017756| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014082| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013666| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.016250| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014787| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014772| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014910| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015151| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013901| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014117| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014988| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014744| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.015838| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014349| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014513| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014624| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.014696| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014278| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.015432| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012931| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014584| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014124| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014092| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013925| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013577| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.014220| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013023| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013826| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013439| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.014729| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013307| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.014311| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013147| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013344| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.012208| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011363| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011693| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011343| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011157| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011422| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010994| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011435| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011666| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010795| lr:0.001000\n",
      "Number data points 4529 from 2005-04-14 to 2023-04-11\n",
      "DXCM training\n",
      "Epoch[1/50] | loss train:0.084034| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015986| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015620| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011747| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010580| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009537| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009090| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008884| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010147| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/50] | loss train:0.008625| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008693| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009000| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008907| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010176| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009456| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009223| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008900| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008712| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008305| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010045| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009065| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009075| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008904| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008420| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008558| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008414| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008601| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008209| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008810| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009102| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008722| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008616| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008653| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008537| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008345| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008247| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008000| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008291| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006821| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006606| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006910| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006161| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006481| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006126| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006812| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006557| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006493| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007000| lr:0.001000\n",
      "Number data points 2639 from 2012-10-12 to 2023-04-11\n",
      "FANG training\n",
      "Epoch[1/50] | loss train:0.068809| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013980| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010190| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009200| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009944| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008038| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008127| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008194| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008623| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008613| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007925| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007705| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008177| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007251| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008917| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008227| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007336| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007902| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009011| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007589| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008131| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007145| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007683| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006903| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006855| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007013| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007177| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007386| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006612| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007338| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007533| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007459| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008148| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007415| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007599| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007201| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007723| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007521| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006713| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007403| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006393| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006354| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006123| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006232| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005680| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005989| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006154| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005711| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005918| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005962| lr:0.001000\n",
      "Number data points 4643 from 2004-10-29 to 2023-04-11\n",
      "DLR training\n",
      "Epoch[1/50] | loss train:0.069227| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011643| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012172| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010645| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010521| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011302| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010512| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007592| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007950| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009061| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008825| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008305| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009052| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007972| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010638| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009503| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008115| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008201| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008135| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007901| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007580| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008973| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007830| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008137| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009604| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007816| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009474| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008715| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008224| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007734| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007756| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011315| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009797| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008211| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008278| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010730| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009807| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007546| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008969| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007982| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006698| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011511| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007895| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006252| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006361| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006773| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006566| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007061| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006932| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006660| lr:0.001000\n",
      "Number data points 3984 from 2007-06-14 to 2023-04-11\n",
      "DFS training\n",
      "Epoch[1/50] | loss train:0.049700| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011313| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011491| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008449| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009734| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008354| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008649| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009883| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007930| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007986| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009352| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008068| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007863| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007397| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008012| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007704| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006927| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007463| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007673| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006567| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008264| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006848| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008201| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007174| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007848| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007521| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007505| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007137| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[29/50] | loss train:0.006701| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007058| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007290| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007250| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007369| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007196| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007213| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007163| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007002| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007450| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006935| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007230| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006100| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006417| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006099| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005965| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005422| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005781| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005987| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005583| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005860| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005739| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DISH training\n",
      "Epoch[1/50] | loss train:0.069211| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019607| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016742| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016046| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014692| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015691| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015805| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015053| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014263| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015575| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012662| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.016031| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012456| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015254| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012427| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013065| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014642| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014012| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013264| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014916| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013227| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012861| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013054| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013879| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012373| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013783| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012421| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013033| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013169| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013770| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013183| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012921| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013489| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012684| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012721| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013394| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013239| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012898| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012312| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013273| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011326| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011254| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010578| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010635| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010615| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010455| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010127| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010553| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010469| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010229| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DIS training\n",
      "Epoch[1/50] | loss train:0.053162| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015087| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014546| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014059| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011780| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011577| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012222| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010048| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010565| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010309| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010394| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011301| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009550| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010385| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010163| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010021| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010113| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010536| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010383| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009751| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010529| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009205| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010432| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009759| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010647| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010373| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009275| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010435| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009170| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009926| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010154| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009544| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008772| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009493| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010096| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008593| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008814| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009686| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007956| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007761| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007770| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007351| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007504| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007543| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007531| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007774| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007845| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007777| lr:0.001000\n",
      "Number data points 3373 from 2009-11-13 to 2023-04-11\n",
      "DG training\n",
      "Epoch[1/50] | loss train:0.045278| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011613| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007681| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007996| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007504| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006797| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005995| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006185| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006376| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007768| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006604| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007579| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005821| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006096| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005692| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005941| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006286| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007375| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006199| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005693| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007702| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006976| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005775| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006056| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005887| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004891| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006276| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005285| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005255| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006159| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006836| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005763| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005353| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006211| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006104| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005183| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006268| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004934| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007155| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005796| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004622| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005071| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004287| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004517| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004526| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004237| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/50] | loss train:0.004959| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004528| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004376| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004496| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DLTR training\n",
      "Epoch[1/50] | loss train:0.057706| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014526| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013418| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011172| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011967| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010754| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010749| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009867| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009992| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009818| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009575| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010598| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010257| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010371| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009414| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009477| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009761| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009848| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010370| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010039| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010201| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010064| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009103| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009972| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008924| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010027| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009507| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009527| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008765| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010478| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008414| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009911| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008870| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009289| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008693| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008580| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009672| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007715| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007898| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007690| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007503| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007762| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007346| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007627| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008084| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007259| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007837| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "D training\n",
      "Epoch[1/50] | loss train:0.058604| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015258| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012065| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010153| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011814| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010415| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010699| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008972| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009588| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009685| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009046| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008660| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009572| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009523| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009541| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008509| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009700| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007980| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008638| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008808| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008893| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009234| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008641| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008359| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008617| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008741| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008969| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008722| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008571| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008483| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008433| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007924| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008586| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008788| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008476| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008154| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008672| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008023| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007795| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007623| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006977| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007165| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007117| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007241| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007023| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006778| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006763| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006885| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006950| lr:0.001000\n",
      "Number data points 4720 from 2004-07-13 to 2023-04-11\n",
      "DPZ training\n",
      "Epoch[1/50] | loss train:0.068201| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012733| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010683| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009122| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009999| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008641| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010096| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008234| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009610| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009964| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007806| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008240| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007736| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008718| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007693| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007862| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008789| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008982| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010627| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008003| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008060| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008271| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008203| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008276| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007751| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008420| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008259| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007688| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009019| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008605| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007888| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007420| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007577| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007203| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008409| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007923| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007594| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007835| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007591| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007447| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006227| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006432| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005998| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006217| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006136| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006417| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006351| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006050| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005866| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006131| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DOV training\n",
      "Epoch[1/50] | loss train:0.083449| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016824| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017174| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012620| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014700| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010887| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010980| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011639| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012603| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012000| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011707| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010325| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/50] | loss train:0.011778| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011341| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011244| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010805| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011081| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011667| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010277| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010540| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011240| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010356| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010948| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010199| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009712| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010360| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010636| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010556| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009881| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011029| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010342| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009725| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010013| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009460| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010626| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009961| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011931| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010112| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008403| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008287| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008045| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008118| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008025| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008014| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007507| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007231| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008068| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007911| lr:0.001000\n",
      "Number data points 1023 from 2019-03-20 to 2023-04-11\n",
      "DOW training\n",
      "Epoch[1/50] | loss train:0.055589| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012227| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009093| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007382| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006846| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006507| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007004| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005891| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005539| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006756| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006852| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005566| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005613| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005452| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005750| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005676| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005265| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005244| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005143| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005165| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005199| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006389| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007596| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006284| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005204| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006053| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005245| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005084| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004918| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004617| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005165| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005407| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005705| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005014| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004927| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004786| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005089| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005146| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004832| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005107| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004686| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004395| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004462| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004373| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004499| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004325| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004540| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004366| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004246| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004335| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DTE training\n",
      "Epoch[1/50] | loss train:0.061388| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017725| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012833| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010760| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011679| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010601| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009421| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010344| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010082| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009811| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010082| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008670| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008763| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008421| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008776| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008280| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009530| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009674| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008163| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009017| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008875| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009233| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008097| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008497| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008173| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008343| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009917| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008204| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008602| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008197| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007920| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008865| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008249| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007937| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008187| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007848| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007383| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006962| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006993| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006930| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006941| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006886| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006875| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006826| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007034| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007051| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DUK training\n",
      "Epoch[1/50] | loss train:0.076303| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015357| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013359| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011954| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010693| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010626| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009384| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011094| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009855| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009847| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010170| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009877| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009273| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009531| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008595| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010651| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009079| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008757| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008617| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008780| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009447| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008629| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009602| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009188| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008880| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009678| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008981| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008592| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[31/50] | loss train:0.008931| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008353| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008413| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009121| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008488| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008158| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008722| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008846| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008756| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008080| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007617| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006957| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006942| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007163| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006901| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006897| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006938| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006824| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006522| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006858| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DD training\n",
      "Epoch[1/50] | loss train:0.066520| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015539| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014467| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012761| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012558| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013563| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012550| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012277| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012557| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014552| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011598| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012326| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011667| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011386| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012703| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011672| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012898| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011391| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011728| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010869| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011569| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011641| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012077| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012367| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011259| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010885| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011404| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012148| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010723| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011943| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010795| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011181| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011491| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011821| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010996| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010544| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010924| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010915| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010327| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009747| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008824| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009210| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009157| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009171| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009062| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009007| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008539| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009033| lr:0.001000\n",
      "Number data points 1558 from 2017-02-01 to 2023-04-11\n",
      "DXC training\n",
      "Epoch[1/50] | loss train:0.060894| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009377| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.005855| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005581| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004591| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004032| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.004210| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004221| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.003446| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.003639| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.003632| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.003921| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.003507| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.003591| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.003938| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.003806| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.003050| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.003193| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.003293| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.003282| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.003516| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.003435| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003450| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.003210| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.003002| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003052| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004281| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003465| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.002984| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003693| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003257| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003064| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003039| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003219| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.003393| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003506| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003076| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003636| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003238| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003146| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003378| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.002742| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.002624| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.002723| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.002717| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002635| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002690| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.002690| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.002574| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.002563| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EMN training\n",
      "Epoch[1/50] | loss train:0.067243| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014421| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011687| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012300| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010709| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011497| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010742| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011412| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010472| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011175| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010471| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010336| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010363| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011026| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010472| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011207| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009555| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009711| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009960| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010796| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009271| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009782| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009771| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010880| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010108| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009365| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009457| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009704| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009910| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009998| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009345| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009746| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010014| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009451| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009654| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009420| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009102| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007840| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007831| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007839| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008159| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007959| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007982| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007752| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007868| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[49/50] | loss train:0.007848| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007834| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ETN training\n",
      "Epoch[1/50] | loss train:0.071361| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015907| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013200| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010752| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012402| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009623| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011782| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011512| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012069| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009729| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010018| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010964| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010731| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009884| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010614| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009819| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010412| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010183| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011139| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009966| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009487| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009778| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011247| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009614| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009361| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009943| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009169| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009419| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010014| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009260| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009495| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009507| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009128| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009834| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009114| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009622| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007833| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007579| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007366| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007185| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007550| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007149| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007285| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006861| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006609| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007278| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EBAY training\n",
      "Epoch[1/50] | loss train:0.069662| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015725| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015000| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.017165| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014389| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012443| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012518| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012670| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011926| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012354| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013147| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011636| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012054| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011616| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012977| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010430| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011876| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010339| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011777| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011834| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011196| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011433| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011980| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011837| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011146| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011109| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011070| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010487| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009989| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012041| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010502| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010445| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011059| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011270| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010520| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010561| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011891| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011130| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010640| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010760| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008742| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008268| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008212| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008184| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008205| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008258| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007953| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008404| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007835| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007765| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ECL training\n",
      "Epoch[1/50] | loss train:0.052600| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013358| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011600| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010570| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010056| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010557| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009802| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009581| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009490| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009553| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009644| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009111| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009856| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009983| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008983| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009152| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009062| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008741| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009646| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009902| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008558| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008462| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008607| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009377| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009037| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009144| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008641| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008666| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008436| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008983| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008689| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009346| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008813| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008527| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008146| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008850| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009580| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008618| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009810| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007623| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007257| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007513| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007362| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006867| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007216| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007374| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006954| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007059| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006925| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EIX training\n",
      "Epoch[1/50] | loss train:0.079910| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013805| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012151| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011637| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010770| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009939| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010266| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010308| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010444| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010136| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008489| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009781| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/50] | loss train:0.009027| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010681| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009632| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009437| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010159| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009099| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010159| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009653| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009572| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009108| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008917| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009178| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009118| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009901| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009648| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009213| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009315| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009251| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009429| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008643| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008662| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009696| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009129| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007834| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008072| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007345| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007485| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007244| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007207| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007173| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007618| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007220| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007469| lr:0.001000\n",
      "Number data points 5797 from 2000-03-27 to 2023-04-11\n",
      "EW training\n",
      "Epoch[1/50] | loss train:0.048946| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014440| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015321| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013060| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014662| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011886| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.017475| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013772| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010887| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012202| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011351| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013416| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012897| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011487| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011426| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012846| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012051| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012817| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009730| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011470| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010140| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009835| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010896| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013392| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010930| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009704| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012505| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011890| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011350| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010124| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011773| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009937| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010802| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009801| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010450| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012781| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009285| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010338| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013465| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008498| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009183| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008313| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008056| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007739| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007633| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007376| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008089| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008757| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008373| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EA training\n",
      "Epoch[1/50] | loss train:0.072284| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014055| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013124| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012477| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011206| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009959| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012566| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010901| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010873| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012233| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009929| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010174| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010512| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010646| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009566| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009951| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010435| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010017| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009982| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010010| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009981| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009556| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009236| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009522| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010038| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008902| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009767| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009820| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008706| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009446| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009445| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009878| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009156| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009707| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008069| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008004| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007872| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007817| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007454| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007645| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007954| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007888| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007539| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007606| lr:0.001000\n",
      "Number data points 5398 from 2001-10-30 to 2023-04-11\n",
      "ELV training\n",
      "Epoch[1/50] | loss train:0.070394| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013022| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011630| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010883| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011082| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010449| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008915| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008828| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009258| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008619| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009110| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009193| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009150| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011890| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009024| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008232| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010349| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009005| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008585| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009938| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008811| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009062| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008532| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008582| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008972| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009619| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008301| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008670| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008700| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[33/50] | loss train:0.008644| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008188| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008880| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007874| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009060| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007989| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009085| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007593| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006945| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006982| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006376| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006399| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006314| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006256| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006113| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006160| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006748| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006691| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LLY training\n",
      "Epoch[1/50] | loss train:0.086012| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017143| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016161| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014640| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014316| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015224| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015897| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014117| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011444| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014120| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012164| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011259| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009988| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012363| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012549| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.017877| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010606| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010579| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009849| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010431| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009087| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010462| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011942| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012755| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010327| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012324| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008988| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009962| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011581| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011154| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011250| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010340| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012407| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010628| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011394| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010052| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011096| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010491| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008630| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007796| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007692| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007855| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007575| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007604| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007128| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007442| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007529| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008262| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EMR training\n",
      "Epoch[1/50] | loss train:0.076481| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015153| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012917| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012789| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013341| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011886| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011916| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011823| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012396| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010874| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010311| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010977| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010521| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009911| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011497| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010348| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011558| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010428| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011204| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009834| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011326| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010741| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009916| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010021| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009244| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009735| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009415| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010288| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009628| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010508| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008815| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010154| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009332| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010045| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010166| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009429| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009709| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007645| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007827| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008359| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007628| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007977| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007793| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007612| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007900| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007633| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007412| lr:0.001000\n",
      "Number data points 2775 from 2012-03-30 to 2023-04-11\n",
      "ENPH training\n",
      "Epoch[1/50] | loss train:0.055604| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010804| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008368| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008218| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008819| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008268| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006740| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006491| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007291| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007702| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006242| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006171| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006333| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007185| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007408| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005936| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006435| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006707| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006506| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007374| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006341| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006002| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006664| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005790| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005931| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005595| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006986| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005673| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006056| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006276| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005315| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006384| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006573| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006591| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005530| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006897| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005372| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005947| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005398| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006159| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005025| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004971| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004547| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005321| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004739| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005033| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004785| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004888| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004497| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004858| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ETR training\n",
      "Epoch[1/50] | loss train:0.061952| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017989| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013527| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011967| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013449| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011960| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010986| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011027| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011657| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011309| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010838| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011183| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009580| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010269| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011470| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010246| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011739| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012740| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010129| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010459| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010428| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010737| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010132| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010004| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010729| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010167| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009572| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010341| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010170| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010255| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010141| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010209| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010341| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010810| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010063| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008070| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008068| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007639| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008153| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008291| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007692| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008038| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008077| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008073| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008007| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EOG training\n",
      "Epoch[1/50] | loss train:0.059092| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015005| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014705| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013056| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011776| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013307| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012764| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011703| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012725| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011941| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011541| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010816| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011372| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013500| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011819| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011122| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010074| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010662| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011416| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011447| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010485| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010732| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010907| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010983| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010647| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010584| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010292| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010870| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011121| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011144| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010643| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010780| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010528| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009844| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010851| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010295| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010228| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011850| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009045| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009099| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008343| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008416| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008568| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008564| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008275| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008563| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008362| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008469| lr:0.001000\n",
      "Number data points 2811 from 2012-02-08 to 2023-04-11\n",
      "EPAM training\n",
      "Epoch[1/50] | loss train:0.068517| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012294| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009786| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009265| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009593| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008831| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008246| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007497| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007645| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006509| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008135| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006673| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006337| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007390| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006555| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007815| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007175| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007554| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010454| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005716| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006642| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005865| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007159| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005835| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006304| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005640| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007096| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006123| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005796| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007424| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005639| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006236| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006547| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005800| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006764| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006076| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005888| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007377| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006214| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006582| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004880| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004922| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004885| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004474| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004686| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005418| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004802| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004860| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004417| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EQT training\n",
      "Epoch[1/50] | loss train:0.066713| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017969| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012884| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013754| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012748| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013747| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012415| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013406| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012019| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013045| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011457| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011893| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012155| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011902| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011515| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/50] | loss train:0.011392| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011566| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011796| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011235| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010945| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012272| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011324| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010921| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011339| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011794| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011276| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011698| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011296| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011004| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011017| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011041| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011073| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009906| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012349| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010707| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011411| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011285| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009336| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009211| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009214| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009198| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008885| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009087| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008909| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009456| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009158| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009250| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EFX training\n",
      "Epoch[1/50] | loss train:0.065370| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013914| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015537| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014354| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011326| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012519| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011688| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010557| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010933| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009721| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010925| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011732| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010171| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010928| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010569| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009864| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010610| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010135| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009636| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010322| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009750| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010217| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008674| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009825| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009684| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009852| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010504| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009930| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009010| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010188| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009306| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009720| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008798| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009017| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009091| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008720| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007611| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007723| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007670| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007791| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007462| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007173| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007700| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006951| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007437| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007199| lr:0.001000\n",
      "Number data points 5701 from 2000-08-11 to 2023-04-11\n",
      "EQIX training\n",
      "Epoch[1/50] | loss train:0.068570| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018958| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011776| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012203| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009644| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010149| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010342| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012145| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010402| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008927| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010466| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008742| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010642| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009607| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008864| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009223| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009527| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009169| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008903| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008900| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009101| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010594| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008656| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008603| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008858| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009142| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008985| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008338| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008972| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008974| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007956| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008603| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008839| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009568| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008315| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007905| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008340| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007725| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007182| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007015| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007407| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007036| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006725| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007027| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006505| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006747| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007230| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EQR training\n",
      "Epoch[1/50] | loss train:0.077890| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013991| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013604| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012069| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011948| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011617| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010579| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009754| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010985| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009645| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010391| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008751| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010151| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009756| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009539| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009913| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009439| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009745| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009003| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010612| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009540| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009370| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008906| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008972| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009211| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009083| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009119| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009441| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008734| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/50] | loss train:0.008809| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008836| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009489| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009481| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009068| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007987| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007544| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007427| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007240| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007422| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007502| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007346| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007157| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007418| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006980| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ESS training\n",
      "Epoch[1/50] | loss train:0.075476| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012587| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011958| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010367| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010718| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009820| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009990| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009762| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009277| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010062| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009835| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009721| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009638| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009295| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009036| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008369| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009820| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009622| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008562| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009798| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009263| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008538| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009042| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008549| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008744| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009586| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008064| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008218| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008974| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009218| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008849| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009265| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008276| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008710| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008780| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008548| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008505| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007273| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007194| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007429| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007201| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006918| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007157| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006812| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006817| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006934| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007038| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EL training\n",
      "Epoch[1/50] | loss train:0.055782| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017449| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012236| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012158| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013405| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012239| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011048| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010965| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011010| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011032| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010583| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010030| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014366| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010376| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011200| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009867| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011520| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011729| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010429| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010903| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010434| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010756| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012708| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009822| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008987| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010468| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009851| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010274| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009634| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008664| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009798| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009146| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010089| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011091| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009693| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009893| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009381| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009773| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010248| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007961| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007327| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007432| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007422| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007756| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007567| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007587| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007164| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007871| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007296| lr:0.001000\n",
      "Number data points 2011 from 2015-04-16 to 2023-04-11\n",
      "ETSY training\n",
      "Epoch[1/50] | loss train:0.060225| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009143| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007846| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006154| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007193| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007519| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005436| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005321| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005132| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006501| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005312| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005508| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005693| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005955| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.004633| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005297| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007107| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005080| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005028| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004972| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005247| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004711| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005487| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005822| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006094| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004619| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005629| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005154| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005382| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005461| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005508| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004765| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004712| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004795| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004958| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004847| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004193| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004400| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004404| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004497| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004611| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003725| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004202| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003909| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003939| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003904| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003820| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004276| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004182| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004381| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RE training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] | loss train:0.076288| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015403| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013145| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012149| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012411| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010173| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011521| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010231| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012839| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010489| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011479| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010383| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011846| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010127| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009173| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010566| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009889| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008990| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009586| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009858| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010287| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010014| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009453| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009319| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010479| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009360| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009237| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009753| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009725| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009546| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009585| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009252| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008816| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009422| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009936| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009258| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008919| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008743| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008439| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007547| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007395| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007563| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007612| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007422| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007303| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007273| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007247| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007521| lr:0.001000\n",
      "Number data points 1222 from 2018-06-04 to 2023-04-11\n",
      "EVRG training\n",
      "Epoch[1/50] | loss train:0.054739| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015791| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011371| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009501| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011070| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011559| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009781| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010992| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009881| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009231| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010092| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009433| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009968| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009295| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009250| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009397| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009972| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008682| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009044| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009258| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008341| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008215| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007917| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009315| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008929| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008664| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008177| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009469| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010341| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008360| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008692| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008324| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008609| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008115| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008182| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008274| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008313| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008578| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007494| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006968| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007174| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007048| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006937| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006466| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007007| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006338| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006802| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007240| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ES training\n",
      "Epoch[1/50] | loss train:0.049522| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014619| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011971| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010174| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010859| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011370| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012174| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008792| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009734| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010571| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009189| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010784| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010321| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008896| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008923| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009464| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008662| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010874| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008488| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008974| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009136| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008473| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008906| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009256| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009170| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008687| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009026| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008206| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009054| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008059| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008307| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008262| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008495| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008487| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009223| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007981| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008479| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008264| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008409| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007452| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007329| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006763| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006763| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006810| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006737| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007155| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006987| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007105| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007035| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EXC training\n",
      "Epoch[1/50] | loss train:0.061507| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017164| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014748| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014343| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014121| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013258| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011957| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012382| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012926| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013183| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012014| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011427| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013396| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011948| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012524| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012218| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011672| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010343| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/50] | loss train:0.011081| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011040| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010710| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010574| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010668| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010299| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010822| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012057| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010711| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009513| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011501| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010339| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010495| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010202| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010613| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011229| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010251| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010629| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009806| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011263| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011103| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008847| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008714| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008629| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008324| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008155| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008232| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008205| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008532| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008452| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008238| lr:0.001000\n",
      "Number data points 4461 from 2005-07-21 to 2023-04-11\n",
      "EXPE training\n",
      "Epoch[1/50] | loss train:0.082728| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014480| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014181| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011233| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010321| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010343| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009989| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009183| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009446| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008713| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009660| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010504| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009022| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009187| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009772| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008732| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011346| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008830| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008086| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008486| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009528| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009000| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008680| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009167| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008520| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009024| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008165| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008564| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008888| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008350| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008666| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009797| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008915| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009219| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008046| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008489| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008251| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009076| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007356| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007180| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006714| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007644| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006871| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007242| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007271| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006936| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006813| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007357| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "EXPD training\n",
      "Epoch[1/50] | loss train:0.075584| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016871| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012690| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016048| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012775| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012238| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010620| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012248| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012447| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011723| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010897| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010190| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010590| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010497| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010704| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009618| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010135| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012701| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009499| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009827| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009728| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010275| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009925| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009298| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010171| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011064| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010837| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009036| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010628| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009944| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008800| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011278| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010011| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009632| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009421| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011238| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009902| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008991| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007733| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008136| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007726| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007396| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007291| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007584| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007255| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007412| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007703| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007032| lr:0.001000\n",
      "Number data points 4696 from 2004-08-16 to 2023-04-11\n",
      "EXR training\n",
      "Epoch[1/50] | loss train:0.077720| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012568| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009275| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009881| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008814| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010206| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007317| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007334| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010820| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009377| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008162| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008574| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009111| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007554| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008556| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007599| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007708| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008894| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007953| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008068| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007978| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008750| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007134| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007466| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007423| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008020| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007865| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006952| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006979| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007193| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006870| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007362| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006857| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008237| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007274| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007352| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/50] | loss train:0.006661| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007426| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007314| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006557| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005898| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005808| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005702| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005593| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005742| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005741| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005543| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005598| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005191| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005350| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "XOM training\n",
      "Epoch[1/50] | loss train:0.083463| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015571| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015907| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015737| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014235| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014192| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012016| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014227| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011779| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012598| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013215| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012608| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011701| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011843| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011136| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012015| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011638| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012629| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012371| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011985| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012170| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011941| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011314| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011970| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012524| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012880| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011700| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010354| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012486| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010272| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011763| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011263| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011975| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010992| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012553| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012169| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011025| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011102| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010915| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011765| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009412| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009447| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009104| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009141| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008377| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008266| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008621| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008733| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008246| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008669| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FFIV training\n",
      "Epoch[1/50] | loss train:0.065977| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013510| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011979| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011065| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012200| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011532| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012067| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011239| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011854| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010218| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010558| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010669| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010963| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009941| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010180| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009686| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010447| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011046| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011301| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010325| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009649| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010151| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010288| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010193| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009794| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009638| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009909| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009843| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010074| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010217| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009683| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009957| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009607| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010120| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009990| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009721| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009400| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009740| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009029| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009855| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007765| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008232| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008372| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007916| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008088| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008275| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008178| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008089| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007886| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008151| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FDS training\n",
      "Epoch[1/50] | loss train:0.073284| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013704| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013682| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010282| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010577| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010724| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010355| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009159| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010670| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009039| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010633| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009356| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009005| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010632| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009548| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009602| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008854| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008849| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009941| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008585| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008944| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009795| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009432| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008207| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009000| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009643| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008180| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008974| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009346| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008458| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009692| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008721| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009008| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009696| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009200| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008302| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007238| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007317| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007033| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007015| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007008| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007126| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007011| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006456| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006965| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006726| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FICO training\n",
      "Epoch[1/50] | loss train:0.064430| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013992| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/50] | loss train:0.017494| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016948| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012403| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014944| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011556| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013404| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010731| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011112| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011322| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011103| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011633| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010798| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011216| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009807| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010972| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010867| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010770| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009738| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010314| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010305| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009659| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009886| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008867| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009837| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010052| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011459| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009988| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009326| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009661| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010841| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009845| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009372| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009901| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010571| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011343| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009946| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008390| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007732| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007380| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007555| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007585| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007096| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007311| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007677| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007376| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007621| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FAST training\n",
      "Epoch[1/50] | loss train:0.071744| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016418| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016220| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013170| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010481| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012069| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010321| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011041| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012460| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010624| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010555| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010837| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011639| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010525| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009089| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010041| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009954| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012137| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009511| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009985| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010856| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009294| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009485| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009252| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009774| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010267| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009183| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010649| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010158| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008787| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009425| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010097| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010726| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010113| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008934| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007417| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007878| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007749| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007532| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007332| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007634| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007658| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007347| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007306| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007471| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FRT training\n",
      "Epoch[1/50] | loss train:0.058933| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013377| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011436| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010346| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009990| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010290| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010019| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010530| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009329| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010153| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009219| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009399| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008832| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009272| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009651| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008589| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009633| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009408| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009284| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008998| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008584| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008583| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009212| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009655| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008478| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008597| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008232| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008971| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008505| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008823| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008657| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008570| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008125| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008781| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008694| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008283| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007546| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007169| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007286| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007235| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006912| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007141| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007289| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007325| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007302| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007229| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FDX training\n",
      "Epoch[1/50] | loss train:0.076687| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015860| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014877| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012010| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011357| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012914| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012795| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012263| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010603| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011515| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010906| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011294| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013165| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010965| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011434| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011170| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011245| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012288| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010232| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010504| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/50] | loss train:0.011338| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010616| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011853| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010505| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010955| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010375| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011149| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010484| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009457| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010755| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012098| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010962| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009859| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009976| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010710| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009792| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010045| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010227| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008768| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008759| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008247| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008508| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008309| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008118| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008152| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007832| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008282| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007962| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FITB training\n",
      "Epoch[1/50] | loss train:0.061030| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015956| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014742| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014949| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014351| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014123| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012609| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014010| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011818| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012791| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012916| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012267| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012846| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013853| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013153| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012924| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012705| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013094| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011943| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011855| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011852| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012420| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012067| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011855| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011523| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012114| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011617| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012471| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011037| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011691| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012209| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011182| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010900| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011486| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011313| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011111| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011575| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011030| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010970| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011112| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009634| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009597| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009753| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009732| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009340| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009261| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009247| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009156| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009026| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009440| lr:0.001000\n",
      "Number data points 3104 from 2010-12-09 to 2023-04-11\n",
      "FRC training\n",
      "Epoch[1/50] | loss train:0.054056| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012594| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009870| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010965| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009663| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014336| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006829| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009385| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.040349| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012390| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006722| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007473| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.049412| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014682| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007562| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006408| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007599| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007315| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.023673| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007458| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006011| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006737| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013653| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010613| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007985| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008358| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010886| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006451| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012953| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009035| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006623| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008252| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011734| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007501| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.020541| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005937| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006174| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.015440| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008381| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009071| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006202| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004990| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005437| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006306| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010393| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005142| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008967| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.035536| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006506| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004819| lr:0.001000\n",
      "Number data points 4125 from 2006-11-17 to 2023-04-11\n",
      "FSLR training\n",
      "Epoch[1/50] | loss train:0.072019| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016876| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016424| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015350| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014374| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015592| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014190| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014259| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014872| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.017428| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013308| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013955| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013035| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013932| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014681| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013118| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012701| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012066| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012817| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014720| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013035| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013381| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012579| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013006| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013166| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014809| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013430| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014262| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013695| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012183| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013376| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013303| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012845| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011014| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013931| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011882| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012123| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.014304| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/50] | loss train:0.014311| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012944| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010439| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010269| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010744| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009858| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009920| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010639| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010031| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009842| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010310| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010251| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FE training\n",
      "Epoch[1/50] | loss train:0.049636| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017672| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014991| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013419| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013653| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013228| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012159| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014317| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013802| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012605| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012908| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013467| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011596| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013538| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013337| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012755| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012561| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011995| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011989| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012074| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011641| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012115| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011754| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012466| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011572| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011576| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011398| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011955| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011206| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013730| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012084| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011310| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012093| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012094| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011735| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011157| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011662| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011114| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012050| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011109| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009794| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009340| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009192| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009143| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009564| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009790| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009409| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009665| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009160| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009483| lr:0.001000\n",
      "Number data points 5486 from 2001-06-20 to 2023-04-11\n",
      "FIS training\n",
      "Epoch[1/50] | loss train:0.066644| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014694| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012601| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012970| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010821| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009027| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011064| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009245| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009590| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010187| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009929| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009601| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009012| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009717| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008873| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009457| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008695| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009228| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009344| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008745| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011397| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008948| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009302| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009109| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009006| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008628| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009129| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008547| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008654| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008310| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009108| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008392| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008121| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009791| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007985| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008941| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007659| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006690| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006808| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006826| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006825| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007033| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007424| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006670| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006737| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007246| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FISV training\n",
      "Epoch[1/50] | loss train:0.060409| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014439| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013448| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011842| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010295| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011705| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009836| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010559| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009536| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009413| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010036| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009109| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008796| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009659| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009360| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009110| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009159| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010077| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009099| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010095| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009133| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009515| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008826| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010125| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009601| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010171| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008796| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009248| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009238| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008732| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009510| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008079| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009171| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008528| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008666| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008308| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008752| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008559| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007627| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007276| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007455| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007251| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007081| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007104| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006887| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007571| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007435| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007144| lr:0.001000\n",
      "Number data points 3100 from 2010-12-15 to 2023-04-11\n",
      "FLT training\n",
      "Epoch[1/50] | loss train:0.062190| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010355| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008304| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007745| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/50] | loss train:0.007708| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008399| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006897| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009389| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005889| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006963| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006286| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007289| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005863| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005824| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006571| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005615| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006172| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006090| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005620| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006295| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006621| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006853| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005791| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005601| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005470| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006488| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005677| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005980| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005827| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005569| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006022| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005477| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006363| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005746| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005021| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006189| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006577| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005860| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005896| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005951| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004968| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004787| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004702| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004599| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004859| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004650| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005075| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004707| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004751| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004842| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FMC training\n",
      "Epoch[1/50] | loss train:0.074138| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016783| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013572| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012635| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011357| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011354| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010600| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010537| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011673| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009769| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011238| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009591| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008842| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009486| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009363| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009722| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010573| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008835| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008996| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009940| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009154| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009084| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010107| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009828| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008729| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010411| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008579| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008821| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008858| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009102| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009396| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008745| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008463| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008842| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008851| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007692| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007219| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007166| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007600| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007337| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007152| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007065| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007260| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007093| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007287| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "F training\n",
      "Epoch[1/50] | loss train:0.070877| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.021042| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.019249| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.023962| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.017928| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.018795| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.018909| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.016650| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.016695| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.020348| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.015731| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.017497| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015622| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.016223| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.017704| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.016903| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014302| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.018377| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014135| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.015934| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.017486| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.015494| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.015448| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014981| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.015944| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.015498| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.016198| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014135| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014223| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.014913| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.014979| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.016449| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.016157| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.015065| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.014568| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.014487| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.014692| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.015644| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.015034| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.014982| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.013089| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011270| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011393| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011161| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011890| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011862| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011543| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011216| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011342| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011649| lr:0.001000\n",
      "Number data points 3370 from 2009-11-18 to 2023-04-11\n",
      "FTNT training\n",
      "Epoch[1/50] | loss train:0.065450| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013562| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011695| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010368| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010055| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007829| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007796| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008933| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007772| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007204| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007787| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009123| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008510| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007885| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007384| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007214| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007800| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006151| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007247| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007780| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006738| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006384| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/50] | loss train:0.007336| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007106| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008223| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008005| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006265| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007914| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008703| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006661| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006564| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008153| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009355| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005972| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007176| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006802| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006351| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008133| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008144| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007461| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005463| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006082| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005653| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005082| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004971| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005391| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005666| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005212| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005294| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005060| lr:0.001000\n",
      "Number data points 1705 from 2016-07-01 to 2023-04-11\n",
      "FTV training\n",
      "Epoch[1/50] | loss train:0.052022| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012092| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012327| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011238| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011225| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011990| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008445| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009555| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010647| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009795| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010569| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010884| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009344| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011120| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009707| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010853| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008692| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008077| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010160| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008884| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009004| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013614| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010552| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010179| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010415| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009255| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008385| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008908| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008679| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010013| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008652| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007946| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009560| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010338| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009376| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008393| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009109| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009453| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008648| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007871| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008080| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007551| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007394| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007319| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007325| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007094| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007119| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009946| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007361| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008008| lr:0.001000\n",
      "Number data points 1029 from 2019-03-12 to 2023-04-11\n",
      "FOXA training\n",
      "Epoch[1/50] | loss train:0.042720| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013442| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011261| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009593| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009037| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008424| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008457| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008121| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008289| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007993| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009236| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007627| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008610| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008355| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007914| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008055| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007927| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008135| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007645| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007463| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008734| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008450| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007593| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007396| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007758| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007721| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008694| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008418| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008914| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007784| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008637| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007753| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008726| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008595| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007441| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007898| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007278| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007877| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008233| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008101| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007130| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006983| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006706| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007110| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007053| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007142| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006520| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007051| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006904| lr:0.001000\n",
      "Number data points 1028 from 2019-03-13 to 2023-04-11\n",
      "FOX training\n",
      "Epoch[1/50] | loss train:0.053531| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014315| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011757| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009533| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009806| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009171| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010248| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010921| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009768| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010194| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009366| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009255| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009313| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008935| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009464| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009384| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009860| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008363| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008925| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008638| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009882| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009500| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008554| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008991| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008598| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009675| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009296| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008518| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008264| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008384| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008537| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008528| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008323| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008561| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008874| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009181| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010959| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009487| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/50] | loss train:0.007877| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008250| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007728| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008168| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007353| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007630| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007914| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007718| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007884| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007884| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "BEN training\n",
      "Epoch[1/50] | loss train:0.095263| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017402| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015752| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012950| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014985| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012086| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011487| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012390| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011101| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011786| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010992| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011975| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011085| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011483| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011250| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012477| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011456| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011621| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011006| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011306| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011155| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011180| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010738| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011267| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010909| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010941| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010831| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011889| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011210| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010572| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010353| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010800| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011242| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011453| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010954| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011910| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011107| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010571| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011248| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009887| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009991| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009375| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009333| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009443| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009320| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009161| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009234| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009123| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009048| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009181| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "FCX training\n",
      "Epoch[1/50] | loss train:0.088711| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019358| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017302| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014075| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014493| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013676| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015163| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014559| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014409| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013411| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014462| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013562| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013316| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013590| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013590| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013172| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012825| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013231| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012712| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012221| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013565| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012788| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013195| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012633| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013620| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012555| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013293| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013140| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013420| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013580| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012726| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012639| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012510| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012491| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012853| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012359| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013515| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011750| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012219| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011587| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010864| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010446| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010789| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010978| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010583| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010835| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010815| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010311| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010366| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010836| lr:0.001000\n",
      "Number data points 5618 from 2000-12-08 to 2023-04-11\n",
      "GRMN training\n",
      "Epoch[1/50] | loss train:0.060512| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015671| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017203| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013870| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013477| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010055| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011361| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011576| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011017| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010971| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012114| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010732| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012443| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010909| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009813| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010109| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010836| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012255| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009967| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010846| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009740| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011104| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010808| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009610| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011002| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009931| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010181| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010583| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010774| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010239| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011195| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010426| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009796| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010197| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008253| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007932| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007654| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007852| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007095| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007286| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007657| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007716| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007910| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007754| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IT training\n",
      "Epoch[1/50] | loss train:0.081843| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018083| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015506| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015454| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013195| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013700| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/50] | loss train:0.012222| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011426| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013452| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012477| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012393| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013130| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011352| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010450| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012779| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011042| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012407| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010474| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011626| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012101| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010135| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011702| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011912| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011100| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011181| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011889| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010297| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010410| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011242| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010685| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010656| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009813| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010822| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009954| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010258| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010744| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010201| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010139| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009925| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008912| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008153| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008156| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008290| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007661| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008069| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008067| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007843| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007570| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007814| lr:0.001000\n",
      "Number data points 67 from 2023-01-04 to 2023-04-11\n",
      "GEHC training\n",
      "Epoch[1/50] | loss train:0.012354| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.004369| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.005526| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.003254| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.002180| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.003544| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.002288| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.001621| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.001720| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.001873| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.001984| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.001514| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.001435| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.001284| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.001472| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.001595| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.001158| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.001086| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.001220| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.001381| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.001331| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.001187| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.001331| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.000948| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.001162| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.001312| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.001140| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.001296| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.000895| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.001371| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.001060| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.001240| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.001163| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.000961| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.001071| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.000962| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.000937| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.000922| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.001099| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.000976| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.000600| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.000928| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.001196| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.000843| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.000957| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.000950| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.000902| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.000822| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.000796| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.001006| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GEN training\n",
      "Epoch[1/50] | loss train:0.070712| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019030| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014795| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014835| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013611| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012071| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011655| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014918| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012994| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013074| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011555| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012929| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012128| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012238| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011485| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012500| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012714| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012652| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011603| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011880| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011631| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010864| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011795| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011759| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011652| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011256| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010931| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011716| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011695| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010971| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010336| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011522| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011348| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011146| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011255| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010616| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011507| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011135| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009449| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008900| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008911| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008952| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008853| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008771| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009369| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008565| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009013| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008902| lr:0.001000\n",
      "Number data points 3313 from 2010-02-11 to 2023-04-11\n",
      "GNRC training\n",
      "Epoch[1/50] | loss train:0.053826| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011962| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009831| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011633| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010341| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008576| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010614| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008835| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007871| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009341| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008894| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008839| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009006| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007991| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007107| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007975| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007668| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010751| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007775| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008014| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007657| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007094| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008863| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[25/50] | loss train:0.010657| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008883| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007002| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007616| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007559| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007013| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008310| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008780| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008954| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007908| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008045| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006886| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006971| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008502| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006713| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007303| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006077| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005642| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006200| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005381| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005559| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005899| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005839| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006204| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005955| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005490| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GD training\n",
      "Epoch[1/50] | loss train:0.056164| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012170| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010143| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011044| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009961| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009631| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010180| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009399| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009944| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009356| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008481| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009557| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009895| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009009| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008990| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008641| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009744| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009026| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008932| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008542| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009213| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009470| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008152| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008781| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009056| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008210| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008544| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007946| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008569| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008900| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008131| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007983| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008190| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008407| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007811| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008104| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008767| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007109| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006914| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006760| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006936| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006668| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006672| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006677| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006869| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006523| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007016| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GE training\n",
      "Epoch[1/50] | loss train:0.086210| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017357| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015810| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015379| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014270| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013843| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012726| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013890| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014218| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013924| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012513| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012789| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013584| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013422| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012890| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012656| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013731| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013224| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013030| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012464| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012941| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012650| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012857| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012286| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012846| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012411| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011911| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012421| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012774| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012641| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012469| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012969| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011880| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011822| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012054| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011427| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012583| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011605| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012679| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011795| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010763| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010756| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009984| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010048| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010230| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009746| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009569| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009931| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010145| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010284| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GIS training\n",
      "Epoch[1/50] | loss train:0.064747| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015896| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012826| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011623| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012588| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011499| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010836| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010072| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010126| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011485| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013179| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010036| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010069| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009700| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011263| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010754| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010319| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008678| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010230| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009746| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009407| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010016| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009926| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009642| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009494| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009420| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009705| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008949| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009187| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010196| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008865| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008554| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009840| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009304| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008468| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010026| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007929| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007082| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/50] | loss train:0.007658| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007211| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007283| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007631| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007609| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007106| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007114| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007092| lr:0.001000\n",
      "Number data points 3118 from 2010-11-18 to 2023-04-11\n",
      "GM training\n",
      "Epoch[1/50] | loss train:0.077497| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013741| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013044| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011774| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011927| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010854| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012642| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012667| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011376| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011103| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010110| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010016| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011256| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010370| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009615| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009167| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010162| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009248| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009676| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010894| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010796| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012331| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010537| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009798| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009749| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011145| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009366| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010382| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008191| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009056| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009699| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010720| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009725| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009623| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008723| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009430| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010170| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010849| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008666| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008748| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007742| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007755| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007508| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008496| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007781| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007156| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007635| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007481| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GPC training\n",
      "Epoch[1/50] | loss train:0.069537| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015230| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013634| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011750| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014282| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012580| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013565| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011749| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009619| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011412| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010751| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009942| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013073| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010417| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010039| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011288| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010053| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009817| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010798| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010450| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009248| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010074| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010377| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009210| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010122| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010359| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009798| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009111| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009351| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011723| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008922| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010467| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010509| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009407| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010279| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008464| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007443| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007801| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008386| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008097| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007521| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007625| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007193| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007110| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007459| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007885| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GILD training\n",
      "Epoch[1/50] | loss train:0.056042| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012511| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010806| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010263| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010729| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009789| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010284| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009032| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010174| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008718| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009131| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009008| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008129| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008743| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008596| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008770| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009381| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009078| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008634| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008772| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008503| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008523| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008767| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007720| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008512| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008353| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008650| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010161| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008165| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008812| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008734| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008037| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008363| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008382| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008090| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008488| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008878| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007803| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007539| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006997| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006961| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007067| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006849| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006999| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007152| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007255| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007111| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GL training\n",
      "Epoch[1/50] | loss train:0.049508| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012154| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011655| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010380| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010635| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011034| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009774| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009908| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/50] | loss train:0.009840| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008740| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010078| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008361| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010091| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009232| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009201| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009490| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009470| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009403| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008587| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010117| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008650| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009298| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008386| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008976| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009352| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008718| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008879| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008580| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008606| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009012| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008962| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008519| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008722| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008815| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008996| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009151| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008322| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007486| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007311| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007155| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007021| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007051| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007324| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007134| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007210| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006977| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007192| lr:0.001000\n",
      "Number data points 5594 from 2001-01-16 to 2023-04-11\n",
      "GPN training\n",
      "Epoch[1/50] | loss train:0.072309| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013923| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011825| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013745| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011139| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010076| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010024| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010025| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009950| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009013| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009289| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011946| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008798| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009062| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009571| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009888| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009674| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009615| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009114| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009119| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008476| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009225| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009398| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007703| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008232| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008526| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008658| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007894| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008069| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008768| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008663| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009277| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008891| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008667| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009071| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008666| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008406| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008245| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008091| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007316| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006942| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007352| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006465| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006364| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007029| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006682| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007044| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006886| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GS training\n",
      "Epoch[1/50] | loss train:0.049717| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018772| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014398| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014034| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012522| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014660| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012265| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013092| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012230| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014474| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012503| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011861| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011028| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012511| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011971| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011637| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012047| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011570| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011641| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010941| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010988| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010291| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011459| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011101| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011624| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011754| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010365| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011377| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012010| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009960| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010579| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010244| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011586| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011117| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011637| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010928| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010453| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010704| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008902| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008770| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008069| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008830| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008596| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008000| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008138| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008717| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008446| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008572| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HAL training\n",
      "Epoch[1/50] | loss train:0.063703| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018745| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017526| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016233| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014461| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015249| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014401| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014070| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014573| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014594| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014925| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013029| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013829| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013553| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013366| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014373| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013391| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012494| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014072| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013415| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013657| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012545| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.014086| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012623| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013135| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012487| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/50] | loss train:0.013410| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014063| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012998| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012657| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013820| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012457| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012642| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013835| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013420| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012841| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013269| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013066| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012515| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012823| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010781| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010538| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010953| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010263| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010734| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010389| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010274| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010409| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010245| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010538| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HIG training\n",
      "Epoch[1/50] | loss train:0.062699| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.021746| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015927| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015357| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014174| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013091| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013639| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013256| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.015582| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013891| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013378| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012773| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013381| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013733| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012207| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014122| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011729| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013304| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012715| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013116| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012626| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013206| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013529| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012369| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013342| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011911| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011867| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012939| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013037| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012850| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012699| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012682| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013328| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012512| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012482| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011545| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012047| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012364| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011853| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011493| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010314| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010499| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010489| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010140| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010184| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010463| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010136| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009946| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009920| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009648| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HAS training\n",
      "Epoch[1/50] | loss train:0.070101| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015214| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012715| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011816| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011896| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009774| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010781| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010230| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010925| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009670| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009538| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010738| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010452| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009827| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009593| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009857| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009430| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010071| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009185| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009509| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009461| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010166| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008632| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009533| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008843| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009289| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009085| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009631| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008983| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009663| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009054| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009087| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009155| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008600| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008861| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008986| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007738| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007531| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007544| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007292| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007390| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007295| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007593| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007613| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007705| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007620| lr:0.001000\n",
      "Number data points 3042 from 2011-03-10 to 2023-04-11\n",
      "HCA training\n",
      "Epoch[1/50] | loss train:0.101934| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011675| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009469| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011655| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008518| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007897| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007183| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014954| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011934| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008066| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006245| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008998| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013306| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006671| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008162| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009022| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008861| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005835| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006232| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007701| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006173| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007321| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007294| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005897| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006311| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009825| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009512| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006280| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005633| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006261| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005597| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007145| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007678| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006114| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005876| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006355| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005528| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005504| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006774| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005983| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005113| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005065| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005203| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005164| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/50] | loss train:0.005736| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005800| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004674| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004932| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009172| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007891| lr:0.001000\n",
      "Number data points 5897 from 1999-11-01 to 2023-04-11\n",
      "PEAK training\n",
      "Epoch[1/50] | loss train:0.068069| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016049| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013926| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013323| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012636| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011967| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011081| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012160| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010886| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010976| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010512| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010905| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010801| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011460| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010992| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010359| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010911| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011240| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010995| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010721| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010546| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010330| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010301| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010534| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011281| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009999| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010981| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010377| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010745| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010592| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009789| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010325| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010092| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011300| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010342| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010732| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009618| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010707| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008763| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008781| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008510| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008903| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008829| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008507| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008313| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008519| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008567| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008586| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HSIC training\n",
      "Epoch[1/50] | loss train:0.068942| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012944| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011488| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010891| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010881| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010740| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010269| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009832| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010404| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009682| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009677| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009828| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009280| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009529| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009742| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008781| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008811| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009307| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008828| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008629| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009373| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009596| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008615| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009357| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008861| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008606| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009381| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008264| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009171| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008843| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008634| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008440| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008926| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007586| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007627| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007173| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007619| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006959| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007402| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007192| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007137| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006958| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007373| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HSY training\n",
      "Epoch[1/50] | loss train:0.072929| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012757| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013271| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013465| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010001| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010314| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010979| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011073| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009804| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009696| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009487| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010504| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010046| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008995| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010364| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008478| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009453| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008706| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009716| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008908| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010393| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008859| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009307| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008949| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009719| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008788| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008765| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010044| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008196| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008818| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009150| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009084| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009781| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007972| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008762| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009165| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008121| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007303| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006839| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006534| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007113| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006622| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006444| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006317| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006629| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006894| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006662| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HES training\n",
      "Epoch[1/50] | loss train:0.067733| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020879| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.019783| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.019870| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.019516| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016892| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015175| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.016201| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.019035| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015798| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/50] | loss train:0.016192| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013545| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015925| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015919| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.015255| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.016015| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.016425| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.017590| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014273| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.015986| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014729| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.015024| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.014790| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.015385| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.015035| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014322| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014200| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.015203| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014042| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013680| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.014729| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013993| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.015433| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013588| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.015112| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013959| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013412| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.015744| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013636| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013785| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.012455| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011408| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011219| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011181| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.012122| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011088| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011809| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010999| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010741| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011119| lr:0.001000\n",
      "Number data points 1882 from 2015-10-19 to 2023-04-11\n",
      "HPE training\n",
      "Epoch[1/50] | loss train:0.054149| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012511| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010542| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009817| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009020| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009627| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009590| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009129| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007429| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008204| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008065| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009004| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008179| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008126| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007804| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009724| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007989| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007533| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008078| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008747| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007576| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007430| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009396| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008161| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007997| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008256| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007865| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007788| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007942| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007294| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007264| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008301| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008689| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007849| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007349| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007663| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007453| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007769| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007015| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006577| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007019| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006602| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006568| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006898| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006425| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006649| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006571| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006835| lr:0.001000\n",
      "Number data points 2347 from 2013-12-12 to 2023-04-11\n",
      "HLT training\n",
      "Epoch[1/50] | loss train:0.045980| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010420| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008760| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008350| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008450| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006973| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007082| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005450| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005689| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007731| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006574| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005979| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007726| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007540| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005660| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006291| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007516| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006999| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005649| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007755| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005130| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005337| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006523| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005612| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005829| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006291| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005381| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006462| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005437| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006195| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005247| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005145| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006217| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005798| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005134| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004963| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005722| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006534| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005064| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005113| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004812| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004374| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004145| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004134| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004164| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004635| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004575| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004680| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004564| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004821| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HOLX training\n",
      "Epoch[1/50] | loss train:0.073729| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017829| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014139| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011655| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013394| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011746| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010868| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011338| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011946| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010232| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010776| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010261| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011438| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010361| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009871| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009376| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009917| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009832| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009955| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010009| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008945| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009692| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010338| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010888| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010293| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009318| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009413| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[29/50] | loss train:0.009508| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008635| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010107| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009886| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009210| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009483| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009580| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009403| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010025| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009990| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009519| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009095| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007627| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007806| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007518| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007357| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006883| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007708| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007506| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007608| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007221| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HD training\n",
      "Epoch[1/50] | loss train:0.087070| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014190| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012911| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014168| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014157| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011561| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010769| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010131| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011477| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010594| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011887| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010699| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010371| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011423| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009449| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010269| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010687| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009978| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009989| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009910| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009775| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009716| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009348| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008803| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009271| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009714| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008720| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009384| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009594| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008591| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008056| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008888| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008637| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008430| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008935| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007171| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007693| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007472| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006627| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007018| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007024| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006683| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006847| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007053| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007397| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HON training\n",
      "Epoch[1/50] | loss train:0.059526| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013087| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011252| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011014| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009324| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010074| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009655| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008816| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009646| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009262| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009106| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009869| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009556| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009319| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009084| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008322| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009020| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008241| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008699| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009693| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008460| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008038| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009313| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008592| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008896| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008508| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008963| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008825| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008720| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008267| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007787| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008341| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008523| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008925| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007943| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008357| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008706| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008177| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007205| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006573| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006761| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007143| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006535| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006502| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006837| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006810| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006576| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006665| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HRL training\n",
      "Epoch[1/50] | loss train:0.074835| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012339| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010832| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010918| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008581| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009559| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009444| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009057| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008930| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008997| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007986| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008548| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008427| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009313| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008948| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008047| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008649| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008616| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008991| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007996| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008445| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008413| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007732| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008210| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007878| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008188| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007732| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008079| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007942| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008400| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008320| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008314| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008270| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008257| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007434| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007845| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008002| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008034| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008031| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007859| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006766| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006787| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006832| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006705| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006592| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006459| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/50] | loss train:0.006072| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006578| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006331| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006648| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HST training\n",
      "Epoch[1/50] | loss train:0.071370| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.021443| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.019174| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014838| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015511| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014957| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014872| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015503| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014037| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013730| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013682| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014483| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014098| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013492| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013934| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013319| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013161| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013408| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013550| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013731| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012516| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012961| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013276| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012561| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012659| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013095| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013451| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012667| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012560| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012658| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013389| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012614| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013017| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012602| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012818| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012842| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012298| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012990| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012521| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012228| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011140| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010914| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011495| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010944| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011072| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010746| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011141| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011115| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010923| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010667| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HWM training\n",
      "Epoch[1/50] | loss train:0.060232| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017953| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.019483| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016498| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015697| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015196| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014388| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014416| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014050| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013978| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013735| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013384| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013546| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013927| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014018| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013775| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012632| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013394| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013926| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012392| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014544| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012918| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012600| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013701| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013161| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012461| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012303| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013137| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012208| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012652| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013165| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012432| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013737| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012946| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013148| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012626| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012474| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013132| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012474| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012254| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011607| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011224| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010601| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010778| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010487| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010849| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010902| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011103| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010483| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010807| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HPQ training\n",
      "Epoch[1/50] | loss train:0.072484| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020232| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016581| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015914| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015148| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.018410| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013705| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.017627| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014427| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014506| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014190| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013533| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015495| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012544| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013294| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013182| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013707| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013415| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014308| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014893| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012566| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013571| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013740| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014478| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013372| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012614| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013702| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012935| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013336| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.014018| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013301| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012241| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.014772| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.014190| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013081| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012472| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012966| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013700| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013558| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013329| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011123| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010320| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010769| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010278| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011170| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009775| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009988| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009945| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009546| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010741| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HUM training\n",
      "Epoch[1/50] | loss train:0.070471| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015302| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013673| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013682| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010693| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010965| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010772| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011518| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011810| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010614| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010775| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009919| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/50] | loss train:0.010540| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010050| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011581| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010053| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009261| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010725| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009354| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009671| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010462| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010163| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010130| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008420| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008833| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009065| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009145| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009877| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009950| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008932| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009367| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009286| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010844| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008855| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010173| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008658| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009286| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009795| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008770| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007995| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007777| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006877| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007416| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007184| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007197| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007116| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007151| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007396| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007158| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "HBAN training\n",
      "Epoch[1/50] | loss train:0.081226| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019267| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016428| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015288| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014322| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015021| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014539| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014578| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014645| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014419| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014615| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013733| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013731| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013281| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013752| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012615| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014165| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014389| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012666| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013770| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013286| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013937| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013561| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013898| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013006| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012858| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013031| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014297| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012845| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012935| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013687| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012812| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012530| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013353| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012372| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013281| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012048| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013516| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013086| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012823| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011426| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011172| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011118| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010630| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010898| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010900| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010998| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011006| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010743| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010655| lr:0.001000\n",
      "Number data points 3034 from 2011-03-22 to 2023-04-11\n",
      "HII training\n",
      "Epoch[1/50] | loss train:0.050699| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010559| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007197| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006462| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006919| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006848| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006079| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005675| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005376| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006176| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007143| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005329| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005417| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005729| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005562| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005832| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005430| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005625| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005468| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005226| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005146| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005288| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005662| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005663| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005284| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004903| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004909| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005362| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005043| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005219| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005354| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005068| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005377| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005092| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005065| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005030| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005152| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004803| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005081| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004802| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004446| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004492| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004569| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004286| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004278| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004031| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004144| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004197| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004139| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004385| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IBM training\n",
      "Epoch[1/50] | loss train:0.051120| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016665| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013113| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011793| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012816| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012503| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011083| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011401| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011325| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011116| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011190| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010852| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011516| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010494| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010801| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011213| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010341| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010927| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010765| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009952| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011027| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010451| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010507| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010966| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010592| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010877| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010265| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010499| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010453| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[31/50] | loss train:0.010149| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010075| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010436| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009601| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010635| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010118| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010458| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009822| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010213| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010430| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009385| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008873| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009032| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008472| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008673| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008807| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008572| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008981| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008870| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008811| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IEX training\n",
      "Epoch[1/50] | loss train:0.059100| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014173| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013202| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011404| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009796| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010639| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010945| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009671| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009210| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011951| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009412| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011401| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008455| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008566| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009080| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009539| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009987| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009587| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008823| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008958| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009626| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009749| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008548| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008645| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009514| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008860| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008325| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008806| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008322| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008564| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008012| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009970| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008401| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008251| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008312| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008490| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008690| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008727| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008731| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007085| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006998| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007020| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006874| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007035| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006503| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006606| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006934| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007253| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007151| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IDXX training\n",
      "Epoch[1/50] | loss train:0.061327| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016012| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011066| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013756| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015713| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013340| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013377| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011142| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011029| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010897| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010968| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011483| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.016456| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011665| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011253| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010615| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010908| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010805| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012009| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010729| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011104| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010638| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011220| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010476| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011224| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010250| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011396| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010363| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010455| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009199| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009300| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010343| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010508| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009812| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012024| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010811| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009536| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008859| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008165| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008406| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008169| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007471| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007537| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007848| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007884| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007608| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007371| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ITW training\n",
      "Epoch[1/50] | loss train:0.054254| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013569| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010532| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010847| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009726| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009380| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010698| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010219| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008824| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009843| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010177| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008853| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009194| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008449| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008398| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009678| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008510| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009550| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008003| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008280| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008824| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007981| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008697| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008717| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007923| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008269| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008430| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008007| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007958| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008715| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008706| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007439| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008317| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007967| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008513| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008309| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009074| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007911| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008750| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007433| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007262| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006908| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006950| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006945| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006838| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006575| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006959| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[49/50] | loss train:0.006648| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006261| lr:0.001000\n",
      "Number data points 5711 from 2000-07-28 to 2023-04-11\n",
      "ILMN training\n",
      "Epoch[1/50] | loss train:0.061824| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016298| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013452| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011648| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012263| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010531| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009792| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010369| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011128| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011225| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011768| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009611| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011490| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009440| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010092| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009868| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010976| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009826| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010027| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009700| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009084| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009391| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008840| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010079| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009638| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009917| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009834| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008798| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010300| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009368| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009277| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009560| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009562| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008998| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010002| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008412| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008637| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009529| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007602| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007313| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007686| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007310| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007172| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007534| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007356| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007260| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007396| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007661| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "INCY training\n",
      "Epoch[1/50] | loss train:0.067117| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017429| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014823| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014179| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014789| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012544| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013206| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013374| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012287| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015865| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012953| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011978| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012433| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013245| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011755| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012356| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012167| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011618| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011306| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011422| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012279| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011903| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011595| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012018| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012852| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011573| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011776| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012350| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011411| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011661| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011207| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012940| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010619| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011499| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010826| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011348| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011798| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011243| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011381| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011654| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009985| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009816| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009582| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009394| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009375| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009430| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008971| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009326| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009349| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009573| lr:0.001000\n",
      "Number data points 1488 from 2017-05-12 to 2023-04-11\n",
      "IR training\n",
      "Epoch[1/50] | loss train:0.041210| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008369| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006562| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005769| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005609| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004821| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.004542| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004696| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.004439| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004366| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004040| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004503| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004163| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004580| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.004784| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004364| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004250| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004118| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004247| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004337| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004199| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004143| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004068| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004261| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004231| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004460| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004563| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004186| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004291| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003901| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004810| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004285| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004010| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003524| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005908| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004943| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004761| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004736| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004244| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004228| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003623| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003548| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003727| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003701| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003539| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003602| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003476| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003709| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003334| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003624| lr:0.001000\n",
      "Number data points 4005 from 2007-05-15 to 2023-04-11\n",
      "PODD training\n",
      "Epoch[1/50] | loss train:0.047272| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011495| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009710| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010298| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010638| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009085| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007425| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008306| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.017547| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007763| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008151| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009042| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008207| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/50] | loss train:0.009090| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011855| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007923| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008136| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007020| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009903| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008639| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007215| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007548| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007120| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007800| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007117| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011855| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007824| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006504| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009189| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009322| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007133| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006600| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008084| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007400| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008239| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006990| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008154| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007102| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007897| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008197| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006557| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006697| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005996| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005979| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006011| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007519| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006338| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005687| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005889| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "INTC training\n",
      "Epoch[1/50] | loss train:0.096839| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020182| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016655| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013833| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015861| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012955| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013186| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012598| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013209| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013551| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011925| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013207| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012425| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013385| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013132| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012352| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012732| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011704| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012507| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012316| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012395| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011324| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012029| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012311| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012687| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011905| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011121| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011332| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011688| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011994| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011836| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011346| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012506| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011615| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012917| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010642| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012166| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011406| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011002| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011133| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009630| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009894| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009935| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009929| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008772| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010232| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009762| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009399| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009760| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009715| lr:0.001000\n",
      "Number data points 4378 from 2005-11-16 to 2023-04-11\n",
      "ICE training\n",
      "Epoch[1/50] | loss train:0.051551| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011863| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010163| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010089| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009121| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008049| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008142| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009665| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007808| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008003| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007694| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007724| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006729| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007949| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007650| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006741| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007572| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007172| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007024| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006555| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007276| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007277| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006822| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006853| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006780| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006776| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007335| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008205| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006564| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006776| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006776| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006589| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006690| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007204| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006269| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006722| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006373| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006764| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006544| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006497| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005926| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005632| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005593| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005435| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005584| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005675| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005162| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005342| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005489| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005228| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IFF training\n",
      "Epoch[1/50] | loss train:0.080334| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013303| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012164| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010855| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010524| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010880| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009888| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010334| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009510| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009281| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009777| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009102| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009644| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009689| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009227| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009234| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009412| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009317| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008987| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009046| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009404| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009444| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009256| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009266| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009142| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009303| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008601| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009195| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009257| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[33/50] | loss train:0.008810| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008622| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008900| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009279| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008797| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009039| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008587| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008472| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007614| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007601| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007490| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007439| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007483| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007603| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007887| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007385| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007612| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007377| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IP training\n",
      "Epoch[1/50] | loss train:0.068860| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016782| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014856| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013163| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012790| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012515| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012664| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011904| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011928| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012647| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011643| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011783| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011764| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011813| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011734| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011286| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011469| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012129| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011381| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011709| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011787| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011931| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010690| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011502| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011224| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010939| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010633| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011076| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010597| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011929| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010500| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011238| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010825| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010731| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011268| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011456| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010674| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011544| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010772| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010776| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009244| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009123| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008919| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009112| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009132| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008952| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008940| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008763| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008731| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008998| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IPG training\n",
      "Epoch[1/50] | loss train:0.061017| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020787| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015037| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013301| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013709| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012502| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012108| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012538| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011744| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012608| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011671| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011215| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012614| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010444| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012819| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011841| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011213| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011973| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011016| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011089| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011396| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011801| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011007| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010772| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010538| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010891| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010766| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010835| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011323| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011666| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010797| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010728| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010674| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010961| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010638| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010277| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010209| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010661| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011127| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010017| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009598| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008978| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008846| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008813| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008706| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008825| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008571| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008575| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008671| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008978| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "INTU training\n",
      "Epoch[1/50] | loss train:0.092441| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017595| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014991| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013586| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012269| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012422| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014843| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011133| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011573| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010688| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010811| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011707| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010993| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010855| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012586| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011166| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011785| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010097| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011829| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013034| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009664| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010673| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010049| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012808| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011275| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011636| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010989| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009769| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009848| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009820| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010093| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009704| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010830| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009503| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010442| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010985| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008818| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008010| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007852| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008090| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007978| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008113| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007416| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007405| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008670| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007626| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5740 from 2000-06-16 to 2023-04-11\n",
      "ISRG training\n",
      "Epoch[1/50] | loss train:0.086012| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014953| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012532| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013343| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013485| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009817| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013587| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010572| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011806| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009587| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010196| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011399| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011116| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010392| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011375| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010484| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010712| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011629| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009057| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010399| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009327| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009765| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009847| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010046| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009979| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009599| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009932| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009195| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009638| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009194| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009582| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010359| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009489| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009215| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012164| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009280| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009576| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010243| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009055| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008566| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007351| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007681| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008040| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007613| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008458| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008831| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007690| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007107| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007422| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IVZ training\n",
      "Epoch[1/50] | loss train:0.054390| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019610| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015127| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.017071| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014764| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014605| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014656| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014775| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014820| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014980| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013463| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014246| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014183| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013305| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013997| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014570| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013857| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013657| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013922| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013922| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013969| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013643| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013646| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013725| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.014003| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014178| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014516| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012836| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013042| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013055| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013629| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012992| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.014408| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013085| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012376| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013106| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013403| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013169| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012769| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013456| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011923| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011569| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011203| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011487| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011675| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010985| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011439| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011620| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011510| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011140| lr:0.001000\n",
      "Number data points 1558 from 2017-02-01 to 2023-04-11\n",
      "INVH training\n",
      "Epoch[1/50] | loss train:0.050054| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009517| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007070| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006422| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005691| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005239| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005105| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005444| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.004556| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004426| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004194| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004596| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004335| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004899| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.004265| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004588| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004414| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004574| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004266| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004207| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.003996| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004132| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003674| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.003938| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.003632| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003883| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004020| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006069| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004615| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003963| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004121| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003801| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003680| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003830| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.003882| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004371| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004468| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004588| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004092| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003631| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003599| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003416| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003383| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003382| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003417| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003232| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003153| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003389| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003302| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003351| lr:0.001000\n",
      "Number data points 2498 from 2013-05-09 to 2023-04-11\n",
      "IQV training\n",
      "Epoch[1/50] | loss train:0.058035| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009887| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007210| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006067| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005694| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005336| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005524| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006149| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005384| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006100| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005046| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004958| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005653| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005035| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005001| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004901| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/50] | loss train:0.004444| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004595| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004529| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004831| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005375| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004958| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004504| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004290| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004510| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004814| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004526| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004287| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004285| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004890| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004321| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003846| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003859| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004614| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004692| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005531| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003970| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004602| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004341| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004895| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003981| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003688| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003501| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003602| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003685| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003646| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003506| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003518| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003365| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003596| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "IRM training\n",
      "Epoch[1/50] | loss train:0.077671| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017357| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017772| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012019| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012204| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011847| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014191| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013036| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011007| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012354| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011110| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011833| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012298| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011981| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011453| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011663| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012123| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009547| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011286| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010632| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009675| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010805| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012343| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009773| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009837| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011588| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011118| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010096| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010008| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010233| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010804| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009515| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010328| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009780| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009829| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009597| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010259| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009533| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011143| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008461| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008721| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008223| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007992| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007992| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008151| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008033| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007765| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007803| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007735| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "JBHT training\n",
      "Epoch[1/50] | loss train:0.058335| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012693| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012062| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010662| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012733| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010041| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010973| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010693| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010100| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011404| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009980| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010562| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010397| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009485| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009404| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009797| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009307| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008753| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008592| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009154| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008799| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008650| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009644| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009559| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010512| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008406| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008686| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008865| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008011| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009051| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008831| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009034| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008970| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008548| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007759| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008065| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007241| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007096| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006934| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006650| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007119| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007078| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006556| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006745| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007320| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "JKHY training\n",
      "Epoch[1/50] | loss train:0.053398| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013674| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013185| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012255| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011003| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011861| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010294| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011058| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009745| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012055| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010307| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009780| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009912| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011134| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009372| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009553| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009415| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010006| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009364| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010028| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009022| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010026| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009394| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008644| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008862| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009103| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009225| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010116| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008987| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008737| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009459| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009386| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/50] | loss train:0.008855| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009046| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008159| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009077| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009232| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009175| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008960| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007619| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007266| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007663| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007023| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007530| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007212| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007416| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007069| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007123| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "J training\n",
      "Epoch[1/50] | loss train:0.073522| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015495| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014160| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013855| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014023| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012441| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011956| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011486| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011884| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011189| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010967| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011706| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011455| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010881| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010617| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011208| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010399| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011070| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011750| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010381| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009821| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011820| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009548| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010124| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011467| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010640| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010937| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009823| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010009| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010111| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010776| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010011| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010309| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010579| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009761| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009971| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009582| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010592| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008923| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008474| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007723| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007958| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008020| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007621| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008094| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008120| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008023| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007933| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "JNJ training\n",
      "Epoch[1/50] | loss train:0.050117| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012816| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010942| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010452| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009939| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010623| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009471| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009258| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011252| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008176| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009651| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008473| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008259| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008840| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009004| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010191| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009262| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008443| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008715| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008531| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008443| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008335| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009090| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008567| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010196| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009008| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008194| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007598| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008193| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008394| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008414| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008129| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008371| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007774| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008252| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008579| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008076| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008468| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008029| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007314| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006935| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007147| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006521| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006406| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006712| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006481| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006777| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006518| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006787| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "JCI training\n",
      "Epoch[1/50] | loss train:0.054325| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012745| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013778| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011910| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011203| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012416| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010407| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009518| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009924| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009594| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010595| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011043| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009136| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010417| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009915| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010227| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010968| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009539| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009482| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008734| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010539| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008743| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009109| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009028| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009471| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009113| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009328| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010070| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009725| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009063| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008408| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008794| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008849| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009307| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009758| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009283| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009773| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007847| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007156| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007394| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007209| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007296| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007213| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007165| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007155| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007048| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006814| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "JPM training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] | loss train:0.060240| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015024| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011408| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011880| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012255| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010098| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010461| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011222| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009776| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011594| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009570| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009561| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009930| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010564| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010857| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009955| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010289| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009566| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010003| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009824| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009693| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009384| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010411| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008978| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009774| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009499| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009253| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008852| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008956| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009374| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009724| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009942| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008430| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009715| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010089| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008856| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007673| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007560| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007486| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007368| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007634| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006906| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007391| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007400| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007394| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007479| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "JNPR training\n",
      "Epoch[1/50] | loss train:0.168672| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.038655| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.032669| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.029800| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.033322| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.033549| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.036481| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.030217| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.029923| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.028990| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.037281| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.030631| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.026731| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.029607| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.024739| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.023957| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.026489| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.030192| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.031856| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.027205| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.030080| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.030400| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.025496| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.029374| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.025930| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.031169| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.029935| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.028979| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.031187| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.024885| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.025776| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.022512| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.031460| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.023562| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.025121| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.025686| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.024529| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.024713| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.025039| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.023709| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.020539| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.019042| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.021854| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.017773| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.019460| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.018054| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.020087| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.018169| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.018218| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.016123| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "K training\n",
      "Epoch[1/50] | loss train:0.059263| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015575| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012127| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011767| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011277| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010330| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012283| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009754| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011765| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010530| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009940| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010652| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010144| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010531| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010340| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010009| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009469| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009428| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010216| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009570| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009837| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009183| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009923| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010062| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008884| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009341| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009920| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009316| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009263| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010008| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009884| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008700| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009008| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009407| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009114| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008605| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007885| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007993| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007454| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007598| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007425| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007919| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007485| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007549| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007666| lr:0.001000\n",
      "Number data points 3765 from 2008-04-28 to 2023-04-11\n",
      "KDP training\n",
      "Epoch[1/50] | loss train:0.051726| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008782| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007957| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007963| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006524| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007646| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006280| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006876| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006333| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006597| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005299| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005851| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006464| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006079| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005945| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006564| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006174| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/50] | loss train:0.005400| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005868| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005629| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006083| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005741| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005814| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006004| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006011| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005618| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005834| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005531| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005598| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005659| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006134| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005875| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004883| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005495| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005385| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005193| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006252| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005510| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006196| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004788| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004918| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004105| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004710| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004749| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004650| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004454| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004340| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004642| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004710| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "KEY training\n",
      "Epoch[1/50] | loss train:0.088179| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017814| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015654| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013909| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.016631| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014302| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013753| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013498| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013605| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.015156| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012756| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013671| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012945| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014740| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012463| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013156| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012304| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012111| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013971| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012172| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012496| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012175| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012862| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011793| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013468| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012311| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012261| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012036| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012453| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012947| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011969| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012925| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011398| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012402| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012897| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012161| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011571| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012439| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011699| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011581| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010651| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010295| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010402| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010637| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010120| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010026| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010393| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010166| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010266| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010314| lr:0.001000\n",
      "Number data points 2133 from 2014-10-20 to 2023-04-11\n",
      "KEYS training\n",
      "Epoch[1/50] | loss train:0.056590| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.007729| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.005965| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005443| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004873| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004542| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.004519| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004200| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005756| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.003698| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004545| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004408| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.003486| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.003322| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.003570| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.003622| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.003694| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.003777| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.003781| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.003494| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.003352| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.003285| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004638| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004084| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.003564| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003417| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.003559| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003654| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.003929| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003567| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003263| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003533| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003773| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003797| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.003579| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003308| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003349| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003322| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003294| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003679| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003084| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003117| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.002936| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003091| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.002915| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002769| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002956| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.002735| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003035| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003144| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "KMB training\n",
      "Epoch[1/50] | loss train:0.086050| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012927| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011216| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011055| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009329| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009496| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009632| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009932| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009775| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008769| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009376| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008639| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008694| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009026| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009041| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009003| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008787| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008805| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008874| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008716| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008224| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009309| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009053| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008081| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007869| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008628| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008127| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008127| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008142| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008585| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008060| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008934| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008088| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008258| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008552| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/50] | loss train:0.007875| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008484| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008735| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008089| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007780| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007149| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006992| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006853| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006614| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006989| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006790| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006936| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006797| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006935| lr:0.001000\n",
      "Number data points 5895 from 1999-11-01 to 2023-04-11\n",
      "KIM training\n",
      "Epoch[1/50] | loss train:0.063885| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017874| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016195| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014327| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013339| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015075| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013210| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013408| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011966| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012594| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013099| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012225| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011545| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013254| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012020| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013388| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012665| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011844| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013228| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012099| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012256| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011643| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012938| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011725| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012660| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012720| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012618| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011738| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011801| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013019| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011587| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011883| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012898| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011388| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012368| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011801| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011013| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011673| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011049| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011208| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010453| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009918| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009838| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009878| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009938| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009753| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010006| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009885| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009878| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009807| lr:0.001000\n",
      "Number data points 3060 from 2011-02-11 to 2023-04-11\n",
      "KMI training\n",
      "Epoch[1/50] | loss train:0.066167| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013526| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011977| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010317| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009900| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011369| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010768| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008456| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008996| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010174| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009180| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008003| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009063| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008583| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009132| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010390| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009582| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010078| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008580| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007826| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009838| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008237| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007948| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009132| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008006| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009178| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008291| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008666| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008711| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007969| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008967| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008977| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008108| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008438| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008747| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007875| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008314| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008908| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007939| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007811| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006748| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007478| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006909| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007348| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007232| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006748| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006747| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007086| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006882| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007338| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "KLAC training\n",
      "Epoch[1/50] | loss train:0.083053| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018328| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017917| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015156| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013351| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013313| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013340| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012465| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012821| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012489| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013178| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013719| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014485| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012868| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012368| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011983| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012116| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010906| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012204| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014284| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012824| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012098| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011267| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011924| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011778| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012415| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012201| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010947| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011270| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010194| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011940| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009896| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010685| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011459| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011269| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010492| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010831| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011018| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012364| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010773| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008675| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007995| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008572| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008761| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008831| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009024| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008078| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008467| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008148| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008244| lr:0.001000\n",
      "Number data points 1956 from 2015-07-06 to 2023-04-11\n",
      "KHC training\n",
      "Epoch[1/50] | loss train:0.062417| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009354| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/50] | loss train:0.007725| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006197| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005124| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005224| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006716| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008567| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007227| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004934| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007047| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004812| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004807| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004729| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005476| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005217| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006281| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004381| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004277| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005648| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009398| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006496| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004329| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006642| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008929| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005792| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004919| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004327| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.003991| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005300| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006732| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005833| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005033| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005288| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004527| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004180| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004271| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004823| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006746| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006192| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005739| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005325| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003965| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004970| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003712| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004167| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003637| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003493| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003769| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003419| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "KR training\n",
      "Epoch[1/50] | loss train:0.072343| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018095| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013054| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011865| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011639| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011577| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011602| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010367| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013538| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009643| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010997| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011700| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010619| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010164| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010588| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009886| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011115| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010007| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008720| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010528| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009751| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010363| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009667| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010734| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009514| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009665| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008839| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009786| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009541| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009171| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009193| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010100| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009509| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009606| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009401| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008054| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007873| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007416| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008147| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007335| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007721| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007803| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007566| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007502| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007478| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LHX training\n",
      "Epoch[1/50] | loss train:0.072145| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014558| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012334| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012439| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011410| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009874| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010338| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012611| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010441| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010869| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010090| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009604| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010050| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009798| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009243| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009307| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011349| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009103| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009516| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011690| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010467| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009289| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010730| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008659| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008923| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009322| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009492| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009616| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009328| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009172| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009853| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008901| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009112| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010282| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008329| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009660| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008221| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007313| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007505| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007129| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007103| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007226| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007289| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007356| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007174| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007288| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LH training\n",
      "Epoch[1/50] | loss train:0.061396| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017379| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014862| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012710| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013426| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010247| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010679| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010378| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010971| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013276| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010296| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011581| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009793| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010048| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010191| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010399| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010558| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010798| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010198| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009557| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/50] | loss train:0.009344| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009638| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010927| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009498| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009053| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009259| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008692| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009197| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008568| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009021| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009077| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009133| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008983| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009860| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009633| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009702| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008332| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008986| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009108| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007174| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007011| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007155| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007155| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007027| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007221| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007360| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007043| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007146| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LRCX training\n",
      "Epoch[1/50] | loss train:0.087779| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016932| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015303| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013858| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014172| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013379| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012788| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010760| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013139| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011646| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011519| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011569| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011480| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011403| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011647| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011706| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011491| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011516| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012694| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011664| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014221| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011841| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012497| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011045| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010865| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012126| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011258| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010112| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011599| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012087| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009264| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010475| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010955| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010432| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010761| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011601| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010702| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011324| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011285| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009337| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008253| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007619| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008489| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008357| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008671| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008579| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007966| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008457| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008632| lr:0.001000\n",
      "Number data points 1613 from 2016-11-10 to 2023-04-11\n",
      "LW training\n",
      "Epoch[1/50] | loss train:0.051995| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011412| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008677| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008061| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007573| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006051| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006165| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006824| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006291| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005912| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005962| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005998| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006219| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006208| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005886| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005436| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005566| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005348| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005717| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006053| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005923| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006064| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004979| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005496| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006027| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005631| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005405| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005492| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005373| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005324| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005282| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005173| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005094| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004819| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005272| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005080| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005777| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005239| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005689| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005176| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004598| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005026| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004689| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004604| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004579| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004600| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004386| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004302| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004617| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004477| lr:0.001000\n",
      "Number data points 4611 from 2004-12-15 to 2023-04-11\n",
      "LVS training\n",
      "Epoch[1/50] | loss train:0.103825| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.022309| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017673| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015722| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.016016| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015461| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.016827| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014482| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.015508| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014249| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014365| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014396| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014986| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012904| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013540| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012969| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012679| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013418| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014053| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013486| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012336| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012478| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013393| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012719| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.014079| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013074| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013264| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013670| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013990| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.014765| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013473| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012417| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013283| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.015557| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.014433| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012088| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013509| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012598| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/50] | loss train:0.014059| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011927| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010755| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011209| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010053| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010292| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009991| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010385| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011081| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010552| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010749| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010391| lr:0.001000\n",
      "Number data points 4150 from 2006-10-13 to 2023-04-11\n",
      "LDOS training\n",
      "Epoch[1/50] | loss train:0.070709| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011026| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011096| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008633| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008375| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008693| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007809| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007999| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007071| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008535| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006616| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006720| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007555| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007225| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008228| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006794| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006775| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006875| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006931| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007297| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006856| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006649| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006776| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006723| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006520| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006782| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007419| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006123| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006500| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007039| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007123| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006561| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006791| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006504| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007261| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006757| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006410| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007339| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006710| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006165| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005562| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005316| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005388| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005692| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005441| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005577| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005371| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005423| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005274| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LEN training\n",
      "Epoch[1/50] | loss train:0.058872| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019107| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015316| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014713| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012434| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014385| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012707| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014157| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012260| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012232| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012212| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012028| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011806| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012927| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013403| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011531| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011741| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011287| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012192| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012234| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012062| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010774| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012617| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011261| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012639| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011607| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011998| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011213| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011938| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011673| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010807| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012171| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010968| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011348| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010983| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010565| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011287| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010890| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010428| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010657| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009904| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009194| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009124| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008908| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009240| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009424| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008731| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008946| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009087| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008601| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LNC training\n",
      "Epoch[1/50] | loss train:0.065680| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.022541| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015811| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.018411| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.016979| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016627| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014235| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015034| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014902| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.016840| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014275| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.015268| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015388| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014738| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013979| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014971| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014579| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014916| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.015060| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014200| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014461| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014331| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.014394| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014454| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.014028| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014232| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014288| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014728| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013240| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013681| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013401| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.014172| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.014688| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013778| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.013131| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.014631| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013952| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013172| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.014217| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013678| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.012356| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011982| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011739| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011655| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011845| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011682| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011758| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.012084| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011773| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011927| lr:0.001000\n",
      "Number data points 1139 from 2018-10-01 to 2023-04-11\n",
      "LIN training\n",
      "Epoch[1/50] | loss train:0.034007| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008138| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.005324| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.004836| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/50] | loss train:0.004329| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004317| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.004235| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004059| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.003959| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004321| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004155| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.003191| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004006| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.003462| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.003824| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.003466| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.003181| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.003287| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.002974| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.003086| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.003577| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.003308| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003098| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.003616| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.003230| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003259| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.003296| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003574| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.003202| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003108| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003545| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003466| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003944| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003947| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004566| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003209| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003999| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003506| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003062| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003080| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003140| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.002577| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.002683| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.002830| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.002788| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002764| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002968| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.002704| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.002518| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.002911| lr:0.001000\n",
      "Number data points 4354 from 2005-12-21 to 2023-04-11\n",
      "LYV training\n",
      "Epoch[1/50] | loss train:0.082446| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012388| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011704| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010878| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010567| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009385| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009579| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011679| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010048| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008907| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010529| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009803| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009114| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010195| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009716| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009020| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008283| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010600| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008134| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008567| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008512| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008674| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009893| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007999| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008479| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009763| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009254| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008676| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008943| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007954| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008869| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008624| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008456| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008122| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008961| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008155| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007723| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007780| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006748| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006627| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006250| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006704| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006248| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006946| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006763| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006462| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006376| lr:0.001000\n",
      "Number data points 4913 from 2003-10-03 to 2023-04-11\n",
      "LKQ training\n",
      "Epoch[1/50] | loss train:0.067481| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011613| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012048| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011483| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011776| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009877| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010169| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009115| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010313| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009861| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009183| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007886| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008842| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009642| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009420| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009130| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007976| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008568| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007845| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008536| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008193| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009055| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008690| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008785| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009156| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008953| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008398| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007731| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008682| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007797| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008145| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008188| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008702| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008426| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008160| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008277| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008706| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008982| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007337| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006849| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006607| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006783| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006633| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006525| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006353| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006260| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006278| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006279| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LMT training\n",
      "Epoch[1/50] | loss train:0.067386| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013782| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011758| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011001| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009935| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010196| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009964| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010559| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010436| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009796| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009860| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009487| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008641| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008829| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008685| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009256| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008335| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/50] | loss train:0.009078| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008497| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008175| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008702| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008601| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009612| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008013| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008184| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008926| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008809| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007687| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008039| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008289| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008027| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007678| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008693| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008236| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009189| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007263| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006851| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006971| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006776| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006919| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006884| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006958| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006783| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006747| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006716| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "L training\n",
      "Epoch[1/50] | loss train:0.065426| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013958| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013588| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011698| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011952| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012829| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011582| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012943| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012882| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010922| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011067| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011206| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011255| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011937| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010185| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011950| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010836| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010771| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011134| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011418| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010786| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011067| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010916| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011318| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010187| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010975| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010778| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010113| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010774| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011232| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011705| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010573| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009860| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010764| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010604| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009637| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010025| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009433| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008852| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008618| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008637| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008871| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008393| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008434| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008578| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008591| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008668| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LOW training\n",
      "Epoch[1/50] | loss train:0.054410| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013457| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013069| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011946| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013613| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011235| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011630| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012145| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010329| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011231| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010315| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010158| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011876| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010038| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009582| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012256| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009977| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009953| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010923| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010274| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008763| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010072| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009437| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009831| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010722| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009824| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009461| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012005| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009153| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009637| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009858| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010376| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009545| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008721| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009908| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007556| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008112| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007115| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007749| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007701| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007061| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007078| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006849| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007589| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007040| lr:0.001000\n",
      "Number data points 3261 from 2010-04-28 to 2023-04-11\n",
      "LYB training\n",
      "Epoch[1/50] | loss train:0.044540| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009559| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008996| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008371| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007080| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008640| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007703| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007430| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007581| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008393| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007252| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006969| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007436| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006812| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006898| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007528| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007153| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006934| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006693| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007093| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006963| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006715| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007040| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007478| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006031| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006555| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006632| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007511| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006639| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006841| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006224| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007061| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006807| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006722| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006957| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006484| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007105| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007014| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006659| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006265| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/50] | loss train:0.005649| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005396| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005402| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005361| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005485| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005427| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005469| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005454| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005071| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005487| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MTB training\n",
      "Epoch[1/50] | loss train:0.058523| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017296| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014589| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015491| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014194| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013072| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012842| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012869| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011701| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012507| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010642| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011505| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011219| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011282| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012351| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011787| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010500| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011619| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010796| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011506| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010620| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012021| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010629| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012708| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011283| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011988| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010921| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011039| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010586| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011654| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010889| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011174| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011210| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012660| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011145| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010559| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011015| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010776| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011188| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009442| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009393| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009043| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009526| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009135| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009160| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008838| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009129| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009017| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009227| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MRO training\n",
      "Epoch[1/50] | loss train:0.069514| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016290| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014571| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015236| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012707| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013384| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013347| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012836| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014045| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013980| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012454| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012789| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012594| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011927| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011740| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012421| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011732| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011401| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013403| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011751| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012239| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012019| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011739| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011684| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011507| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011024| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011528| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012363| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012013| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011605| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011545| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011838| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011284| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011584| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011324| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011023| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011960| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011117| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011490| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011578| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010088| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009960| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009438| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009693| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009335| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009728| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009511| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009699| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009653| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009649| lr:0.001000\n",
      "Number data points 2969 from 2011-06-23 to 2023-04-11\n",
      "MPC training\n",
      "Epoch[1/50] | loss train:0.069938| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012568| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009931| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007179| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009276| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008054| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008471| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006777| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009469| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007997| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008379| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007234| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006444| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007805| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006676| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007643| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006248| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008225| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007841| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006009| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007129| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006659| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007463| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005872| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006765| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005487| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006826| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007068| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006954| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006776| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006590| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007106| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006918| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006191| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006057| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006851| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007392| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006264| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005869| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005319| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004901| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004780| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004870| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005001| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005321| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004737| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005275| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004569| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004675| lr:0.001000\n",
      "Number data points 4638 from 2004-11-05 to 2023-04-11\n",
      "MKTX training\n",
      "Epoch[1/50] | loss train:0.076240| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010900| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010907| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008639| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010554| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009360| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/50] | loss train:0.007827| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009030| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007626| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008222| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008586| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009087| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007510| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008936| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008918| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007451| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009963| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007499| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008015| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007919| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008133| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007749| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007720| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006831| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007247| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008131| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008036| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007781| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007827| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007420| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009257| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007295| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008685| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007220| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007530| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007754| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007490| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007360| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007761| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008061| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006168| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006361| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006275| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005817| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006044| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006150| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006119| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006039| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005689| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005882| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MAR training\n",
      "Epoch[1/50] | loss train:0.063458| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014190| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012130| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012271| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010492| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011607| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010616| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011045| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009823| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008711| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011018| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009792| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010414| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010657| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011398| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009830| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009731| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009555| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010114| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010035| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009267| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009377| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009503| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009029| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009218| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009210| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009206| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009726| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009655| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009301| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010245| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009890| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008737| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008803| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009692| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008984| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007298| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008215| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007923| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007704| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008176| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007798| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007815| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007403| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007204| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007369| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MMC training\n",
      "Epoch[1/50] | loss train:0.058018| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014844| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011894| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016689| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010907| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010610| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010503| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010755| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010496| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011039| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012157| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010427| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011383| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009451| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009175| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009830| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009029| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010340| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010062| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009540| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010118| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010279| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009825| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008651| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009399| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009915| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009911| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010339| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009519| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009212| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010290| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009270| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009724| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008985| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010210| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008423| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009114| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008910| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010533| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008973| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007727| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007437| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006890| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006737| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006525| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006961| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007126| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006859| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007093| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006996| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MLM training\n",
      "Epoch[1/50] | loss train:0.066674| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015153| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016416| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011893| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011782| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011687| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012701| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012185| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011035| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010383| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011183| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011380| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011677| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010707| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010442| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010689| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009450| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010142| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009445| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010228| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010089| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[25/50] | loss train:0.009447| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009544| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009840| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010838| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009537| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010341| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010086| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009083| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009921| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010302| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009749| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009582| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007817| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008122| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008086| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007935| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007812| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007574| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007614| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007631| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007683| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007796| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MAS training\n",
      "Epoch[1/50] | loss train:0.073004| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019199| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014537| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012473| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012457| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011211| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012759| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010995| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010660| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010662| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010450| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010878| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010236| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009794| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010286| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010925| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010360| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010119| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009773| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009699| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009702| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009583| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010119| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009764| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009583| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008863| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010419| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009531| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010072| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009393| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009037| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010332| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009888| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010061| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009236| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009887| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008906| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009991| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009952| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007973| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007727| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008085| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007534| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007328| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007716| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008041| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007641| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007324| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007550| lr:0.001000\n",
      "Number data points 4248 from 2006-05-25 to 2023-04-11\n",
      "MA training\n",
      "Epoch[1/50] | loss train:0.061851| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009768| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009006| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008664| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007642| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008445| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008077| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007314| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006243| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006553| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006532| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006627| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007292| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007147| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006890| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006411| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006301| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006053| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006562| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005999| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006631| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006410| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006546| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005919| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005888| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006394| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005540| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007304| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006246| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006199| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006656| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005600| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005793| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005769| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006256| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006148| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006241| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006879| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005872| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006419| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005378| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004992| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005222| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004991| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005103| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004844| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005261| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004979| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004852| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005398| lr:0.001000\n",
      "Number data points 1859 from 2015-11-19 to 2023-04-11\n",
      "MTCH training\n",
      "Epoch[1/50] | loss train:0.048050| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008024| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006989| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006422| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.005632| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005433| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.004613| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004753| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.004651| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004037| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004778| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004584| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004757| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004515| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.004424| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004027| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.003832| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004355| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004393| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.003880| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004709| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004535| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003959| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005052| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.003889| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004019| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004150| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003978| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004054| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004419| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004430| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004254| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003780| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004343| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.003913| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003679| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003738| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004068| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003991| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004359| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003799| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003284| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/50] | loss train:0.003489| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003358| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003352| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003295| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003547| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003529| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003423| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003314| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MKC training\n",
      "Epoch[1/50] | loss train:0.061073| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014260| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012431| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011434| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010088| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010338| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009889| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009938| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009461| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008497| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009707| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008525| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008899| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009101| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008889| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008874| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009204| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008812| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008216| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008059| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008152| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009205| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007784| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008901| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008295| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008123| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008327| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008575| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008087| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008461| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009508| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008357| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008517| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007752| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008528| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008242| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008863| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007200| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007065| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006758| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006396| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006758| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007083| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006595| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007090| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006751| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006852| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MCD training\n",
      "Epoch[1/50] | loss train:0.076014| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016348| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014356| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012522| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010032| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009909| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011969| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009952| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011211| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009895| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009661| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009313| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009175| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009335| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009809| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008363| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009287| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008714| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009336| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009956| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008702| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008901| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008819| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008475| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009988| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009706| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008740| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008775| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009030| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008872| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008509| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009655| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008529| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008513| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009273| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008646| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008357| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007322| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007621| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007276| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007211| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006965| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007061| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007033| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007433| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006984| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006681| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MCK training\n",
      "Epoch[1/50] | loss train:0.084161| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018030| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015545| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013987| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014393| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012535| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013052| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011342| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010594| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012865| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012202| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011236| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010787| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010307| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010290| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011705| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011928| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011513| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009238| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010485| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010451| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010298| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010167| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010630| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009388| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010989| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008975| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011438| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010187| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010233| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009277| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010690| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011716| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009883| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010221| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009698| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009743| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010044| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010467| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008864| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008335| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007938| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007323| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007453| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007695| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007248| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007426| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007784| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007591| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MDT training\n",
      "Epoch[1/50] | loss train:0.059899| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014852| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013700| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011495| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012336| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010415| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011179| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010541| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011047| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011130| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009976| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011062| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012060| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010526| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009541| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010583| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011216| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009725| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010386| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011302| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009637| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009805| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010502| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009423| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010347| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009318| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009131| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010091| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009747| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009620| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009425| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009784| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009180| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008775| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010123| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009053| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008533| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007631| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007671| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007649| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007602| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008048| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007525| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007637| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007626| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007993| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MRK training\n",
      "Epoch[1/50] | loss train:0.052967| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014175| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012142| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012284| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011698| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011251| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011591| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012729| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011085| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010763| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011597| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010211| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009984| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011137| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011446| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010219| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010035| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009397| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009250| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010501| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010024| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011192| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009458| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009348| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010023| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010042| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009870| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009712| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009192| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009419| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010111| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008882| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009108| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009695| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007587| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007502| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007215| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007160| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008121| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007428| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007276| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007460| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007194| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007311| lr:0.001000\n",
      "Number data points 2741 from 2012-05-18 to 2023-04-11\n",
      "META training\n",
      "Epoch[1/50] | loss train:0.050442| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012912| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011419| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008391| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007013| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007057| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006291| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006932| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007918| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008872| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006619| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007189| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006061| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005810| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006531| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005755| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006182| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006436| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006704| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006243| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006079| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006612| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005612| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006011| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005917| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005229| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006468| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007793| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007015| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006576| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006060| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005785| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005663| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005898| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006968| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005265| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005559| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006408| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006330| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005384| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005178| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004636| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004762| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004894| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004682| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004671| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004807| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004402| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004675| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004675| lr:0.001000\n",
      "Number data points 5790 from 2000-04-05 to 2023-04-11\n",
      "MET training\n",
      "Epoch[1/50] | loss train:0.071845| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018008| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014916| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.017650| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013713| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012251| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013204| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013747| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011976| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012925| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012548| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012545| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012074| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011829| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011334| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011615| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012182| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011357| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010733| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011919| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011737| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011388| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010694| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011543| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011451| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010997| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/50] | loss train:0.011259| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011099| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011004| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010915| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011963| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011745| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011240| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011118| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012180| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011276| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010729| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011904| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010181| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010460| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009199| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009458| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009079| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008682| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008697| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008544| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008324| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008540| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008414| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008496| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MTD training\n",
      "Epoch[1/50] | loss train:0.067776| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013560| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011232| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011724| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011341| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011854| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011578| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010479| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010302| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010684| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012126| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009352| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010770| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010293| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009084| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009540| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009633| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008893| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009813| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008785| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010396| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009756| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008839| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010029| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009897| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010602| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009141| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009685| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009161| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009027| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008834| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009515| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010451| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008249| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008448| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008523| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008447| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007983| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008104| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006986| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007032| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006728| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007108| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007284| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007297| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006928| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006428| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006796| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MGM training\n",
      "Epoch[1/50] | loss train:0.070575| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018905| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.019803| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.021606| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.016958| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016044| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.016591| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014751| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.016566| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013899| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014284| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013751| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015437| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012842| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012389| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014040| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014424| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013698| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014429| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014048| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.015544| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.017135| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013763| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014483| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012963| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013707| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012849| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012811| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012451| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.015728| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.014671| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.014649| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.014477| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013505| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.014945| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.014168| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013104| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012603| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013445| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013875| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010663| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011100| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010173| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009816| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010440| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010227| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009784| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009664| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009795| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009420| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MCHP training\n",
      "Epoch[1/50] | loss train:0.067360| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017361| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012559| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013146| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012009| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013007| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010973| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013002| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012117| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013322| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011781| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011574| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010345| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012724| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010364| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010954| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011749| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011285| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010463| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011069| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010520| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010062| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010029| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009578| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010583| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010235| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010507| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010401| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009946| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010114| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010207| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010343| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010242| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009437| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010776| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009743| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009829| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009899| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011720| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008708| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007864| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008281| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008396| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/50] | loss train:0.008199| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007789| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008416| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007677| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008033| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007954| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MU training\n",
      "Epoch[1/50] | loss train:0.065287| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.021341| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015070| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014281| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015118| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015630| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015828| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014773| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014793| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014202| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014261| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012969| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013949| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013358| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013154| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013485| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013893| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013171| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013819| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013584| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013315| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013691| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012901| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013639| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012179| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013407| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013223| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013298| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012354| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013784| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012831| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012811| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013483| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012594| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012918| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012636| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013496| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012624| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013025| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012408| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010673| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010941| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010415| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010518| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010881| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010479| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010577| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010861| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010273| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009805| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MSFT training\n",
      "Epoch[1/50] | loss train:0.051271| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016512| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013995| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011849| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012841| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012087| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014068| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010998| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011465| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011083| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010048| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010710| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010837| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011502| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010506| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011067| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009902| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009329| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010214| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010358| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010145| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011661| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009311| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011006| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010626| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009920| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009544| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009189| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008877| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009256| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009856| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008020| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008888| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008488| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008583| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009614| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008810| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007936| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007453| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007039| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007479| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006947| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007449| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006642| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007114| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007267| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006921| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MAA training\n",
      "Epoch[1/50] | loss train:0.078145| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014948| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011724| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011541| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010719| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010492| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011518| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011476| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010016| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011580| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009831| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011097| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010819| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010668| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010655| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009672| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008652| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008967| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009608| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009013| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009547| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010176| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010469| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009020| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011115| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008176| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009098| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008536| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008848| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008580| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009901| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009124| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010247| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008511| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009843| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008425| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009068| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007732| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007327| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006783| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006798| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007528| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007034| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007192| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007212| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006560| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006548| lr:0.001000\n",
      "Number data points 1092 from 2018-12-07 to 2023-04-11\n",
      "MRNA training\n",
      "Epoch[1/50] | loss train:0.053595| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006465| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005797| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006359| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004776| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005220| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004538| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.004972| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004658| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/50] | loss train:0.003959| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004819| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005136| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004523| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.003979| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004430| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004009| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004219| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.003698| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004866| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004481| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004008| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003953| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004973| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004808| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004090| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.003931| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003471| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.003807| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004118| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003862| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003982| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003486| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004438| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004270| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003268| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003649| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003379| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003449| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003362| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003351| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003308| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.002960| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.002836| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003202| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002761| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003240| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003003| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003360| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003143| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MHK training\n",
      "Epoch[1/50] | loss train:0.055315| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017519| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015845| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015057| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013537| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011984| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011015| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012974| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011462| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011333| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010286| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011851| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011500| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010582| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010247| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011354| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009744| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011259| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010810| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010457| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011101| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010699| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010583| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010617| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010808| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010381| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010569| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010408| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009957| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009904| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010134| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010055| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010951| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009838| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011047| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009893| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010309| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011019| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009277| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008564| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008154| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008407| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008232| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008377| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008361| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008357| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008452| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008359| lr:0.001000\n",
      "Number data points 4978 from 2003-07-02 to 2023-04-11\n",
      "MOH training\n",
      "Epoch[1/50] | loss train:0.076890| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016634| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015402| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011480| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013455| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010579| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011758| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011820| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012197| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010600| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009341| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011461| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011525| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007888| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008218| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009354| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009175| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008945| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009175| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010669| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009615| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008695| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008374| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009394| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010381| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008490| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008687| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008680| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007983| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008218| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008684| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008664| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007503| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010705| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007843| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007535| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008819| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006959| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006383| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006724| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007099| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006884| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006566| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006153| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006636| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007074| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006663| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TAP training\n",
      "Epoch[1/50] | loss train:0.078654| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016230| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015486| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012747| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014335| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011156| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011425| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010448| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011927| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012082| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011233| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012000| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010360| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011655| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010649| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009912| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010685| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011610| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011244| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010544| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009978| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011663| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011386| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010679| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009793| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010627| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011199| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011042| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[29/50] | loss train:0.011308| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010566| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009186| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010740| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010780| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010548| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009969| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010424| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010414| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010783| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010492| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010070| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008549| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008818| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008571| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008277| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008307| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008536| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008057| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007999| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008170| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008185| lr:0.001000\n",
      "Number data points 5491 from 2001-06-13 to 2023-04-11\n",
      "MDLZ training\n",
      "Epoch[1/50] | loss train:0.065540| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016511| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012598| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011650| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009590| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011266| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008787| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011019| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009261| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007886| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009336| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009127| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008898| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008931| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008486| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009477| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008285| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009343| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008484| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009091| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009473| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008243| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008427| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008287| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008892| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008335| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008645| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007443| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008486| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009244| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008139| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007779| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008684| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008114| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008152| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009029| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007670| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008132| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008168| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006842| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007105| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006924| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006873| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006762| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006292| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006555| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006611| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006749| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006782| lr:0.001000\n",
      "Number data points 4628 from 2004-11-19 to 2023-04-11\n",
      "MPWR training\n",
      "Epoch[1/50] | loss train:0.068245| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013664| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015462| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013135| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011484| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012258| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012814| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009572| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009160| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010331| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009054| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009294| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009917| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009104| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010092| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010099| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009405| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008626| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009209| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009654| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008696| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009054| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008663| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007965| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007978| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010845| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007626| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009063| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008896| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008982| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008272| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008316| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009197| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008089| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007606| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007880| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006859| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007115| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006410| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006629| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007658| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007456| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006761| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006461| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006881| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MNST training\n",
      "Epoch[1/50] | loss train:0.058873| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012693| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010395| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011088| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011485| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010186| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009608| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009428| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010609| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008593| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008755| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008227| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009625| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009099| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008934| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009415| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008848| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008866| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008677| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008598| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008051| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008535| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008508| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009075| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007973| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009052| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008017| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007895| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008423| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008704| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008135| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008156| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008139| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008465| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007661| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007765| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008653| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008091| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007263| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006694| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006454| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006531| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006791| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[46/50] | loss train:0.006825| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006877| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006830| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007043| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MCO training\n",
      "Epoch[1/50] | loss train:0.052071| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015843| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011564| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011990| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012182| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010189| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010360| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011361| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012676| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009372| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011273| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009688| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009685| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011384| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009086| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009591| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008982| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009142| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009262| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009363| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008514| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008918| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009994| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008857| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008785| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009166| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008788| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009040| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008972| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009017| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008343| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008605| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010325| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009716| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008071| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007462| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007327| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006434| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007336| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006994| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007109| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007157| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007349| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006694| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006883| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MS training\n",
      "Epoch[1/50] | loss train:0.091728| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.021009| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017063| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015306| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014631| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015352| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014448| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015885| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013759| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014605| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013664| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013643| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013360| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014065| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012869| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012673| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013407| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013942| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012684| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011699| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013795| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014110| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013307| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012686| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013924| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012984| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011744| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012503| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013157| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012261| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011727| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011793| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012138| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012651| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.014282| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013397| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011646| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012290| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012945| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012170| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009715| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010569| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010084| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009473| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009298| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009176| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009362| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010136| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009026| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009786| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MOS training\n",
      "Epoch[1/50] | loss train:0.085060| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.021659| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.019252| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.022017| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.019420| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016591| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.019573| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.017685| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.020695| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.019389| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.016237| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.017339| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.016308| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.018148| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.017762| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.016866| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.016105| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.021096| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.016327| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.017625| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.016252| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.017584| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.015682| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.017218| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.017226| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014616| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.015093| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014874| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.015177| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.016878| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.015894| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.017339| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.016998| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.016022| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.016024| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.017765| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.014823| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.014587| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.014999| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.016826| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.014886| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.012573| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011932| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.012294| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010926| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011836| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011774| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011644| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.012276| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.012176| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "MSI training\n",
      "Epoch[1/50] | loss train:0.074662| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018875| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015096| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013301| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.018556| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013439| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013980| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012553| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014912| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013356| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013428| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[12/50] | loss train:0.011378| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013980| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011367| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012271| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012405| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012236| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013056| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010967| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012176| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013112| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012818| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011363| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012740| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012171| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011867| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011336| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012111| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011257| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.013018| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010428| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011777| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010903| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011715| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011296| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011980| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013500| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011030| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011924| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009790| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010013| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009367| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009269| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009528| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009525| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009634| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009113| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008920| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009221| lr:0.001000\n",
      "Number data points 3876 from 2007-11-15 to 2023-04-11\n",
      "MSCI training\n",
      "Epoch[1/50] | loss train:0.046712| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009385| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007920| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008614| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008168| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012731| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011489| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007720| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007240| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007222| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006297| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007413| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006757| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010063| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008340| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008149| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006206| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008380| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007326| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006937| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007309| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006444| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007970| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006552| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007525| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006717| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006763| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009580| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006641| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006405| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006407| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008010| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007822| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006915| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006612| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007106| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007263| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006821| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005979| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006622| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005422| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005040| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005115| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005523| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005100| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007190| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005537| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007150| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005253| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004908| lr:0.001000\n",
      "Number data points 5231 from 2002-07-01 to 2023-04-11\n",
      "NDAQ training\n",
      "Epoch[1/50] | loss train:0.062362| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013659| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013484| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010608| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010725| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010645| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012665| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009473| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008038| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008901| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010136| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010086| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010688| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009399| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008735| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008974| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009217| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009685| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008634| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008223| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008637| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009979| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008311| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008595| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008142| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009369| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009652| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008079| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008328| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008604| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008766| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008097| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008335| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008762| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009718| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008145| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008752| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007655| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007739| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008019| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007112| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007069| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006366| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006483| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006456| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006941| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006023| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006616| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006398| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006019| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NTAP training\n",
      "Epoch[1/50] | loss train:0.091651| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.023244| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018047| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.020431| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.020090| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.017156| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.020714| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.018187| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.018405| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.019481| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.017425| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.017003| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.017766| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.017235| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.018307| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.019523| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.017100| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.017952| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.017072| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.016370| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.016402| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.015809| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.015671| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.016453| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.018172| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014635| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.017906| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.016528| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[29/50] | loss train:0.016645| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.016700| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.016546| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.016202| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.016887| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.016512| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.015733| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.016854| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.016419| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.016353| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.015832| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.016061| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.014657| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.013456| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.013773| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.013552| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.014043| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.013414| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.013146| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.013952| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.013351| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.013665| lr:0.001000\n",
      "Number data points 5257 from 2002-05-23 to 2023-04-11\n",
      "NFLX training\n",
      "Epoch[1/50] | loss train:0.050056| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015666| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013273| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009605| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012080| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012679| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010218| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009615| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010395| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011435| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009991| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010770| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009066| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009665| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010919| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008861| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009526| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009419| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009525| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009278| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010991| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009494| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009550| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009368| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009283| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010591| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008556| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010215| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008806| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009314| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008406| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008619| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009730| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008879| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010357| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008780| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008537| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009749| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008135| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007420| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007503| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007095| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007555| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007188| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007221| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007341| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007266| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007624| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NWL training\n",
      "Epoch[1/50] | loss train:0.076250| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020811| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016186| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015473| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015457| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015477| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015056| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012826| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.017814| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014843| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012870| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014939| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013438| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013459| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014649| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013518| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012743| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.015678| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012828| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014906| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011812| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012751| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011632| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014482| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013142| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012994| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011951| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012678| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012033| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012701| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012863| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013188| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013423| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012557| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012178| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011730| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012670| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011353| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011792| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012941| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010677| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010082| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010227| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010410| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009780| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009860| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010013| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009924| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009525| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009470| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NEM training\n",
      "Epoch[1/50] | loss train:0.078163| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018709| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018770| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016907| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015957| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015690| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015702| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015328| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.016170| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.016656| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014885| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.015648| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015041| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015325| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.015645| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014484| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.015392| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014682| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014806| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014235| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.015381| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014542| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013856| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.015030| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013969| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013995| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014763| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014111| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014003| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.014833| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.014777| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013759| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.015061| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013697| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.014817| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.014256| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.014056| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.014239| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013534| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013709| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011995| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011870| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011081| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011481| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011726| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011234| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/50] | loss train:0.011115| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011344| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011266| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011934| lr:0.001000\n",
      "Number data points 2470 from 2013-06-19 to 2023-04-11\n",
      "NWSA training\n",
      "Epoch[1/50] | loss train:0.059426| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014871| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012791| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009109| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009078| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011071| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009423| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009697| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013867| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012364| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010604| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011451| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011599| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009474| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010692| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007974| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009364| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009256| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008975| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008117| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008036| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009250| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008710| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009190| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009901| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008764| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009513| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008170| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010495| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009055| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008641| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006983| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008067| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008749| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008567| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008526| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008204| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007845| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007526| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006785| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006733| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006922| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006958| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006938| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006177| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006151| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006141| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006341| lr:0.001000\n",
      "Number data points 2470 from 2013-06-19 to 2023-04-11\n",
      "NWS training\n",
      "Epoch[1/50] | loss train:0.056884| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013418| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011848| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010959| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013980| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011097| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011240| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013132| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011596| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009886| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012984| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009625| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009788| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008356| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008198| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008749| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007250| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010141| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009040| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011965| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011950| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010455| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008940| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008160| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008047| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010505| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012963| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008054| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009401| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009089| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009576| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011935| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009670| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010763| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008356| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010393| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007840| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007974| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011234| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008762| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007805| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008390| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009699| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007199| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006707| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006685| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007933| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007578| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007077| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NEE training\n",
      "Epoch[1/50] | loss train:0.083509| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014454| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011415| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012773| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010322| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009444| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012071| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009790| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010500| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010573| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010129| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009603| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009465| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009257| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009984| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010619| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009731| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008870| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011326| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008746| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010173| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009694| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009242| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008530| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009926| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008667| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009553| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008623| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008189| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009123| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008689| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008888| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008600| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008437| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008604| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008086| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008204| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008091| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007642| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007160| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007105| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006788| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006849| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006700| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007001| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006471| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006644| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006546| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NKE training\n",
      "Epoch[1/50] | loss train:0.076051| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013886| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013974| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012826| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012208| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013058| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010584| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011612| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009915| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011218| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010820| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009990| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010811| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010549| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010662| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010668| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009862| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010751| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010088| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010138| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009942| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009916| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009949| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009445| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009328| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009658| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009662| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009200| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010658| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008749| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009451| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011349| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009482| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009746| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009303| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009723| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009413| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009847| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008369| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007437| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008027| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007522| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007469| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007645| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007016| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007147| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007126| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007481| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NI training\n",
      "Epoch[1/50] | loss train:0.061627| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012004| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012575| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009530| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009471| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009900| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008904| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008514| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010290| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008458| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008157| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009067| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008722| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008540| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008648| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008271| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008089| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008630| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008310| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008426| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008275| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008123| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008727| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008016| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008778| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008194| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008399| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007979| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007671| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008109| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008695| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007718| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007909| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007916| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008406| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007754| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007919| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008281| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006882| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006963| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006625| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006595| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006457| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006964| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006771| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006594| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006757| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006701| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NDSN training\n",
      "Epoch[1/50] | loss train:0.062172| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014249| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012393| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011516| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011427| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010547| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011146| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011769| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010369| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009489| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010604| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010044| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010262| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011140| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008647| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010970| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008997| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009341| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009337| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008500| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009727| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008704| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008812| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008612| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009775| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008832| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008823| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010194| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010202| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008775| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010197| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009363| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008800| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009024| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008751| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009261| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008467| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009619| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007791| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007939| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007269| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007454| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007211| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007273| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007167| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007245| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007443| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007355| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NSC training\n",
      "Epoch[1/50] | loss train:0.085024| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016348| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015205| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012337| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011367| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009982| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009856| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012937| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010989| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012285| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010161| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009686| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010798| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009975| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010259| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010576| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010086| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010419| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009656| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008981| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009419| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009065| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009269| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009530| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009803| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[31/50] | loss train:0.009144| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009484| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008729| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009309| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007930| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009366| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008766| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008652| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009047| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007413| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007110| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007081| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007597| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007163| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007005| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007089| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007640| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006927| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006774| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NTRS training\n",
      "Epoch[1/50] | loss train:0.073703| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018340| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018326| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014610| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012519| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013311| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012664| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012178| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012460| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012591| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013764| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012366| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014239| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012393| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012663| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011932| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012285| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012192| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011856| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011781| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012051| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011967| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012644| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011617| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011027| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011865| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011856| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012072| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011029| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011454| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011646| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011973| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012173| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011550| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012124| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011163| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011301| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010764| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011523| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011024| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010009| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009994| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009612| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009561| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009273| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009223| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009386| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009118| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009070| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009443| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NOC training\n",
      "Epoch[1/50] | loss train:0.063766| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014145| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011582| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011768| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010258| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010829| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009921| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010613| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010504| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010690| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009677| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010252| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010235| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010351| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009120| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010357| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009104| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008722| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010001| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009524| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008813| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008918| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008995| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008898| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008453| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010325| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009092| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009297| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010886| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009194| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009187| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008773| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008927| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008645| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009268| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009289| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008211| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009010| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008638| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008085| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007428| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007163| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007647| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007448| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007119| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006893| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006776| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006882| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006912| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006975| lr:0.001000\n",
      "Number data points 2574 from 2013-01-18 to 2023-04-11\n",
      "NCLH training\n",
      "Epoch[1/50] | loss train:0.058519| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011265| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009972| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009005| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007020| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006955| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007105| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006748| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006965| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006287| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006435| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006638| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006343| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006237| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006933| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006075| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007142| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005985| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005877| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006045| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006581| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006014| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006007| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005804| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006039| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006195| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006025| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006082| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006333| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005897| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006234| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005590| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006031| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005788| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006335| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005676| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005471| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005804| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005923| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005811| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005292| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005243| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005162| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005006| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005017| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004840| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005117| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004719| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[49/50] | loss train:0.004881| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005117| lr:0.001000\n",
      "Number data points 4872 from 2003-12-02 to 2023-04-11\n",
      "NRG training\n",
      "Epoch[1/50] | loss train:0.065581| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014608| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014646| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014931| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013149| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012222| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012491| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013712| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014101| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012513| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011725| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011636| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012254| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012300| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012645| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011414| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011418| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011290| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012919| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010778| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011842| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010869| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012149| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011630| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010342| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011978| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011493| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011151| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011529| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010882| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010953| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011169| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011083| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010762| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011057| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009825| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011407| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011022| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010511| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011328| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009581| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009293| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009629| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009374| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009461| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009039| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009070| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009234| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009102| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009038| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NUE training\n",
      "Epoch[1/50] | loss train:0.068554| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018880| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016207| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015395| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.016098| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016109| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.016637| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014142| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014847| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013764| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014943| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013074| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014247| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015243| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012238| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013380| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012946| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012020| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013997| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014508| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014363| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014816| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012628| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011091| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.014070| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013346| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012743| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014699| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014096| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012414| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010495| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012005| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012350| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011976| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011401| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013478| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011722| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011768| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011649| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.014093| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009469| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009014| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009488| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008262| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009067| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009007| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009244| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009234| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008724| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008974| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NVDA training\n",
      "Epoch[1/50] | loss train:0.085731| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019360| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016440| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014679| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015020| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.017654| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015980| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015134| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.016642| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013867| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012943| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014721| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015621| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.015254| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012844| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.015985| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013637| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.015316| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014625| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012672| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013233| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014905| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012433| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013305| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011286| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012934| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.016399| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012475| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.014063| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012546| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012646| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011112| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012624| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011764| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011707| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012342| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013658| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011775| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012281| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013126| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009749| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009736| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009736| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009467| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008957| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009553| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009783| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009098| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008532| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008999| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "NVR training\n",
      "Epoch[1/50] | loss train:0.072300| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014525| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013704| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011133| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011009| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012227| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010186| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010784| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010836| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009970| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011036| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009843| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/50] | loss train:0.009544| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010911| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009641| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009585| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009851| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009930| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009746| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009780| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010007| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010862| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009297| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010131| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008246| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010629| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009280| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008613| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008006| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008752| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009744| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008554| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008694| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009181| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007766| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007770| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007370| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007477| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007247| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007096| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006667| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007036| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007121| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007152| lr:0.001000\n",
      "Number data points 3191 from 2010-08-06 to 2023-04-11\n",
      "NXPI training\n",
      "Epoch[1/50] | loss train:0.055094| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008857| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008508| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008421| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009026| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007083| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006696| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007778| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006882| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006881| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007124| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007199| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006875| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006767| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007773| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006793| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006398| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006333| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006460| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007415| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006833| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005700| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007255| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007170| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006162| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006016| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006379| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006307| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006052| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006878| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006303| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006716| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006781| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005970| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006497| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006422| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006118| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006069| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006090| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005600| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005382| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004836| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005282| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005083| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005109| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005387| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005606| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004958| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004949| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ORLY training\n",
      "Epoch[1/50] | loss train:0.106446| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018853| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014636| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013896| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010980| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012011| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010275| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012710| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009867| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011803| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009735| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011388| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009826| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010385| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010053| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010717| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010417| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010419| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011579| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010839| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009562| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011978| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009015| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010957| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010696| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010836| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009451| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009450| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010895| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010552| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009204| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009863| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009303| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010108| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008097| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007949| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008080| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007429| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007694| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007451| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007186| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007346| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007119| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007055| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "OXY training\n",
      "Epoch[1/50] | loss train:0.060799| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015836| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013440| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012419| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012153| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012499| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011714| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011301| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010951| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011945| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011365| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010980| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011478| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011183| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010700| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011430| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011400| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010844| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010757| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011011| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010846| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011118| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010549| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009955| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010873| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010984| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010697| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010182| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010401| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010734| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010228| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[33/50] | loss train:0.010264| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010370| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010256| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010683| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010272| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009989| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010159| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010371| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009362| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009324| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008982| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008559| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008747| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008817| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008921| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008893| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008590| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008956| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ODFL training\n",
      "Epoch[1/50] | loss train:0.083568| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017737| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013327| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016542| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015953| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016606| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011224| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011869| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013506| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011573| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011491| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011977| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010003| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011314| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011252| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011059| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010630| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010938| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012871| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010228| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010484| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011446| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010332| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011563| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010600| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010508| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009839| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010493| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011308| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011225| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010177| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009971| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010651| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009532| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009114| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009909| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009743| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009961| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008243| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008405| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007969| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007419| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008177| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007477| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008034| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007818| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007843| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007715| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "OMC training\n",
      "Epoch[1/50] | loss train:0.081334| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014782| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014506| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011651| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011876| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011120| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011472| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010754| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011865| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011166| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010173| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010590| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010689| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010357| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010279| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010184| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010398| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010432| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010972| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011515| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010296| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010250| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010531| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010059| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009509| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010281| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009395| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010333| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010861| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010244| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009756| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010504| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009679| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011377| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009566| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010027| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009587| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008506| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008798| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008406| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008231| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008187| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008085| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008214| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008287| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008684| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008199| lr:0.001000\n",
      "Number data points 5772 from 2000-05-02 to 2023-04-11\n",
      "ON training\n",
      "Epoch[1/50] | loss train:0.076519| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020228| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.020360| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015771| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015515| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.020064| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013706| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014367| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.014635| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014311| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.017002| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014195| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.014000| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012179| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014910| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.015252| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013743| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014753| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014888| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010764| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011941| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011891| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011833| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011952| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012769| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013711| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013561| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011861| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012914| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012752| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011419| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013747| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.014507| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011848| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012513| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013835| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013101| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012520| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013916| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012989| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009695| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009660| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009006| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008695| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009764| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009692| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009359| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009159| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008949| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008262| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "OKE training\n",
      "Epoch[1/50] | loss train:0.055353| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012722| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012085| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012833| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010700| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011447| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013071| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011915| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010699| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009547| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010763| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011877| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010626| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010201| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010243| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009761| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009582| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010600| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009311| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010021| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010330| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010050| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009566| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009500| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010580| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010538| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008841| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009339| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010094| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009211| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009021| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009141| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009930| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010484| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010039| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009184| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008860| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009400| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008201| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007823| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007738| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007800| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007754| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007486| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008032| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007956| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007206| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007461| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ORCL training\n",
      "Epoch[1/50] | loss train:0.075021| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016529| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016968| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013678| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013335| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013100| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011857| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015482| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012291| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011389| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014537| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011231| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011907| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011576| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011969| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009784| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010592| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010756| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009866| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011995| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010191| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011574| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012032| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010172| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010580| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010135| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011030| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011542| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010963| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011061| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009881| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010818| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009594| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010910| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010449| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011058| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011530| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009816| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011155| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009636| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009176| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008238| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008409| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008051| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008681| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008312| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008066| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007861| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008565| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008198| lr:0.001000\n",
      "Number data points 469 from 2021-05-14 to 2023-04-11\n",
      "OGN training\n",
      "Epoch[1/50] | loss train:0.040355| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013045| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.008574| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006688| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006625| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005396| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005676| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005261| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005102| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004747| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004433| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004854| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004659| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004561| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.004324| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004794| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004187| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004411| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004271| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004289| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004051| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004593| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004245| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004186| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004309| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004815| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004682| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004263| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004894| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004691| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004232| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003988| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004054| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004669| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004232| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004488| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003865| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003997| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004292| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003878| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003847| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003677| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003893| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003973| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004184| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003838| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003798| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003898| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003864| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003890| lr:0.001000\n",
      "Number data points 771 from 2020-03-19 to 2023-04-11\n",
      "OTIS training\n",
      "Epoch[1/50] | loss train:0.042802| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008623| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006620| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005904| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004890| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004373| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.004137| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004888| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.003910| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.003479| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.003462| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.003439| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.003513| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.003711| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.003948| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.003570| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/50] | loss train:0.003634| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.003383| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.003109| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.003216| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.003494| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.003718| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003070| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.003141| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.002938| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003353| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.003654| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003653| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.003371| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003148| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.002840| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.003113| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003011| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003046| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.002860| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003121| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003165| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003476| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.002793| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003046| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.002611| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.002494| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.002971| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.002775| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.002800| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002533| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002810| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.002576| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.002476| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.002693| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PCAR training\n",
      "Epoch[1/50] | loss train:0.069196| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016504| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013874| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012821| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013045| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011401| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011787| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010762| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010320| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009479| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010159| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010404| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009248| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009968| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009714| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009856| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011712| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009884| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010969| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009643| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009495| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009555| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009209| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009400| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009693| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008975| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010149| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009839| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009691| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009401| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009251| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008899| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009863| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009126| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009108| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009259| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008841| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009479| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008019| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007602| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007472| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007611| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007281| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007043| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006736| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007269| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007627| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007311| lr:0.001000\n",
      "Number data points 5837 from 2000-01-28 to 2023-04-11\n",
      "PKG training\n",
      "Epoch[1/50] | loss train:0.081113| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013644| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012141| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012361| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011686| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010464| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011262| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009923| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010246| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009487| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009909| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009008| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009304| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009655| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009985| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009479| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009673| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009108| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008891| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008685| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009142| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008834| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009263| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009212| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009311| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008864| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008783| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009003| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008947| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008851| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008496| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008462| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008608| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008549| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008114| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009234| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008701| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009340| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008620| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007761| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007192| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007210| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007138| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007184| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006686| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007081| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006885| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006944| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006940| lr:0.001000\n",
      "Number data points 4342 from 2006-01-03 to 2023-04-11\n",
      "PARA training\n",
      "Epoch[1/50] | loss train:0.072112| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015221| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012087| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013494| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013178| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011085| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010813| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009914| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010674| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010173| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010677| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009723| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011392| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009471| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008870| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009731| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009841| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009349| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010112| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009675| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009796| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010197| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010557| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009653| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010591| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009143| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009396| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009739| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009506| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009290| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008528| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/50] | loss train:0.011319| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009995| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008986| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009494| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009727| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008524| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007750| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007423| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008167| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007443| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007676| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007464| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007057| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009043| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007253| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007252| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PH training\n",
      "Epoch[1/50] | loss train:0.100772| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017679| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013926| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013355| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011733| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010738| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013924| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011003| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013178| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009754| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011300| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011438| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012036| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009220| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010934| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010384| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010292| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011798| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010334| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010849| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009840| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009599| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010285| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009197| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009897| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010682| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008965| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009875| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009472| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009429| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010175| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009653| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010265| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010874| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009262| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009317| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009719| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008192| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008351| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007564| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008066| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008094| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007718| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007338| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008130| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007390| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007631| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PAYX training\n",
      "Epoch[1/50] | loss train:0.060808| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013450| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012734| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014392| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013076| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011376| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010779| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009157| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010594| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009650| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009260| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009113| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010513| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010279| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011183| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009548| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009679| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009661| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009761| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009341| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009669| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008187| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010646| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008756| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009724| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008717| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009837| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009490| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009130| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008466| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008849| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009215| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010015| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008936| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009185| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009118| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007453| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007212| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007023| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006846| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007066| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007007| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007289| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006902| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007194| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007306| lr:0.001000\n",
      "Number data points 2263 from 2014-04-15 to 2023-04-11\n",
      "PAYC training\n",
      "Epoch[1/50] | loss train:0.055311| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008575| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006145| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005330| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004789| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005204| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005411| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004658| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005135| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004873| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005570| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005195| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004538| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005678| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.003767| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.003968| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004435| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004744| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.003839| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004801| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004147| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004104| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004750| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004366| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004499| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004769| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004148| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004128| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004119| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003848| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004034| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004081| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004320| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004405| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004633| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004261| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004118| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004353| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003755| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004288| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003656| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003679| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003496| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003461| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003595| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003247| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003531| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003729| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003340| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003355| lr:0.001000\n",
      "Number data points 1946 from 2015-07-20 to 2023-04-11\n",
      "PYPL training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] | loss train:0.065210| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.007790| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006250| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005077| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004986| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004864| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005273| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004274| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.004153| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004232| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004025| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.003715| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.003475| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004171| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.004647| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004162| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.003856| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004348| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004471| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004067| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004320| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004043| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003606| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006319| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.003992| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003506| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.003733| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003886| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.003696| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004201| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003760| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004795| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004334| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003522| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.003812| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004320| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004251| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003370| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004722| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004595| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003642| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003055| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003379| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003097| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003374| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003089| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003180| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003016| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003118| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003103| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PNR training\n",
      "Epoch[1/50] | loss train:0.079467| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016580| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016710| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013386| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014954| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011855| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012414| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012324| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010510| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011703| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012017| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014039| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011908| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011122| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011778| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011025| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010663| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010670| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011248| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010538| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011037| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010560| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010902| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011685| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010766| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010588| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012398| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010616| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010600| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009999| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011039| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009434| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009617| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011286| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010095| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009809| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009992| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009562| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010806| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011326| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008488| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008247| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008113| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008599| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007955| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008248| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008463| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007773| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007945| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008183| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PEP training\n",
      "Epoch[1/50] | loss train:0.060730| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015101| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011996| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011541| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009324| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009760| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010167| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010725| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010165| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008615| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010134| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009435| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009063| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009356| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008574| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010070| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009115| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009547| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008872| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008815| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008227| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009708| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008312| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008841| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008608| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008707| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008570| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007891| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008679| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008697| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008309| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008060| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009273| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008075| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008336| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008562| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007951| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008125| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006972| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007232| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006656| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006627| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006648| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006322| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006453| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006850| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006723| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006926| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PKI training\n",
      "Epoch[1/50] | loss train:0.078346| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016269| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012785| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014814| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012350| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010348| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013596| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012539| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010734| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010992| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011372| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010512| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012563| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011301| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010394| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010077| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009625| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010422| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/50] | loss train:0.009316| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011605| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010133| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010455| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009716| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009641| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009435| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011461| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009696| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009299| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010708| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011563| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009546| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010293| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009789| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009641| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010000| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009558| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009895| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009846| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008798| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007816| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007558| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007800| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007302| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007499| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007654| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007761| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007685| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007507| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PFE training\n",
      "Epoch[1/50] | loss train:0.078559| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014628| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014112| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013705| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014162| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016425| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013450| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011187| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012022| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011632| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011525| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012259| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010533| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011746| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012372| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011926| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011326| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011749| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010595| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010440| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012796| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011079| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010560| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011683| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010559| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010804| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011884| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010286| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011395| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009673| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010003| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011026| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010156| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010121| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010461| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010643| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009700| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010872| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009818| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008355| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008405| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008226| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007563| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008133| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008190| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007277| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007762| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007822| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007381| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PCG training\n",
      "Epoch[1/50] | loss train:0.087506| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018566| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014195| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012728| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014929| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013136| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011916| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012719| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010482| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011017| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012991| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011943| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012006| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011270| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011635| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011294| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010548| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010069| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009829| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011263| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010624| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009823| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010274| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010050| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009934| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011294| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010544| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010093| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010643| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010071| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010868| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010970| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010577| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010281| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009674| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010103| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010579| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010384| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008808| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008397| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008236| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007943| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007900| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007776| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007939| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008114| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008135| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008064| lr:0.001000\n",
      "Number data points 3794 from 2008-03-17 to 2023-04-11\n",
      "PM training\n",
      "Epoch[1/50] | loss train:0.060649| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012011| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010232| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009253| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009195| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006969| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008055| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006962| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008019| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008606| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007471| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007071| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006876| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007009| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007852| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006425| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008532| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007184| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007005| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006736| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007400| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006988| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006394| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007719| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006863| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007216| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006534| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006784| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006138| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006188| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006745| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006705| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006542| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006469| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006635| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006528| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/50] | loss train:0.006788| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006297| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006419| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006681| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005592| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005370| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005388| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005194| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005169| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005279| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005100| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005015| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005020| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005079| lr:0.001000\n",
      "Number data points 2767 from 2012-04-12 to 2023-04-11\n",
      "PSX training\n",
      "Epoch[1/50] | loss train:0.084551| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011853| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010200| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010289| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009399| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011176| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009380| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008610| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008415| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008790| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008692| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009032| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008091| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010014| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008459| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008368| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008556| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007922| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008571| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009772| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008318| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007664| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009474| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008333| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008786| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007074| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007540| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007479| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008300| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008451| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007900| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008262| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008622| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008978| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008826| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008016| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007745| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007301| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007906| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007197| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006808| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007039| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006459| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006809| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006446| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007026| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006472| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006143| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006526| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PNW training\n",
      "Epoch[1/50] | loss train:0.069200| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014856| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011563| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010676| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010266| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009306| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008942| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009669| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009284| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009515| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009179| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009422| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008637| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008865| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008968| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008614| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009228| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008008| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008297| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009143| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009106| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008361| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009306| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008720| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008872| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008701| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008426| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008773| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007862| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008504| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008733| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008365| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008349| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008388| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008104| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007476| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007438| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007406| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007088| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007097| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007313| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006952| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006918| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007006| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PXD training\n",
      "Epoch[1/50] | loss train:0.065988| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015172| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016457| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013319| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013352| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013326| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012379| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013167| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012231| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011668| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010712| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011161| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011423| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011161| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012206| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011427| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011356| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011043| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011551| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011039| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010979| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011781| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010716| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011154| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011234| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011898| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011035| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009903| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011521| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011367| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010839| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010908| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010282| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010530| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011011| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010521| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010364| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010876| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010096| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008559| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008192| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008779| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008800| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008515| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008542| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008506| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008312| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008348| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008531| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PNC training\n",
      "Epoch[1/50] | loss train:0.077692| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016206| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/50] | loss train:0.013335| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011992| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012962| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010386| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012893| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011846| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011306| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010448| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011275| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012501| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010360| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010833| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009608| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011397| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010359| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010929| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010519| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010391| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009259| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010218| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009586| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010507| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010537| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009457| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009934| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009403| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009729| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010465| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009903| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009552| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010223| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010114| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010649| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008718| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007647| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008340| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007448| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007770| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007618| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007512| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007802| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007613| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007499| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "POOL training\n",
      "Epoch[1/50] | loss train:0.059239| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014540| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014073| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012608| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010954| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012042| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013765| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011162| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012489| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011521| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010241| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012339| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010666| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010075| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012134| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010822| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011093| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010998| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010207| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.016280| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010761| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010071| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009939| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010581| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010561| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009821| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011237| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010228| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009306| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011322| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010203| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011358| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009882| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009716| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009664| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009970| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010132| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008959| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007768| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007245| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007160| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007597| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007342| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007250| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007957| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006733| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007583| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PPG training\n",
      "Epoch[1/50] | loss train:0.054143| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014843| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011374| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011087| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009895| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009983| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009426| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011198| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009994| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010107| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009001| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010710| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009546| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009084| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009660| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009292| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011126| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009534| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009895| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009252| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010520| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009154| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008963| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009415| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009410| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008698| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008953| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009640| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008574| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008896| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009122| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009317| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008427| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008727| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008510| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009366| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008222| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007507| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007433| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007268| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007088| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007048| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007500| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007551| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007348| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007472| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PPL training\n",
      "Epoch[1/50] | loss train:0.063622| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015460| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013096| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013058| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012242| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010956| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011418| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010532| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010215| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011282| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009860| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010258| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010048| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011216| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010485| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010639| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009850| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010290| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010242| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/50] | loss train:0.010895| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010181| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010006| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010070| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009619| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009999| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009438| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009583| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009760| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010065| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010319| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009796| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009546| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010910| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010217| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009163| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009897| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008429| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008484| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008237| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008171| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008057| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007910| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008008| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008062| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008053| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008235| lr:0.001000\n",
      "Number data points 5403 from 2001-10-23 to 2023-04-11\n",
      "PFG training\n",
      "Epoch[1/50] | loss train:0.061872| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016211| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013016| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013005| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013315| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012080| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010843| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010970| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011189| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011724| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012589| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012494| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012399| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011676| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009934| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011889| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011645| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011713| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011028| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010255| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011771| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010347| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010363| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011090| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010098| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011189| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010782| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009908| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009850| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010502| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010390| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010018| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010309| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010973| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010300| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009729| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011378| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009630| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010865| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010074| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008499| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008338| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008312| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008331| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008210| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007946| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008322| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008352| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008042| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007872| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PG training\n",
      "Epoch[1/50] | loss train:0.061008| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014843| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013037| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011015| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011372| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010338| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010774| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010247| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010698| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009361| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009859| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009736| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010518| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009208| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009513| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010128| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009057| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008892| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008732| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009274| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009475| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008821| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008822| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009555| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009124| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009730| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009451| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008771| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008816| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009287| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009106| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008514| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008559| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008781| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008609| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009045| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008430| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009213| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007304| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007011| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007157| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006924| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006690| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006752| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006811| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006439| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006880| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007117| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PGR training\n",
      "Epoch[1/50] | loss train:0.100069| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015942| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017400| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013099| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012416| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013768| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011422| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011421| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011129| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011462| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012423| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011407| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011449| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011950| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009232| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012132| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011315| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010200| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010078| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011115| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010607| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010009| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010665| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009229| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010823| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010111| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009306| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010505| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010697| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009223| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010883| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009537| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009470| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008629| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/50] | loss train:0.009261| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007909| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008014| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007053| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007066| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007234| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007176| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007317| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006885| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006889| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006697| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PLD training\n",
      "Epoch[1/50] | loss train:0.059254| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016432| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014840| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012918| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012551| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009973| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011330| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012407| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010852| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009791| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011193| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011881| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011134| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011325| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011239| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011485| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009670| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010084| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009910| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009388| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009258| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009570| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011001| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009783| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009809| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008844| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009670| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009074| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010948| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009251| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008624| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008497| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008150| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007871| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007249| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007737| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007619| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007495| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007633| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007646| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007100| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007331| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007055| lr:0.001000\n",
      "Number data points 5367 from 2001-12-13 to 2023-04-11\n",
      "PRU training\n",
      "Epoch[1/50] | loss train:0.061687| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017974| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014298| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012471| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013540| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011748| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013396| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011181| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011227| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012132| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012151| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012866| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011691| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011746| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011050| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011016| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010857| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011020| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010539| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011331| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010960| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010140| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010968| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011637| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010813| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010191| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010517| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010561| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010169| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010591| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009736| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010423| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010412| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009821| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010867| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010320| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010793| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008978| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008734| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008883| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008680| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008549| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008502| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008489| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008351| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008626| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008642| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PEG training\n",
      "Epoch[1/50] | loss train:0.066247| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014424| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010884| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011649| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010409| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010322| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009356| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010456| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010476| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010163| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009225| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008784| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009560| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008707| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009508| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009024| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008925| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009216| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008735| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009414| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009534| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009544| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008757| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008855| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009337| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008582| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008785| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008210| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008785| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008320| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008454| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008471| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008833| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008712| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007936| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007514| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007386| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007350| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006900| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006970| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007298| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006998| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006883| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007289| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006951| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PTC training\n",
      "Epoch[1/50] | loss train:0.056353| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016664| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013706| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014275| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/50] | loss train:0.012223| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011450| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011953| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011611| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012534| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011158| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010653| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011188| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010521| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011691| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011011| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010490| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012124| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011355| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010983| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011157| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010491| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010352| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009836| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010899| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010265| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010668| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011945| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009729| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009578| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009989| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011253| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010554| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010581| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010567| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009908| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009619| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010220| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009825| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009688| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008574| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008325| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008100| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008348| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008254| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007753| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008499| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008039| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008251| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008072| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PSA training\n",
      "Epoch[1/50] | loss train:0.069485| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013925| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012462| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010961| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010509| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012425| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010876| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010728| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008819| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010921| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011326| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009632| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010093| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010329| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010395| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008997| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009590| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008965| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010874| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009575| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009791| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008763| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008804| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009464| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009765| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009301| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009677| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009675| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009909| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008921| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009277| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010096| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009580| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009203| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009514| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009360| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007482| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007543| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007449| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007552| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006805| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007121| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007156| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007270| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006915| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006588| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PHM training\n",
      "Epoch[1/50] | loss train:0.065961| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016018| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015956| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013731| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014821| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013127| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012641| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014170| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013206| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012006| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012943| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013173| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011284| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011542| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012826| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011684| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012339| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013640| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013577| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010548| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011520| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012735| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011515| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011271| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011168| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011068| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010953| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011666| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012315| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011316| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011731| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011584| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011410| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011770| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011806| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011241| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012105| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010954| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012711| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010904| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009987| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009797| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009535| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009226| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009226| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009263| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009792| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009223| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009119| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009187| lr:0.001000\n",
      "Number data points 2082 from 2015-01-02 to 2023-04-11\n",
      "QRVO training\n",
      "Epoch[1/50] | loss train:0.048412| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012587| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010544| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008477| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012580| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008380| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008769| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011884| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.018428| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010015| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011696| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008841| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008683| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.018390| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008272| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010106| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011469| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008492| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.015650| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006699| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005975| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010887| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/50] | loss train:0.007146| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005795| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006788| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007247| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007775| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006467| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006734| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009995| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007564| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005532| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009046| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.018710| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011639| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007132| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005653| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006045| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007797| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007278| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005651| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008585| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004686| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006092| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.016933| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005945| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008269| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005975| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004825| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006479| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "PWR training\n",
      "Epoch[1/50] | loss train:0.084977| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017536| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.020425| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.017228| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015618| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015642| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015826| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015313| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013140| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014219| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.016983| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012671| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013824| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.018733| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012828| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012737| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013369| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012557| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013495| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011810| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012010| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010966| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014687| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012205| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013109| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013789| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012462| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012734| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012277| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.015352| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012158| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012785| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013713| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012267| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011714| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011792| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011154| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011649| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011865| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009782| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009335| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009011| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008916| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008871| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008591| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009246| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009521| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009392| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008402| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "QCOM training\n",
      "Epoch[1/50] | loss train:0.085913| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020401| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015253| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014908| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.017285| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013875| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014248| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014008| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013726| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012689| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012560| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012058| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012445| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011874| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012187| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011398| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013379| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012567| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011822| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012260| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011946| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012592| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012716| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011551| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011492| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011396| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012947| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010888| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010211| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011201| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012149| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012429| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012256| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010362| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011281| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012554| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011858| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012397| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010941| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008819| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009207| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009202| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009515| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009357| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008374| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009001| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008379| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008859| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009015| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "DGX training\n",
      "Epoch[1/50] | loss train:0.070093| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015429| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015987| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012290| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010904| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011673| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010709| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011135| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012225| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010330| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010622| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011262| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010064| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009976| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011482| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009563| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010719| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010082| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009255| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011267| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011283| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010775| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009022| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009116| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009625| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009000| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009885| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009730| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010417| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009526| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010074| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009433| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009001| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010273| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009759| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009199| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009856| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/50] | loss train:0.007853| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007466| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007662| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007720| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007819| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007704| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007493| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007122| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006911| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007777| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RL training\n",
      "Epoch[1/50] | loss train:0.060831| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013119| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013343| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014082| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012275| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011950| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011423| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010960| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011359| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011824| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010904| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011233| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011182| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011105| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010287| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011334| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010761| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010400| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010886| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010298| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010339| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011210| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010356| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011214| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010386| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011015| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011218| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010293| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010985| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010118| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010624| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010679| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009729| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011125| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010117| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010731| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009568| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010738| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010497| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010915| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008889| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009037| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008736| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008785| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008993| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008785| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009207| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008567| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008550| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008965| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RJF training\n",
      "Epoch[1/50] | loss train:0.071308| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015756| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012801| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012189| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012516| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011765| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010857| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013428| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013311| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011582| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010871| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011002| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012168| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010962| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012224| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010669| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011115| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010171| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009890| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009746| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010027| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010078| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010045| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010085| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011994| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010777| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009680| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011279| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009345| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009577| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009995| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009856| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011549| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011274| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009544| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009728| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009449| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010408| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008376| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008331| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007924| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007910| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007952| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007582| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007709| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007860| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007716| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007856| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RTX training\n",
      "Epoch[1/50] | loss train:0.062952| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013761| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011204| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011874| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010786| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010976| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011215| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011668| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010844| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008993| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010734| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009914| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010671| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011573| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010504| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010231| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008875| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010753| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009719| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009618| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010620| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009771| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010524| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009484| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010076| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009302| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009743| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010290| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009094| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009780| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008721| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008760| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009853| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009676| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009340| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009370| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007869| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007353| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007824| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007597| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007421| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007407| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007418| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006883| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007362| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007517| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "O training\n",
      "Epoch[1/50] | loss train:0.059163| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013190| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011164| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012348| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009118| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009721| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/50] | loss train:0.008490| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010944| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009955| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009005| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009335| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009209| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011133| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008527| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008928| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008711| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008541| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009125| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008453| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009274| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008844| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008575| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009194| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008723| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008505| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008548| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008824| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008752| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008731| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009490| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008169| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008604| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008438| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008878| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008782| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007506| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007148| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006923| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006965| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007049| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006822| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007102| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006990| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007262| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007282| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "REG training\n",
      "Epoch[1/50] | loss train:0.059545| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015360| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013020| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011915| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011555| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010960| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011490| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011429| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011810| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012382| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010417| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011720| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011260| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011013| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010934| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010157| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010585| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011257| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010905| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010393| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010384| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011576| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010943| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009956| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010968| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010125| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010645| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010818| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009876| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010354| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010718| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010034| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010036| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010859| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010117| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010043| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009901| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009419| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010459| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008893| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008685| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008316| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008491| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008494| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008257| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008368| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008590| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008492| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008439| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "REGN training\n",
      "Epoch[1/50] | loss train:0.057617| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012947| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011783| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010730| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009800| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010956| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009361| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010249| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008906| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010884| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010057| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009869| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009174| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009799| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008622| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009257| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009250| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009352| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008832| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009904| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009145| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009507| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008786| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009677| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008291| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009973| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009367| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008508| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009322| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008836| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008945| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009925| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008603| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008533| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009049| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007895| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007640| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007051| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007575| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006945| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007288| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007321| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007359| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007361| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007408| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RF training\n",
      "Epoch[1/50] | loss train:0.057151| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015449| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013439| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013291| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012291| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013227| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012788| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012217| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011760| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012580| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011649| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010964| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012046| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011928| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010684| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011105| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011430| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011489| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010792| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011662| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010762| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011281| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010670| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011366| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[25/50] | loss train:0.011115| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011225| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010903| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010982| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010812| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011006| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011030| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010930| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011699| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011231| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010705| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011032| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010922| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011416| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011169| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010954| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009520| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009529| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009053| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009409| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009864| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009424| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009179| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009304| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008885| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008943| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RSG training\n",
      "Epoch[1/50] | loss train:0.066861| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013626| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011188| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010927| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009531| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010800| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010315| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009956| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009273| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010591| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008447| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009830| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008847| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009545| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010889| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008779| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009696| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010060| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008584| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009210| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009934| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008068| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009301| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008591| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008697| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010575| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008136| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009037| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009216| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009120| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009025| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008871| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008795| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008253| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008233| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007931| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009446| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007263| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007114| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006504| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006785| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006459| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006558| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007221| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006561| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007121| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006507| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RMD training\n",
      "Epoch[1/50] | loss train:0.080615| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016153| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013324| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012691| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011697| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011395| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011561| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012186| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011359| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011627| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010501| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011616| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012439| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010862| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012674| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010670| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009964| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010944| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010536| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010997| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011274| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011396| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010754| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009721| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009763| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009816| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009522| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010490| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010230| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010370| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010540| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010175| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009960| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008782| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010744| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009218| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010508| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009328| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007590| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007851| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007669| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007380| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007355| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007490| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007141| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007271| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007313| lr:0.001000\n",
      "Number data points 5897 from 1999-11-01 to 2023-04-11\n",
      "RHI training\n",
      "Epoch[1/50] | loss train:0.095341| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016760| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.020032| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014548| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013748| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014467| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014056| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014286| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.015218| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011894| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013278| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014560| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011548| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.016547| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.015615| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011488| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011329| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011197| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012594| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012466| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012020| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010759| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.014581| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012758| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010808| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010493| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013428| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011195| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010647| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013154| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011690| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012216| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010173| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011232| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011060| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010852| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011753| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010626| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010356| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012571| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011787| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009044| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/50] | loss train:0.008784| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009198| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008936| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008704| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008732| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008739| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008798| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008353| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ROK training\n",
      "Epoch[1/50] | loss train:0.060996| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013099| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017702| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013185| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011765| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011182| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011415| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011219| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010243| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011808| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010170| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010316| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009847| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009430| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010459| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010870| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009508| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010660| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009738| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009242| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011360| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010314| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010218| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009106| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009301| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010297| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009147| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009760| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008350| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009434| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010110| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009744| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009082| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010156| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009185| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009345| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008805| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008097| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007944| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007897| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007291| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007150| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007549| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006478| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006849| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007852| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007106| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ROL training\n",
      "Epoch[1/50] | loss train:0.076465| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013735| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014094| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010957| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009971| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010921| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010584| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010879| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010630| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011047| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010890| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010750| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009641| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009407| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011302| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009164| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009962| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009070| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009156| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008884| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008586| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008725| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008290| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009060| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008719| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009166| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009077| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009545| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008920| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009192| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008688| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008354| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008906| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008593| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008649| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008907| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009179| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008168| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009842| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007710| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007112| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007337| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006890| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007524| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007271| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007235| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006722| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007334| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006763| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ROP training\n",
      "Epoch[1/50] | loss train:0.068976| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013825| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011708| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010262| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010311| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012198| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009667| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009978| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009864| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008935| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009352| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010495| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009939| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009105| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009998| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008322| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009298| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008255| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008665| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009570| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008361| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008576| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009544| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008316| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008649| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008290| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007859| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008508| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008518| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008190| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008349| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008582| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008883| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009137| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008541| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008600| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008289| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008590| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008243| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008146| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007035| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007207| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007053| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006721| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006961| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006900| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006512| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006877| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006944| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007135| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ROST training\n",
      "Epoch[1/50] | loss train:0.056968| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016009| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012030| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011083| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011189| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011224| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011068| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011424| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/50] | loss train:0.011169| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009878| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009807| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009912| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011125| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009506| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009272| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010242| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009738| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009365| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009347| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009358| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009018| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009339| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008969| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009228| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009443| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009215| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009232| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009072| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008392| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009244| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008548| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009401| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008615| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009444| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008945| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007640| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007870| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007820| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007447| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007325| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007323| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007555| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007354| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007202| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007459| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "RCL training\n",
      "Epoch[1/50] | loss train:0.072712| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016011| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013528| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014871| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012463| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012906| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012988| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011768| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012036| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013073| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012223| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010706| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010839| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012032| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011888| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010991| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011641| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011471| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011151| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011149| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010981| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010941| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011151| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010602| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010939| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010787| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011315| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010875| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010039| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010796| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011324| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010564| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010217| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011188| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010357| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010770| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010819| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010637| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010846| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009798| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008685| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009050| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008784| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008598| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008922| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009426| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009185| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008639| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008471| lr:0.001000\n",
      "Number data points 5597 from 2001-01-02 to 2023-04-11\n",
      "SPGI training\n",
      "Epoch[1/50] | loss train:0.074243| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013548| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012656| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014548| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011227| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010750| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011265| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011201| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010348| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009301| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010514| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009585| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009620| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007925| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009399| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009655| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008605| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009259| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009052| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010039| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009006| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009594| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008617| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009916| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009222| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007860| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009293| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007897| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010026| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008353| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008416| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008031| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008370| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008647| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009098| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009192| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006794| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006769| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007137| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006292| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006473| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006824| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006592| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007318| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006474| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006557| lr:0.001000\n",
      "Number data points 4733 from 2004-06-23 to 2023-04-11\n",
      "CRM training\n",
      "Epoch[1/50] | loss train:0.060661| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013020| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012227| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011125| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010289| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010382| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008610| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010912| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009597| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008953| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009384| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008700| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010219| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010147| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010309| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010002| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008432| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007786| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009960| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008297| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009573| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010264| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008504| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008799| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008432| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008000| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/50] | loss train:0.008874| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008523| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009027| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008748| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008549| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008691| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007972| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007738| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008534| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008297| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008435| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009328| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007414| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006640| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006888| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006821| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006815| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007098| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006446| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007066| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006640| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006619| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SBAC training\n",
      "Epoch[1/50] | loss train:0.073552| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017894| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012126| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012384| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010733| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010658| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010052| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010108| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009568| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009244| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009652| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009644| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008901| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011200| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009858| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009478| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008640| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010619| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009322| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011694| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008872| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010240| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009714| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009144| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008363| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008922| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008722| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009138| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010251| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007789| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008836| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008795| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008370| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009150| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008940| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007485| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007620| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007012| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006833| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007098| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007060| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007587| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007148| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007492| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006980| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SLB training\n",
      "Epoch[1/50] | loss train:0.077254| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020240| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015624| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014777| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014517| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014992| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013262| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013457| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013458| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013231| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013971| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013230| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013217| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013039| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012363| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011676| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012688| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012609| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012064| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011373| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013015| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012324| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012652| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011916| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012325| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011796| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012160| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012294| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012271| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011622| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011027| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011340| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011524| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011818| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011948| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011592| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011151| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012131| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012086| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012631| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010613| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010255| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010127| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009938| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009823| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010249| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009961| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010150| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009759| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010133| lr:0.001000\n",
      "Number data points 5117 from 2002-12-11 to 2023-04-11\n",
      "STX training\n",
      "Epoch[1/50] | loss train:0.075450| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016439| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.020806| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012841| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013239| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013653| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011975| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011161| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011513| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012894| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011420| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010873| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010240| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010312| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011104| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011305| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012308| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010656| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011171| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010368| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010696| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009904| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010472| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010984| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010402| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009716| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009603| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010456| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009684| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009193| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009592| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009821| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010442| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009336| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010119| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011220| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009006| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010637| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009378| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008026| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007848| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007666| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008050| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/50] | loss train:0.007721| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007246| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007310| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007207| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007567| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008175| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SEE training\n",
      "Epoch[1/50] | loss train:0.076937| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017123| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015087| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012133| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013137| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011725| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012116| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011473| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012793| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011974| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012369| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012184| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011322| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011050| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011169| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011995| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012699| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011512| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010362| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011366| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010749| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011549| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010756| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010837| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011185| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011786| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010681| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011247| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010685| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011214| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011089| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011394| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010945| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010944| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010363| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010816| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010731| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010326| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010497| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009109| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009207| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008324| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008635| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008928| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008810| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008809| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008547| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008565| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008851| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SRE training\n",
      "Epoch[1/50] | loss train:0.062797| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014136| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012010| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011867| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010680| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010644| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009903| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010086| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010428| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010574| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010696| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009773| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009428| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009682| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008775| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008572| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010075| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008404| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009807| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010033| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008685| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009446| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009217| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008971| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008697| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008837| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009248| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009645| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009353| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008667| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009622| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009066| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009011| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009590| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009044| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009959| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007609| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007724| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007245| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007525| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007355| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007365| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007217| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007119| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007118| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007436| lr:0.001000\n",
      "Number data points 2712 from 2012-06-29 to 2023-04-11\n",
      "NOW training\n",
      "Epoch[1/50] | loss train:0.047530| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008123| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007286| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005969| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006789| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005847| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006045| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006778| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005340| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005262| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005908| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005880| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005988| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005223| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006111| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005644| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005550| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005795| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004838| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005506| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005428| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005289| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005230| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004968| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005343| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005037| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005284| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004838| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004659| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005322| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004715| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004784| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005024| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005484| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005700| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005169| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004719| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005032| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005212| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005684| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004097| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004087| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004396| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004235| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003930| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004415| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003894| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003833| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004001| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004126| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SHW training\n",
      "Epoch[1/50] | loss train:0.058432| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013352| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012998| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011858| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011373| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011825| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009400| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011038| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010564| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010111| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/50] | loss train:0.010267| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011044| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011539| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009713| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010283| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011423| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009766| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009017| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009664| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009510| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009819| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010598| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008891| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010542| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009398| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009415| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008944| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008672| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010495| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008937| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009270| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008635| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009881| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009919| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008919| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008659| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008759| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008443| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008446| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009552| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007876| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007415| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007311| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006893| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007500| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007224| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006891| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007206| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006990| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007176| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SPG training\n",
      "Epoch[1/50] | loss train:0.060537| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013519| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011456| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011566| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009615| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010993| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010473| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010097| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010138| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009662| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009243| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009319| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010679| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009136| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008909| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009613| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009331| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009108| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009346| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009290| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009638| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008524| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008552| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009113| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008595| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009475| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008768| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009805| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008604| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008704| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008900| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009380| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008960| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008787| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009381| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008448| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008705| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007615| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007489| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007245| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007149| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007467| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007309| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007276| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007487| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007128| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007148| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SWKS training\n",
      "Epoch[1/50] | loss train:0.063824| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015787| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014608| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013177| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012897| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012497| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012928| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011680| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011183| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011070| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012596| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011883| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011510| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010826| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011509| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010032| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013504| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010683| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011614| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010355| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011169| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010950| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011293| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011391| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010684| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010653| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010637| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009826| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010511| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010957| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010481| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010550| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010393| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009588| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011212| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009970| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011171| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010345| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009833| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010356| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009014| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008596| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008442| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008270| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008676| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008455| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008504| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008528| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008063| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008175| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SJM training\n",
      "Epoch[1/50] | loss train:0.057022| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012904| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011593| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011553| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010102| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010367| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009986| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010901| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009554| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008894| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010012| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008876| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009057| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008659| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009192| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009278| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008985| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008256| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008426| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008878| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008563| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008733| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008908| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008524| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008354| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008301| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[29/50] | loss train:0.009487| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007955| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007977| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008516| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008389| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008419| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008642| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008344| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008502| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008778| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008259| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008040| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007015| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006896| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006728| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006631| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006997| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006893| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006532| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007071| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006781| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006973| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SNA training\n",
      "Epoch[1/50] | loss train:0.069466| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013789| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011539| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011906| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010678| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010177| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010507| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011065| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009304| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010053| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009406| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008958| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011767| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009140| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009725| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010688| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009469| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009964| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008481| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009125| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008356| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009245| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008922| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009188| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008358| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008344| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008427| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009009| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008435| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008748| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008085| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007969| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008094| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008430| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007383| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007363| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007083| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006969| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006724| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007252| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006962| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006930| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007021| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006785| lr:0.001000\n",
      "Number data points 2025 from 2015-03-26 to 2023-04-11\n",
      "SEDG training\n",
      "Epoch[1/50] | loss train:0.083251| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015554| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011082| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008193| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008646| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006674| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007370| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006618| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006244| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006992| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007454| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006130| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005620| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006710| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006355| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006720| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006047| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007026| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006204| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005429| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005853| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005131| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005147| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008245| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006312| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005538| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006012| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004900| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004870| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006244| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005412| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.007845| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005810| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005801| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005630| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007137| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005064| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005914| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005110| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006020| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004502| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004499| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004732| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004289| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004394| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004599| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004651| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004294| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004597| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004207| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SO training\n",
      "Epoch[1/50] | loss train:0.083879| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014729| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013887| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011382| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011093| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009792| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011503| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011588| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011168| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009761| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008829| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009654| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010268| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008895| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010034| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008819| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009551| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008193| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010342| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009707| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009815| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009640| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008896| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008921| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008876| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008359| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008697| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009740| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010032| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008708| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008599| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008727| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009311| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008520| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008640| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008762| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007696| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006890| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007182| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007260| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006933| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007015| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[47/50] | loss train:0.006833| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006656| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006917| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007011| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "LUV training\n",
      "Epoch[1/50] | loss train:0.063903| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013743| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013365| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012355| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012694| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012214| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010054| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010686| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011984| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010561| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011887| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010903| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010396| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010699| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009912| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011405| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010338| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010350| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009878| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010879| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010107| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009924| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010019| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010098| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010149| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009637| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009884| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010507| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009075| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009527| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010623| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009495| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009694| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009795| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010052| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009466| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009836| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009370| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008296| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008328| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008384| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008391| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007919| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008082| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007764| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008033| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007962| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008225| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SWK training\n",
      "Epoch[1/50] | loss train:0.069628| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015265| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011645| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012309| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010480| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012863| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011356| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010966| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011483| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010652| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009368| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010480| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009222| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010351| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010006| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009877| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009247| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009521| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011406| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010008| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010656| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009381| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009871| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010713| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010164| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009389| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010183| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008981| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009249| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010103| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009942| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009425| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008975| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009816| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010026| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009984| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008718| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009808| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009944| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007835| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007847| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008011| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007862| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007699| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007483| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007406| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007273| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007214| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007656| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SBUX training\n",
      "Epoch[1/50] | loss train:0.052743| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014451| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012372| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011537| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012021| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009576| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009756| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011265| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009268| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010354| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010554| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009652| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008975| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010026| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009791| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009164| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009185| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009798| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008843| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009324| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009745| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008949| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010431| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008354| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008875| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009292| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009358| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008627| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009706| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008136| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009336| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009133| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009579| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008817| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008064| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008493| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008231| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008450| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008544| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007374| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007180| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007288| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007022| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007288| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007210| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006971| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007258| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007404| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006963| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "STT training\n",
      "Epoch[1/50] | loss train:0.073027| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.020527| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.017625| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015779| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.016165| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015032| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.017762| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.015629| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.015388| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014660| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.015606| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.015239| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[13/50] | loss train:0.015157| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014372| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.015880| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014557| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013881| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.014265| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013800| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.014691| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013119| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.014394| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013552| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.014190| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013575| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013816| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013213| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014427| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013672| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.014574| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.014053| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013938| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013584| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.013330| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.014483| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012712| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.013851| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.015533| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012907| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.013587| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011780| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011644| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.011584| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.011775| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.011975| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.011214| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.011696| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.011513| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.011607| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011624| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "STLD training\n",
      "Epoch[1/50] | loss train:0.082551| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.023412| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.021099| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.019308| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.018809| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.016976| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.016218| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013887| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013368| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.020170| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.018129| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.016128| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.017815| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013380| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.014934| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.014444| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013023| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013050| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012098| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.015840| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.016059| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011279| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.015391| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.013280| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013701| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012890| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.014459| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.014851| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013276| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011738| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012712| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.013849| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013039| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011904| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.015240| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011628| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012048| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.014734| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012916| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011319| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010009| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009503| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010840| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008625| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009770| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009417| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008614| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010008| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008432| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008677| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "STE training\n",
      "Epoch[1/50] | loss train:0.084712| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014992| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014346| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013465| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011831| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012011| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011082| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011143| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010040| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011793| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009293| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010057| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009477| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010860| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010168| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010169| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009673| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009602| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009741| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008778| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009672| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008840| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009282| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009984| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008728| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008877| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008921| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008809| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009170| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008774| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009107| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008721| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008816| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008306| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008751| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007656| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007487| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007012| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006816| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006931| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006778| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007123| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006525| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007334| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006860| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SYK training\n",
      "Epoch[1/50] | loss train:0.067505| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016178| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011831| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012282| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011323| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009893| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010016| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009980| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010955| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010127| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009514| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010284| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009091| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009350| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009106| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009490| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009630| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009483| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009751| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009823| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009611| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009541| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009597| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008958| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008957| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008820| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009719| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008547| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[31/50] | loss train:0.008789| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009080| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008893| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009182| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009295| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008930| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009453| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008541| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008641| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008412| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007750| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007073| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007370| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007025| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007010| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006865| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006979| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006961| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006965| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007107| lr:0.001000\n",
      "Number data points 2189 from 2014-07-31 to 2023-04-11\n",
      "SYF training\n",
      "Epoch[1/50] | loss train:0.058005| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013120| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010962| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011751| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010663| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011530| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013374| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010143| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008421| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010910| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009386| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009539| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010852| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009535| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012324| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009482| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009758| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009671| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008936| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009386| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009551| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008460| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008496| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009449| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008900| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008863| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009864| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009376| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009496| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009016| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008665| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009391| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008334| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008899| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008693| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009452| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009172| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008325| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008673| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007454| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007549| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007977| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007898| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007644| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008001| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007522| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007191| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007242| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SNPS training\n",
      "Epoch[1/50] | loss train:0.063546| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016994| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015336| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013931| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012691| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014283| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012344| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013058| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011653| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012379| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011660| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012647| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011608| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012035| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011522| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011958| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011603| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010845| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013035| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010192| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010447| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011435| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010040| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011172| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010859| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010710| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009497| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009530| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011058| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011836| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009862| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010106| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010901| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008835| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010577| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009571| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010229| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010549| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012192| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008456| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007532| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008246| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007507| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007764| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007837| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007357| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008067| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007633| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007712| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "SYY training\n",
      "Epoch[1/50] | loss train:0.057964| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014863| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015210| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012162| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010581| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010856| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012140| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010633| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009964| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009973| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009565| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011194| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010082| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011033| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010019| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009156| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009922| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009985| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010691| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009195| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009916| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009238| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009993| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009272| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009722| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009992| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009121| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010227| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009281| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009302| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008433| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009072| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010000| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008700| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008973| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009298| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008748| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009014| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008793| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007638| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007721| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007751| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007906| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007526| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007656| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007282| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007535| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[49/50] | loss train:0.007833| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007196| lr:0.001000\n",
      "Number data points 4023 from 2007-04-19 to 2023-04-11\n",
      "TMUS training\n",
      "Epoch[1/50] | loss train:0.054384| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010619| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009665| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.008729| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008208| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008454| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007366| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007154| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007402| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.006980| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007869| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007059| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007061| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007687| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006870| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007150| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008110| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007122| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007283| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007052| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006453| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006336| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006634| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006835| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006345| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006260| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006273| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006362| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006988| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.006538| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006984| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006451| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006286| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005614| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006129| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006582| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007185| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006232| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006700| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005487| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005400| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005452| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005354| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005307| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005298| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005015| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004830| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004896| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004826| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TROW training\n",
      "Epoch[1/50] | loss train:0.069360| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016033| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.018526| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013983| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011467| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011832| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012724| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013721| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011865| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012204| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012111| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011018| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012282| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012830| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012593| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010349| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010439| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011274| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010755| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011439| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011305| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010258| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010684| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010929| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012454| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011111| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010311| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010348| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010710| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011185| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010104| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011383| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013049| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009894| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010102| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010798| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009336| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009997| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008848| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008456| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008349| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008028| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008047| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007938| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007638| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007585| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008383| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007890| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TTWO training\n",
      "Epoch[1/50] | loss train:0.080845| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016792| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013045| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014278| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012040| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012803| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010582| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012113| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010266| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010002| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010573| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009953| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010719| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009449| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011941| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011202| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010987| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009987| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009387| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010896| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010042| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009632| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009479| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009757| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010593| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009628| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010215| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009768| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010298| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009689| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010002| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009241| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009548| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009781| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009427| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011605| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009761| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008285| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007792| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007729| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008220| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007817| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007600| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008056| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007637| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007393| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007321| lr:0.001000\n",
      "Number data points 5662 from 2000-10-06 to 2023-04-11\n",
      "TPR training\n",
      "Epoch[1/50] | loss train:0.062697| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016195| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014398| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013659| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012919| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012864| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012877| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011844| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013693| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012359| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011671| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012243| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012850| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012542| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[15/50] | loss train:0.011755| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011792| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011509| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011728| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011906| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011365| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011433| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011195| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011518| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012141| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011578| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012113| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011289| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012152| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011055| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011076| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010954| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011261| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010807| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011353| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010872| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011228| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011413| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011139| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010887| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011270| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009660| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009496| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009758| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009225| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009539| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009269| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009179| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009335| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009572| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009435| lr:0.001000\n",
      "Number data points 3106 from 2010-12-07 to 2023-04-11\n",
      "TRGP training\n",
      "Epoch[1/50] | loss train:0.058370| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015929| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014677| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009702| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010011| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009280| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009589| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011278| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009396| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008338| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.015334| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009562| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008373| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010745| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011133| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011167| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.019123| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008457| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010357| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008100| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.015444| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008190| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010377| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.015752| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010173| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008287| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008369| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008159| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012650| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007965| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010368| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007115| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007016| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007297| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008528| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009284| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009456| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009224| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009629| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008321| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007481| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007250| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006274| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006670| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007274| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006648| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007752| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009279| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TGT training\n",
      "Epoch[1/50] | loss train:0.079168| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016864| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016704| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011995| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014158| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012814| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012585| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011676| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012388| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011986| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011855| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.016447| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012150| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011048| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013857| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012008| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011890| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011147| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010557| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011233| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011290| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012451| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010714| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010126| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012445| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010391| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011178| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011688| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011232| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011110| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009862| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011111| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011611| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010471| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011800| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011381| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010037| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010543| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008870| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008530| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007664| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008088| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008618| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007594| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007941| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008268| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008026| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007867| lr:0.001000\n",
      "Number data points 3984 from 2007-06-14 to 2023-04-11\n",
      "TEL training\n",
      "Epoch[1/50] | loss train:0.065061| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012751| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010982| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009072| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007975| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008414| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007664| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008164| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.008014| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008365| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006891| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007704| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007353| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007473| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007802| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007310| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007210| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007293| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007123| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.006891| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006992| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006933| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006435| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007302| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007467| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006425| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006765| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006112| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006308| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007172| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005998| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006718| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[33/50] | loss train:0.007156| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006676| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006657| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006862| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007889| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007364| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006714| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006252| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005685| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005410| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005544| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005847| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005370| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005480| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004992| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005436| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005125| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005791| lr:0.001000\n",
      "Number data points 5882 from 1999-11-23 to 2023-04-11\n",
      "TDY training\n",
      "Epoch[1/50] | loss train:0.079307| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015851| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013069| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011622| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011970| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011704| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010123| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010615| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010941| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010249| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010634| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009993| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009190| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010843| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011166| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009498| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009361| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009134| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009992| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010345| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009927| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010441| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009868| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008941| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009056| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009737| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009855| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009272| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009262| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008946| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010989| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010319| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008661| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008600| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009257| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009475| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008689| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009201| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007754| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007414| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007132| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007094| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007117| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006931| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006776| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007080| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006974| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007145| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TFX training\n",
      "Epoch[1/50] | loss train:0.072650| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013214| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011782| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011080| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009498| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009932| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009098| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010534| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010166| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009678| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009724| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009704| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009254| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009458| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011904| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009168| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008615| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009206| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008192| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010172| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008961| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008985| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009755| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008962| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008308| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008736| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008388| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008418| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009329| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008407| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008424| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008746| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008536| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008934| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008913| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009134| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007598| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007447| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006871| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007236| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007183| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006916| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007408| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007014| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006939| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TER training\n",
      "Epoch[1/50] | loss train:0.089922| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.027126| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.026571| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.017407| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.017152| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.017098| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.015261| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.018734| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.017798| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013948| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.014704| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014665| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.015665| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014425| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.015938| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013815| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014301| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013181| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013388| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013896| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013282| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.015342| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013380| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012972| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013325| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.014045| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.013825| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013236| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013830| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.013221| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012991| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012554| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013269| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.014008| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.014001| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.013644| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012509| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013175| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.013915| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012551| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.011846| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010280| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010689| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010541| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010784| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010616| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010551| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010377| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010608| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.011320| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 3218 from 2010-06-29 to 2023-04-11\n",
      "TSLA training\n",
      "Epoch[1/50] | loss train:0.081954| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014714| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010212| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009893| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007893| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007514| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007857| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007924| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007506| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009528| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007678| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006908| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007573| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006671| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008085| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007430| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.007807| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008814| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008035| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007189| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007930| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007705| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007403| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007040| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006760| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006588| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007338| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007074| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008643| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006350| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007058| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007086| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007961| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008640| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.007643| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006240| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006733| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007727| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006069| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005959| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005978| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005992| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005543| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005699| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005602| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005317| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005462| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005687| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TXN training\n",
      "Epoch[1/50] | loss train:0.070956| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015402| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013069| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012860| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011206| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011530| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009581| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012592| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011329| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010974| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010924| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010480| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011036| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010030| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010544| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012199| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009421| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010035| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010883| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011209| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008531| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009376| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010106| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009458| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009782| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009636| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008767| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008875| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009855| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008627| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008438| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009462| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009158| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009100| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008798| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008807| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007658| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007467| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007272| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007198| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007116| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007018| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006778| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007286| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007450| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007162| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TXT training\n",
      "Epoch[1/50] | loss train:0.074573| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016815| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016439| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013568| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014017| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015074| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014926| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012921| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013020| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013654| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011898| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011937| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011445| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.014257| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012237| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013372| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012567| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012116| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012272| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011895| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011638| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012779| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012323| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011902| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011695| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011782| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011947| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012604| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011916| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012157| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011374| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011544| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011776| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011814| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012350| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011589| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011432| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011191| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.012059| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012036| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010396| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009649| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010040| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009711| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.009899| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009624| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009789| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009459| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009738| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009314| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TMO training\n",
      "Epoch[1/50] | loss train:0.077356| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014280| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015555| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012131| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013410| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013133| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011880| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012721| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013351| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011266| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010997| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010958| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010362| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010566| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009592| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010300| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/50] | loss train:0.010210| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009262| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010881| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010998| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008659| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009235| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008822| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011065| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009547| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009472| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009529| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009647| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008569| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009913| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008947| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010431| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008884| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008798| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009311| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009218| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008638| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009591| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008628| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.006940| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006722| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007252| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007118| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007060| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007016| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007055| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007032| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007004| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006712| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TJX training\n",
      "Epoch[1/50] | loss train:0.060613| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011862| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013105| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012205| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012120| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010649| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009733| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011025| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009928| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009598| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009786| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009643| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009512| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010107| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009409| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008878| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009323| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011280| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009029| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010323| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008729| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009181| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008990| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009042| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009505| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010105| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009032| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009146| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009369| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008999| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008331| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008210| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009049| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008930| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008745| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009538| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009382| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008886| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007274| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007737| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007589| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007722| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007723| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007172| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007272| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007344| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007090| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007722| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TSCO training\n",
      "Epoch[1/50] | loss train:0.056926| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013981| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010807| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011832| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014206| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009965| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011795| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010344| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011277| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011965| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010186| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009189| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009056| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010755| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010722| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011500| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010437| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009389| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010133| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008775| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010370| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009298| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009835| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008779| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009119| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010190| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009529| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008555| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008796| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009416| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010141| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008929| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008258| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009677| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008998| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008757| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009040| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009065| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008351| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007068| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007365| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007361| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007450| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007046| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006567| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007167| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006835| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006521| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006809| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TT training\n",
      "Epoch[1/50] | loss train:0.073450| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016927| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011584| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012136| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011770| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010634| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011028| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010046| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009441| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011031| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011114| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011163| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009100| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009063| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009963| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010024| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010140| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009507| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010287| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008911| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009813| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009885| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009617| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009454| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008738| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009669| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009035| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008745| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008749| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008732| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008380| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008983| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009440| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009267| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[35/50] | loss train:0.009575| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009435| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008502| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008190| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008673| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008751| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007376| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007061| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007644| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007081| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006591| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006996| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006901| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006825| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007157| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006872| lr:0.001000\n",
      "Number data points 4298 from 2006-03-15 to 2023-04-11\n",
      "TDG training\n",
      "Epoch[1/50] | loss train:0.055020| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012804| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011049| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009847| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.008203| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009246| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008817| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007646| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007463| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.008803| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007405| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008495| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007350| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006885| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008987| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.007523| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.007596| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006892| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.007670| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007087| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006833| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006960| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006678| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006637| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.006894| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007018| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007286| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006437| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.007698| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006117| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007252| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006702| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006426| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.007233| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006798| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007007| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006288| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.006385| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.006506| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008091| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005443| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005703| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005660| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005574| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005348| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005446| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005342| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005497| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005478| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005239| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TRV training\n",
      "Epoch[1/50] | loss train:0.068346| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014701| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014113| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011402| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011199| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011041| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010430| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010949| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010544| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010644| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009080| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009890| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010267| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009567| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009572| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009649| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010033| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008849| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009274| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010121| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009225| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008621| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009149| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010264| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008494| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008277| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009021| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008848| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008918| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008829| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008794| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008934| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008845| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008767| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008917| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009249| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008717| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008385| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008684| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007613| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007545| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007314| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007501| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007439| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007303| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007444| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006714| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007190| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007224| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TRMB training\n",
      "Epoch[1/50] | loss train:0.085329| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018334| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014738| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015175| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012426| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014582| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014149| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011772| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011902| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011875| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012623| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011748| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012169| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012288| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011804| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011794| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011658| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011095| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012188| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010594| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012378| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010915| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010502| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011336| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009934| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011638| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011346| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010169| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010210| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010043| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009960| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010646| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011322| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011320| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011060| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010275| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010213| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010647| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008721| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008427| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008521| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008354| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007903| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008526| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007994| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008061| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007798| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008084| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TFC training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] | loss train:0.067585| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016133| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013690| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014838| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.015892| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.015146| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013296| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012785| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013582| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014590| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013620| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012889| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013067| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012097| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012509| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012977| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.014041| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013241| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.014343| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012422| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011886| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.013906| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012026| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011972| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.012278| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.013509| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012102| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013266| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012979| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012303| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012331| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.014201| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012979| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012156| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011814| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012007| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012653| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.013445| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011770| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012977| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010228| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009879| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009913| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010158| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.010044| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009683| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010040| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009933| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009774| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009752| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TYL training\n",
      "Epoch[1/50] | loss train:0.076846| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019502| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015614| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014269| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012944| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012117| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011375| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011125| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011474| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011765| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.013540| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011406| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011054| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011857| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010243| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010410| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010750| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009993| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009232| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009547| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009934| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009379| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008730| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010254| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013090| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009906| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010819| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009600| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009767| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009038| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010448| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010031| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009230| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010175| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010192| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009407| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008921| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007875| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008172| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007518| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007367| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007366| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007436| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007640| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007214| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007601| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007786| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "TSN training\n",
      "Epoch[1/50] | loss train:0.058155| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014485| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012953| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010650| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010808| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010064| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010530| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009753| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009282| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010824| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009705| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009590| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008933| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009522| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009036| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009390| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009706| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009932| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008541| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008841| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009513| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008723| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009665| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009100| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008601| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009067| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008118| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009105| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009378| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009160| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008572| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008852| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008161| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008431| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.007948| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008461| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009248| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008772| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008693| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007603| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007572| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007247| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007547| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007281| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007012| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007229| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007563| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006846| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006930| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "USB training\n",
      "Epoch[1/50] | loss train:0.071731| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015890| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012174| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013782| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012003| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010399| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011029| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011653| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010109| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010684| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011601| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011278| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010578| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010827| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010249| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010470| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009698| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[19/50] | loss train:0.010947| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010850| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010410| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009441| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010528| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010291| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009282| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009287| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010215| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010287| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008922| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009457| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009448| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009320| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009724| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009592| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009805| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008734| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009927| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009323| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009603| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008559| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008241| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008031| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007918| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008234| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007927| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008025| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008214| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007971| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007623| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "UDR training\n",
      "Epoch[1/50] | loss train:0.079420| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016087| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013352| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013130| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011265| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011656| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010540| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011818| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011563| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010138| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010536| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010231| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011037| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011159| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011050| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010303| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011747| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010445| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010453| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009614| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010166| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008939| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009884| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009707| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010483| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010103| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009692| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009692| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008972| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009515| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009813| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010287| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010929| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009774| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009637| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010512| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010279| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009248| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007942| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008248| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007847| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007625| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008226| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007860| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008093| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007715| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007066| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008035| lr:0.001000\n",
      "Number data points 3891 from 2007-10-25 to 2023-04-11\n",
      "ULTA training\n",
      "Epoch[1/50] | loss train:0.049956| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010637| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.010764| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010272| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007744| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.008175| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009092| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007166| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.007888| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.007595| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007449| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008470| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007551| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.007278| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007911| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006862| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009252| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006721| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.006878| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007397| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006297| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.007537| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.007133| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.007491| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.007033| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.007065| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007844| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.006357| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006741| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006784| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.007330| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006407| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.006732| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.006605| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.007026| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006687| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006762| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007342| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007007| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.007156| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005934| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005482| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.005552| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.005239| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005578| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.005245| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.005324| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.005364| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.005490| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.005258| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "UNP training\n",
      "Epoch[1/50] | loss train:0.061387| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013808| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015593| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010742| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011566| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011416| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010065| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011535| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009862| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010065| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009775| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011781| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009101| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009113| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010321| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009654| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008726| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010359| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009137| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009245| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009650| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009193| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008808| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009771| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008854| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009366| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008579| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008674| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009279| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008762| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008315| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008672| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008854| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008888| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008562| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009205| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[37/50] | loss train:0.008275| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008371| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008084| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008297| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007012| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007195| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007050| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007112| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007118| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007401| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006585| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006971| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007218| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006847| lr:0.001000\n",
      "Number data points 4324 from 2006-02-06 to 2023-04-11\n",
      "UAL training\n",
      "Epoch[1/50] | loss train:0.053948| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015593| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014066| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.015320| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012972| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013036| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012140| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012989| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010044| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011047| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013252| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014489| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013528| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011385| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013230| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011795| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010781| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011189| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010877| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010363| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011586| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011552| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010473| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012777| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011207| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010682| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009924| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012754| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012644| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010749| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010962| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010620| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.013625| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010392| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011436| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011667| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011756| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010469| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010103| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011567| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010365| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.011653| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008720| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009436| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008727| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010307| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007955| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008303| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008806| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008730| lr:0.001000\n",
      "Number data points 5891 from 1999-11-10 to 2023-04-11\n",
      "UPS training\n",
      "Epoch[1/50] | loss train:0.064977| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017010| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014676| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013520| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011543| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013364| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014393| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011564| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012408| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011776| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011298| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012027| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010376| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011323| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009819| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012314| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011080| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009902| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010347| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010068| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009760| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009843| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010454| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010371| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010252| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008796| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011492| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010212| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010413| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010630| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009994| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010365| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009925| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008523| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009954| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010767| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010361| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009499| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009662| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008001| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008079| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008010| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007320| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007800| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007762| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007401| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007793| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007898| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007753| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "URI training\n",
      "Epoch[1/50] | loss train:0.067415| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014686| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014123| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013515| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012466| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011902| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010313| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011993| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011818| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011443| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010922| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010802| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010424| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010331| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010700| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010867| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010653| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010502| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011160| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010324| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011601| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011020| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009543| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010472| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010233| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009930| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009560| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008855| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009697| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009161| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011625| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009870| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010220| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009512| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009630| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009578| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009340| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009252| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009476| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008439| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007874| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007139| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007860| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007330| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007160| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007706| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008493| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007803| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007715| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "UNH training\n",
      "Epoch[1/50] | loss train:0.092047| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015326| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/50] | loss train:0.012567| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012338| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011201| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011403| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.009249| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009367| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010047| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011894| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011036| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010900| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009666| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009581| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010822| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009872| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009616| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010627| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010389| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010373| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010071| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009053| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010842| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009127| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009070| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009035| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008420| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009981| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009302| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009437| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008506| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008521| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010508| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008636| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008799| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010189| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008396| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008247| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007036| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006796| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007106| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007088| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007230| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006839| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006684| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007137| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006944| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006529| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "UHS training\n",
      "Epoch[1/50] | loss train:0.080221| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017209| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013845| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013697| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011676| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011538| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011149| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011483| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010838| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009541| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011064| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010167| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009840| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009657| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010240| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009152| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010508| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009442| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008771| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010749| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009463| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009937| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009037| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009031| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009226| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008616| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009498| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009720| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009647| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009340| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009253| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009440| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009780| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009309| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009752| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008808| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008781| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007822| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007731| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007683| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007749| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008048| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007965| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007940| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007926| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007705| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "VLO training\n",
      "Epoch[1/50] | loss train:0.060598| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016997| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.016364| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013661| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014432| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013420| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014871| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013324| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012861| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012726| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012839| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011911| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011846| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.012852| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010831| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012677| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.012815| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011463| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.015802| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009863| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012224| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012341| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010859| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010976| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010873| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011255| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009946| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010469| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010683| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011502| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011884| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011029| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011146| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011283| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011476| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011713| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010308| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010276| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011133| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011242| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009225| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008928| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008285| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008241| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008759| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008629| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008073| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008853| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008366| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008321| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "VTR training\n",
      "Epoch[1/50] | loss train:0.047248| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012488| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011644| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011560| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009781| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011811| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010275| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009683| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009244| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009791| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010249| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009474| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010112| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009656| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009392| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008594| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009374| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008831| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008945| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[21/50] | loss train:0.009167| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009584| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008718| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009237| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009745| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008526| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008879| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009251| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009243| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009007| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008797| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009183| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008527| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009590| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008675| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008828| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008344| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008346| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009167| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009749| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008041| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007497| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007675| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007621| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007299| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007311| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007531| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007389| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007452| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007319| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "VRSN training\n",
      "Epoch[1/50] | loss train:0.063466| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015579| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012940| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013997| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012204| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012274| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010786| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011970| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010585| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010678| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010484| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012333| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011096| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011404| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009960| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010290| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010653| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010400| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010400| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009465| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010184| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010466| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010411| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009628| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009642| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010182| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010243| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009478| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010178| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009673| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009894| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010968| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009646| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009499| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009223| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009579| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010468| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010027| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008713| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008581| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008078| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008175| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007893| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008348| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008093| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008168| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008272| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007968| lr:0.001000\n",
      "Number data points 3400 from 2009-10-07 to 2023-04-11\n",
      "VRSK training\n",
      "Epoch[1/50] | loss train:0.055411| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010253| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.009932| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007354| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007976| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006859| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006503| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.006119| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006398| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005669| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005527| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005648| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006187| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006483| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006023| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005727| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006363| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005687| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005781| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.004772| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005499| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.005274| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.006593| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004939| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005470| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005797| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004813| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005320| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006567| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005405| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005283| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005354| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005690| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005114| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005810| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005258| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004943| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004933| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005051| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004917| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004281| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004538| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004373| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004402| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004175| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004347| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004364| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003977| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004575| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004216| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "VZ training\n",
      "Epoch[1/50] | loss train:0.081845| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015102| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012311| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013007| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009170| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010063| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011362| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009516| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009429| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009374| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009378| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011127| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008978| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010341| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.008643| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010037| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008525| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009266| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009173| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008967| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009778| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009064| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008281| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009468| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008342| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008596| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009135| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008864| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009052| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009009| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008219| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008849| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009188| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009604| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008927| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008325| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008914| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008761| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[39/50] | loss train:0.009263| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008349| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007436| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006947| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006967| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007386| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007397| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007710| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007052| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007294| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007256| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007177| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "VRTX training\n",
      "Epoch[1/50] | loss train:0.077760| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017640| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014126| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012817| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013102| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011895| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013085| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011378| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011985| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.014603| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010903| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010747| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011911| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011167| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011921| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011106| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011396| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012112| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011640| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011850| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010494| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011084| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011027| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010693| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011262| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010790| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010761| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011079| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010926| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010106| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010480| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010742| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011032| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010386| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.011818| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010880| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010383| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010361| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010675| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011208| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008992| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008713| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008274| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008671| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008573| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008147| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008530| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007888| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008517| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008208| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "VFC training\n",
      "Epoch[1/50] | loss train:0.079122| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012720| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011333| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011306| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010652| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010279| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010349| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010186| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009385| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009162| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009610| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009136| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009151| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009156| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009612| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008984| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009467| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008355| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009435| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008966| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009304| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010004| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010170| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008299| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009101| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008566| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009146| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009296| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008618| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008731| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008903| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009288| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008492| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008713| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008905| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009123| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008998| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007980| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007431| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007276| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007600| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007351| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007402| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007308| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007291| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007244| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007201| lr:0.001000\n",
      "Number data points 603 from 2020-11-16 to 2023-04-11\n",
      "VTRS training\n",
      "Epoch[1/50] | loss train:0.037418| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.008149| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.005968| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.005402| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004499| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.004355| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.003968| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.004194| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.003831| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.003672| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.003654| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.003811| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.003494| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.003922| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.003682| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.003820| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.003943| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.003977| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.003566| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.003441| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.003349| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.003998| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.003500| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.003277| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.003600| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.003793| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.003059| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.003418| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.003382| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.003592| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.004104| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.004020| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003668| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.003626| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.003314| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003389| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.003503| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.003298| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.003382| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.003711| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003698| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003435| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003072| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003060| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003251| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.002862| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.002956| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003041| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.002724| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003210| lr:0.001000\n",
      "Number data points 1379 from 2017-10-17 to 2023-04-11\n",
      "VICI training\n",
      "Epoch[1/50] | loss train:0.046541| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009305| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006326| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009306| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/50] | loss train:0.008121| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007730| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005658| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010368| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005693| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005302| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.007530| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.006229| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.004930| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005481| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007435| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006792| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004961| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005515| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005624| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005987| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004069| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004810| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004017| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006863| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004020| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.004916| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.007105| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005203| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004706| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006691| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005195| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009022| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.007548| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004145| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.004900| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.004765| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.006907| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005354| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005310| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.006619| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005448| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003846| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003939| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004728| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003074| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003918| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003250| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006184| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004125| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003952| lr:0.001000\n",
      "Number data points 3792 from 2008-03-19 to 2023-04-11\n",
      "V training\n",
      "Epoch[1/50] | loss train:0.058308| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.009729| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007922| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.009455| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.006689| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.007332| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.007730| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.007570| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006373| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005938| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006021| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007059| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006235| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.006631| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.007147| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005356| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.006792| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.006779| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005886| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005727| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.007139| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006011| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005761| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005915| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005861| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005730| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.005818| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005395| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.005760| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.005684| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005602| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005506| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005684| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005347| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005545| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005846| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005662| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005800| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005643| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005790| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004760| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004715| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004714| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004624| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004977| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004931| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004610| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004465| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004525| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004696| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "VMC training\n",
      "Epoch[1/50] | loss train:0.072541| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016324| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012332| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012234| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011581| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013700| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012518| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011314| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012330| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011088| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011541| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011684| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010993| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011341| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011042| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010625| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011436| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011134| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010788| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010503| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009513| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009745| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010610| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010200| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010161| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009324| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010055| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011123| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009812| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009703| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010322| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009933| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009851| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009534| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010392| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009705| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010595| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010126| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009995| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008526| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008980| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008073| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007925| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008414| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008081| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007791| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008078| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008160| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007938| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WAB training\n",
      "Epoch[1/50] | loss train:0.069800| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012569| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011466| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010910| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010035| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010176| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010504| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008660| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009060| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009371| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.008488| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009800| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009366| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008714| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009667| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008783| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009098| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009084| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008815| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008624| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008836| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009300| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/50] | loss train:0.008620| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008371| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009037| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008289| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008755| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009290| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009277| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008294| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008505| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008737| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008121| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008685| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008765| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009105| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008565| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008206| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008314| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008122| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007322| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007624| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007373| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007191| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007456| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007150| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007258| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007159| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007197| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007537| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WBA training\n",
      "Epoch[1/50] | loss train:0.055824| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.013993| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013932| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012980| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012471| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012035| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011194| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011423| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012238| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012039| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010650| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010959| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011398| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010482| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011204| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011300| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010970| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010533| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010498| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010309| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011174| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010405| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010321| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010812| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011353| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011009| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010379| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010916| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010461| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010536| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010084| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010697| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009946| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010165| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010059| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010960| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010592| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011041| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009908| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008826| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009346| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008614| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008937| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008903| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008898| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008539| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008939| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008363| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008564| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WMT training\n",
      "Epoch[1/50] | loss train:0.061678| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012867| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011696| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013119| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009738| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011130| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011069| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012038| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010380| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011919| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009921| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009468| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009151| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011014| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009124| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010645| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010686| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009526| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010355| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009701| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009439| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010138| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011216| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009639| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009647| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009225| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010031| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010137| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008917| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008836| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009528| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008789| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009455| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008431| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008931| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009196| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009809| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008110| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006979| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007393| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007362| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007014| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007300| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007432| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006955| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007358| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006977| lr:0.001000\n",
      "Number data points 4468 from 2005-07-08 to 2023-04-11\n",
      "WBD training\n",
      "Epoch[1/50] | loss train:0.068066| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017222| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015309| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.017331| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014044| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013009| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.014827| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014500| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010855| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012934| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011106| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010891| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011691| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011390| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011802| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011152| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011019| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010594| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010952| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012873| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010968| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011461| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013144| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011558| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011273| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010745| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010751| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.012199| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.012291| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011480| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011165| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010567| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011283| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011415| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010513| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010234| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011386| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010631| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009853| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010896| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[41/50] | loss train:0.009607| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009357| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009778| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009090| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008391| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008274| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008362| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007920| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007973| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008091| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WM training\n",
      "Epoch[1/50] | loss train:0.094019| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012174| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012928| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011293| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010354| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011004| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.008955| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008765| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009291| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009902| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010822| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.008703| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.008566| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009182| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009086| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009608| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009049| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009337| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.007721| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008511| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009341| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009370| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009033| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008546| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008897| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008835| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008585| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.008085| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.007981| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008989| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008435| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008064| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008243| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008842| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008187| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008961| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.007536| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007827| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008739| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007265| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.006988| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007203| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007123| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006264| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006693| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006710| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006601| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006642| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006813| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WAT training\n",
      "Epoch[1/50] | loss train:0.072822| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015878| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015352| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012488| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012226| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011302| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013849| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011256| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011769| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012212| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010747| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009786| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011495| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010308| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.010413| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013152| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010431| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010572| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010569| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008914| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010880| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010005| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011222| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011141| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010406| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010596| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010281| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011603| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010556| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010481| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010124| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009748| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009459| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009455| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009622| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009482| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008790| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009505| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009088| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009491| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008188| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007822| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007505| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007633| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007482| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007831| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007349| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007224| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007406| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007386| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WEC training\n",
      "Epoch[1/50] | loss train:0.069588| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014879| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014287| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012036| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010357| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010862| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010514| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010121| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010522| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010226| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009578| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010279| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009658| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009566| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009781| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.008963| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008777| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010286| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009345| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.008676| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009134| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009110| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009309| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008545| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008979| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.008288| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.008367| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.008608| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009416| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008730| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008797| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009308| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008954| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008239| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009062| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008330| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008526| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008826| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008490| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008506| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007527| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007144| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007088| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.006862| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007023| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006927| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006930| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007197| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007135| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006999| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WFC training\n",
      "Epoch[1/50] | loss train:0.063952| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016974| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012971| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014139| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012012| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011931| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/50] | loss train:0.011509| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010774| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011547| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011613| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011937| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010820| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011065| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011460| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011242| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010994| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011261| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010609| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011323| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010921| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010979| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010975| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010151| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010672| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011581| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011386| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010232| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010613| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010670| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010540| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010472| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010833| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010454| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010536| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010972| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010530| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010207| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010256| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010536| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010812| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009610| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009176| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009312| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008957| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008800| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008856| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008724| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008730| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008917| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008981| lr:0.001000\n",
      "Number data points 5591 from 2001-01-02 to 2023-04-11\n",
      "WELL training\n",
      "Epoch[1/50] | loss train:0.051700| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.011968| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011408| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011622| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011129| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009192| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010028| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009347| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009878| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009468| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.010441| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009155| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011104| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010144| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009770| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009266| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008528| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009427| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.008838| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009768| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009922| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009565| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008984| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008772| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009424| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009373| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009338| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009963| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009261| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008993| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008585| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009699| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009673| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009260| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008150| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009656| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008829| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009310| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008887| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007478| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007272| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007553| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007473| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007620| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007471| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007488| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007520| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007587| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007092| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WST training\n",
      "Epoch[1/50] | loss train:0.058374| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016654| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014395| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013380| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012888| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012525| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012171| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.013276| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.010986| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010350| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011759| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010782| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012730| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010962| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012970| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010856| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.012249| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010171| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010746| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010955| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011799| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.013304| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009675| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010196| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009272| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009806| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009758| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009736| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009955| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010163| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010555| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010477| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009023| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009737| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010623| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010139| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009333| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008757| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010497| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008300| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007385| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007751| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007547| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007629| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007850| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007228| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007058| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007235| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007417| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WDC training\n",
      "Epoch[1/50] | loss train:0.063773| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016245| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012934| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012647| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.013947| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011696| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012260| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011070| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011370| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012436| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011219| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010986| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010356| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011275| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011028| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011039| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010719| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011232| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009952| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010559| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010243| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010227| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010587| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011771| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[25/50] | loss train:0.009437| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011077| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010059| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010848| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010288| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010446| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010770| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010030| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010240| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009721| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010197| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010769| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010222| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010265| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010030| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010327| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008365| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008845| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008237| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008455| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008480| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008288| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008635| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008352| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008362| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008428| lr:0.001000\n",
      "Number data points 1963 from 2015-06-24 to 2023-04-11\n",
      "WRK training\n",
      "Epoch[1/50] | loss train:0.061964| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015366| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.012681| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012516| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.010633| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.009592| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011099| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011142| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012006| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010225| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009709| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012874| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010973| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010308| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009641| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009567| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.009795| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009272| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009179| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009754| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011681| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009138| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010008| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010077| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010088| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010008| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010552| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010131| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010708| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009276| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.011731| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009743| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010994| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009411| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010466| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009343| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011993| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009222| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008476| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008707| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009857| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008723| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008109| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007578| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008352| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008681| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008593| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007688| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.009235| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008813| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WY training\n",
      "Epoch[1/50] | loss train:0.067171| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016414| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013205| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013027| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012845| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013196| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011746| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.014258| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011730| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011945| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011871| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011347| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011513| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011141| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012532| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011494| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010885| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010910| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011357| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011267| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012596| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011367| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011442| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010412| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011062| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010799| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011238| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.011054| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011148| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011257| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010810| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011825| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010807| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011506| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009967| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011498| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010374| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010872| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011423| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010117| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009645| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009300| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009250| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008930| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008815| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009310| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.009278| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008618| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008914| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008794| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WHR training\n",
      "Epoch[1/50] | loss train:0.065503| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.017412| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013084| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012441| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011574| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013286| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011580| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011454| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011991| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011390| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011434| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011963| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011729| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010662| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011122| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010941| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011283| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010627| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010999| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.010951| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011650| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010467| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010839| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010361| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009773| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010916| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009674| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010152| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010604| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010516| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010180| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009894| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010599| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.009944| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010313| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010238| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009823| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.009640| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.010429| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009511| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009358| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008557| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43/50] | loss train:0.008454| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008440| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008339| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008584| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007782| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008071| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008298| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008316| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "WMB training\n",
      "Epoch[1/50] | loss train:0.072101| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016085| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014725| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.014681| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014247| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.013509| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013540| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012668| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012709| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013274| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.012501| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011511| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012454| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011861| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.012021| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.012045| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011814| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011084| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011656| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011312| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.012383| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011755| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011586| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010516| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011854| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012355| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011091| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010648| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011262| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.011306| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010485| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.011099| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.011860| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010630| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010529| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.011594| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.011159| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011275| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011304| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010984| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009208| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009405| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.009057| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009121| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008803| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008875| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008676| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008833| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008874| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.009058| lr:0.001000\n",
      "Number data points 1829 from 2016-01-05 to 2023-04-11\n",
      "WTW training\n",
      "Epoch[1/50] | loss train:0.049446| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010810| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007348| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006970| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009012| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005364| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.005984| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.009062| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006521| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005782| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.004857| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.007278| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.007101| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005699| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.006854| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.006229| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005976| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005627| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.005353| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009143| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.006372| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.006085| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005004| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.006113| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005875| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.006583| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.006035| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.007610| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.006776| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.006105| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005892| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005779| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.005863| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005972| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.006408| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.006483| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005227| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005843| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008874| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005816| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.005066| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.005508| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007831| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004307| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.005118| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004782| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004306| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004327| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004595| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006117| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "GWW training\n",
      "Epoch[1/50] | loss train:0.081591| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.015272| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014382| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013360| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011914| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.011055| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012895| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.011595| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011222| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011867| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011512| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010363| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011253| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010075| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009761| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010459| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011477| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.010296| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.010480| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.012267| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011226| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008952| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.011900| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010785| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011082| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011210| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009499| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009466| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010630| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010485| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010287| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009191| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010036| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011319| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008879| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010689| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009372| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011125| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009775| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.010197| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008572| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008111| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007393| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007238| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007824| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007866| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007546| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007409| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007984| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007225| lr:0.001000\n",
      "Number data points 5149 from 2002-10-25 to 2023-04-11\n",
      "WYNN training\n",
      "Epoch[1/50] | loss train:0.068178| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.016013| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.014048| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012718| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012668| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012299| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012888| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012021| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9/50] | loss train:0.014246| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.012021| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011628| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.012046| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.012172| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.011323| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011353| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.011633| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.011446| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011398| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011418| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011426| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.011614| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011605| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010492| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011634| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.011648| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010946| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011275| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010800| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.011290| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010945| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010115| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012040| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010391| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011705| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010154| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009915| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.010312| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.011778| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011016| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011076| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.009561| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.009050| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008781| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.009195| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.008891| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.009412| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008859| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.009122| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.008877| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008726| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "XEL training\n",
      "Epoch[1/50] | loss train:0.070612| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.014386| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011085| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.011190| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011837| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010833| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011911| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010227| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009698| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.009814| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009435| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010087| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009868| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009689| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009978| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009321| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010078| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009295| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009404| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009280| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.008508| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.010034| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.009800| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.009635| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008631| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.010166| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.009155| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009342| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009149| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009566| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009316| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008544| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.009408| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008631| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.010104| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.008112| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008339| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008590| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008507| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008672| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007577| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007576| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.006801| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007189| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007021| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006889| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.007091| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006956| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007292| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.006725| lr:0.001000\n",
      "Number data points 2891 from 2011-10-13 to 2023-04-11\n",
      "XYL training\n",
      "Epoch[1/50] | loss train:0.057507| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010123| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.007158| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.007069| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.007509| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.006254| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.006838| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005820| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.006099| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.005853| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.006960| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.004923| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.006174| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.005185| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.005503| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.005786| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.005155| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.005102| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004546| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005522| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.005484| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004968| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.005606| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.005955| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.005141| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005030| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004870| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.005301| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004427| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004867| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.005404| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.006020| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.004730| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.005293| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.005380| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.005218| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.005293| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.005241| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.005485| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.005679| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.004730| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.004572| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.004409| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.004244| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.004012| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.004275| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.004165| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.004159| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.004077| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.004109| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "YUM training\n",
      "Epoch[1/50] | loss train:0.064373| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.012947| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.011183| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.010051| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.009711| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010381| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.010578| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.008929| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.009688| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.010054| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.009854| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.009578| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.009415| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.009209| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.009787| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.009422| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.008587| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.008636| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.009417| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.009429| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.009097| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.008555| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.008663| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.008566| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.008653| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009594| lr:0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[27/50] | loss train:0.008594| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009012| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009911| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.008346| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.008077| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.008434| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.008774| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.008356| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.008372| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009359| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.008295| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.008978| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.007938| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.008188| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.007492| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007010| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007423| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007060| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.006885| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.006777| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.006768| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.006757| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.006953| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007083| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ZBRA training\n",
      "Epoch[1/50] | loss train:0.051544| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018097| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015737| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.013464| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.011036| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.012576| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.012627| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012413| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.012191| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013070| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013271| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.011048| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.011871| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010163| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.016127| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010514| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010813| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.011407| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.012378| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011686| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.013169| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.011903| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010418| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.011453| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.009363| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.011840| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.010169| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.009498| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.010272| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.010619| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.010898| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.010963| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010077| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.011076| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009334| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.010128| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009984| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010830| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.008958| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.011386| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008013| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.007967| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.007234| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.007861| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007347| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.007906| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008274| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.007139| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007429| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.007729| lr:0.001000\n",
      "Number data points 5462 from 2001-07-25 to 2023-04-11\n",
      "ZBH training\n",
      "Epoch[1/50] | loss train:0.080110| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.019363| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.013754| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.012583| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.012085| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.010562| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.011662| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.010893| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.011506| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.011315| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.011478| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.010861| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.010863| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.010487| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.011121| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.010772| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.010431| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.009826| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.011143| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.011131| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.010725| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.009721| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.010177| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.010394| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.010073| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.009629| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.011204| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.010212| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.009284| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.009914| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.009451| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.009627| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.010066| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.010306| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.009465| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.009152| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.009730| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.010023| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.009514| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.009487| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.008591| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.008286| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.008207| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.008202| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.007988| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.008120| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.008025| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.008238| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.007835| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.008208| lr:0.001000\n",
      "Number data points 5898 from 1999-11-01 to 2023-04-11\n",
      "ZION training\n",
      "Epoch[1/50] | loss train:0.057894| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.018796| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.015148| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.016321| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.014351| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.014399| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.013615| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.012548| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.013681| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.013865| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.013217| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.014416| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.013391| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.013947| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.013388| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.013748| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.013189| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.013618| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.013333| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.013277| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.014036| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.012347| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.012792| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.012644| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.013073| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.012184| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.012437| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.013600| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.013270| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.012287| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.012178| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.012658| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.012446| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.012638| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.012178| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.012803| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.012394| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.012241| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.011738| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.012794| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.010976| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.010569| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.010708| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.010469| lr:0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[45/50] | loss train:0.010436| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.010817| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.010349| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.010411| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.010727| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.010076| lr:0.001000\n",
      "Number data points 2565 from 2013-02-01 to 2023-04-11\n",
      "ZTS training\n",
      "Epoch[1/50] | loss train:0.057234| lr:0.010000\n",
      "Epoch[2/50] | loss train:0.010175| lr:0.010000\n",
      "Epoch[3/50] | loss train:0.006948| lr:0.010000\n",
      "Epoch[4/50] | loss train:0.006089| lr:0.010000\n",
      "Epoch[5/50] | loss train:0.004731| lr:0.010000\n",
      "Epoch[6/50] | loss train:0.005995| lr:0.010000\n",
      "Epoch[7/50] | loss train:0.004954| lr:0.010000\n",
      "Epoch[8/50] | loss train:0.005135| lr:0.010000\n",
      "Epoch[9/50] | loss train:0.005543| lr:0.010000\n",
      "Epoch[10/50] | loss train:0.004630| lr:0.010000\n",
      "Epoch[11/50] | loss train:0.005104| lr:0.010000\n",
      "Epoch[12/50] | loss train:0.005122| lr:0.010000\n",
      "Epoch[13/50] | loss train:0.005247| lr:0.010000\n",
      "Epoch[14/50] | loss train:0.004769| lr:0.010000\n",
      "Epoch[15/50] | loss train:0.004818| lr:0.010000\n",
      "Epoch[16/50] | loss train:0.004691| lr:0.010000\n",
      "Epoch[17/50] | loss train:0.004634| lr:0.010000\n",
      "Epoch[18/50] | loss train:0.004349| lr:0.010000\n",
      "Epoch[19/50] | loss train:0.004618| lr:0.010000\n",
      "Epoch[20/50] | loss train:0.005248| lr:0.010000\n",
      "Epoch[21/50] | loss train:0.004099| lr:0.010000\n",
      "Epoch[22/50] | loss train:0.004316| lr:0.010000\n",
      "Epoch[23/50] | loss train:0.004266| lr:0.010000\n",
      "Epoch[24/50] | loss train:0.004597| lr:0.010000\n",
      "Epoch[25/50] | loss train:0.004138| lr:0.010000\n",
      "Epoch[26/50] | loss train:0.005077| lr:0.010000\n",
      "Epoch[27/50] | loss train:0.004153| lr:0.010000\n",
      "Epoch[28/50] | loss train:0.004703| lr:0.010000\n",
      "Epoch[29/50] | loss train:0.004244| lr:0.010000\n",
      "Epoch[30/50] | loss train:0.004453| lr:0.010000\n",
      "Epoch[31/50] | loss train:0.003685| lr:0.010000\n",
      "Epoch[32/50] | loss train:0.005324| lr:0.010000\n",
      "Epoch[33/50] | loss train:0.003972| lr:0.010000\n",
      "Epoch[34/50] | loss train:0.004256| lr:0.010000\n",
      "Epoch[35/50] | loss train:0.003862| lr:0.010000\n",
      "Epoch[36/50] | loss train:0.003971| lr:0.010000\n",
      "Epoch[37/50] | loss train:0.004275| lr:0.010000\n",
      "Epoch[38/50] | loss train:0.004563| lr:0.010000\n",
      "Epoch[39/50] | loss train:0.004389| lr:0.010000\n",
      "Epoch[40/50] | loss train:0.004139| lr:0.010000\n",
      "Epoch[41/50] | loss train:0.003860| lr:0.001000\n",
      "Epoch[42/50] | loss train:0.003529| lr:0.001000\n",
      "Epoch[43/50] | loss train:0.003466| lr:0.001000\n",
      "Epoch[44/50] | loss train:0.003256| lr:0.001000\n",
      "Epoch[45/50] | loss train:0.003697| lr:0.001000\n",
      "Epoch[46/50] | loss train:0.003378| lr:0.001000\n",
      "Epoch[47/50] | loss train:0.003315| lr:0.001000\n",
      "Epoch[48/50] | loss train:0.003606| lr:0.001000\n",
      "Epoch[49/50] | loss train:0.003514| lr:0.001000\n",
      "Epoch[50/50] | loss train:0.003297| lr:0.001000\n"
     ]
    }
   ],
   "source": [
    "# API call limit per minute\n",
    "limit = 5\n",
    "\n",
    "check = time.time()\n",
    "\n",
    "for i,ticker in enumerate(tickers):\n",
    "    \n",
    "    # account for API calls per minute\n",
    "    if not i + 1 % limit:\n",
    "        sleep = check + 60 - time.time()\n",
    "        if sleep > 0:\n",
    "            time.sleep(sleep)\n",
    "        check = time.time()      \n",
    "    \n",
    "    date_data, close_price_data, num_data_points, display_date_range = get_data(config, ticker)\n",
    "    \n",
    "    scaler = Normalization()\n",
    "    normalized_close_price_data = scaler.fit_transform(close_price_data)\n",
    "    \n",
    "    data_x, data_x_unseen = prepare_data_x(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    data_y = prepare_data_y(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    \n",
    "    split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "    model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "    \n",
    "    print('{} training'.format(ticker))\n",
    "    for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "        loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch[{}/{}] | loss train:{:.6f}| lr:{:.6f}'\n",
    "                  .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, lr_train))\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    torch.tensor(data_x_unseen)\n",
    "    x = torch.tensor(data_x_unseen).float().to(config[\"training\"][\"device\"]).unsqueeze(0).unsqueeze(2) # this is the data type and shape required, [batch, sequence, feature]\n",
    "    prediction = model(x)\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "    prediction[0] = scaler.inverse_transform(prediction[0])\n",
    "    \n",
    "    df = pd.DataFrame([[next_day,prediction[0]]], columns = ['date','close price']) \n",
    "    path = \"csv/TimeSeries/\" + ticker + \"_predict.csv\"\n",
    "    df.to_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41321d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished in 46798.09273433685 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Program finished in {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17768cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(columns = ['date','ticker','close price'])\n",
    "\n",
    "for ticker in tickers:\n",
    "    path = \"csv/TimeSeries/\" + ticker + \"_predict.csv\"\n",
    "    df1 = pd.read_csv(path)\n",
    "    data = {'date': [df1['date'][0]],\n",
    "            'ticker': [ticker],\n",
    "            'close price': [df1['close price'][0]]}\n",
    "    df2 = pd.DataFrame(data)\n",
    "    predictions = pd.concat([predictions,df2], ignore_index=True)\n",
    "    \n",
    "predictions.to_csv(\"csv/predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
