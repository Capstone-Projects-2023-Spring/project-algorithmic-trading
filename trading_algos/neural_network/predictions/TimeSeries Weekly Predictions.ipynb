{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f0ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from alpha_vantage.timeseries import TimeSeries \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas_market_calendars as mcal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cfbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file (placing here for now, some fields will change on later impplementations)\n",
    "\n",
    "config = {\n",
    "    \"alpha_vantage\": {\n",
    "        \"key\": \"2JMCN347HZ3BU9RC\", \n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"key_adjusted_close\": \"5. adjusted close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 1,\n",
    "        \"train_split_size\": 1,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"xticks_interval\": 90, # show a date every 90 days\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since for now we are only using close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\",\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epoch\": 100,\n",
    "        \"epoch_stop\": 10,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9744ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "# tickers = np.array(sp500[0]['Symbol'])\n",
    "# if ('BF.B' in tickers)\n",
    "#     tickers[tickers.index('BF.B')] = 'BF-B'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tickers = ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ATVI', 'ADM', 'ADBE', 'ADP',\n",
    "       'AAP', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE',\n",
    "       'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN',\n",
    "       'AMCR', 'AMD', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK',\n",
    "       'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA',\n",
    "       'AAPL', 'AMAT', 'APTV', 'ACGL', 'ANET', 'AJG', 'AIZ', 'T', 'ATO',\n",
    "       'ADSK', 'AZO', 'AVB', 'AVY', 'BKR', 'BALL', 'BAC', 'BBWI', 'BAX',\n",
    "       'BDX', 'WRB', 'BRK.B', 'BBY', 'BIO', 'TECH', 'BIIB', 'BLK', 'BK',\n",
    "       'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO',\n",
    "       'BF-B', 'BG', 'CHRW', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH',\n",
    "       'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE',\n",
    "       'CNC', 'CNP', 'CDAY', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG',\n",
    "       'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX',\n",
    "       'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP',\n",
    "       'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CTVA', 'CSGP', 'COST',\n",
    "       'CTRA', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA',\n",
    "       'DE', 'DAL', 'XRAY', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DISH',\n",
    "       'DIS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DTE', 'DUK', 'DD',\n",
    "       'DXC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV',\n",
    "       'LLY', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EQT', 'EFX', 'EQIX',\n",
    "       'EQR', 'ESS', 'EL', 'ETSY', 'RE', 'EVRG', 'ES', 'EXC', 'EXPE',\n",
    "       'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FICO', 'FAST', 'FRT', 'FDX',\n",
    "       'FITB', 'FRC', 'FSLR', 'FE', 'FIS', 'FISV', 'FLT', 'FMC', 'F',\n",
    "       'FTNT', 'FTV', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GEHC',\n",
    "       'GEN', 'GNRC', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GL', 'GPN',\n",
    "       'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'PEAK', 'HSIC', 'HSY', 'HES',\n",
    "       'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ',\n",
    "       'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'ILMN', 'INCY',\n",
    "       'IR', 'PODD', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG',\n",
    "       'IVZ', 'INVH', 'IQV', 'IRM', 'JBHT', 'JKHY', 'J', 'JNJ', 'JCI',\n",
    "       'JPM', 'JNPR', 'K', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI',\n",
    "       'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS',\n",
    "       'LEN', 'LNC', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB',\n",
    "       'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH',\n",
    "       'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM',\n",
    "       'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ',\n",
    "       'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP',\n",
    "       'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'NDSN',\n",
    "       'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI',\n",
    "       'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OGN', 'OTIS',\n",
    "       'PCAR', 'PKG', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PNR', 'PEP',\n",
    "       'PKI', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'POOL',\n",
    "       'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC',\n",
    "       'PSA', 'PHM', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX',\n",
    "       'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP',\n",
    "       'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE',\n",
    "       'NOW', 'SHW', 'SPG', 'SWKS', 'SJM', 'SNA', 'SEDG', 'SO', 'LUV',\n",
    "       'SWK', 'SBUX', 'STT', 'STLD', 'STE', 'SYK', 'SYF', 'SNPS', 'SYY',\n",
    "       'TMUS', 'TROW', 'TTWO', 'TPR', 'TRGP', 'TGT', 'TEL', 'TDY', 'TFX',\n",
    "       'TER', 'TSLA', 'TXN', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG',\n",
    "       'TRV', 'TRMB', 'TFC', 'TYL', 'TSN', 'USB', 'UDR', 'ULTA', 'UNP',\n",
    "       'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VRSN', 'VRSK',\n",
    "       'VZ', 'VRTX', 'VFC', 'VTRS', 'VICI', 'V', 'VMC', 'WAB', 'WBA',\n",
    "       'WMT', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC',\n",
    "       'WRK', 'WY', 'WHR', 'WMB', 'WTW', 'GWW', 'WYNN', 'XEL', 'XYL',\n",
    "       'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dbfd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to account for API limitations (on the weekend)\n",
    "\n",
    "# index = tickers.index('REG')\n",
    "# tickers = tickers[(index+1):]\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b855a278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-04-11', '2023-04-12', '2023-04-13', '2023-04-14', '2023-04-17']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "next_month = mcal.date_range(mcal.get_calendar('NYSE').schedule(start_date=today, end_date=(today+relativedelta(months=1))), frequency='1D')\n",
    "next_month = [date.strftime('%Y-%m-%d') for date in next_month]\n",
    "next_month = next_month[1:]\n",
    "next_week = next_month[0:5]\n",
    "next_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3219e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from the configuration file\n",
    "def get_data(config, ticker, period):\n",
    "    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"]) \n",
    "    data, meta_data = ts.get_daily_adjusted(ticker, outputsize=config[\"alpha_vantage\"][\"outputsize\"])\n",
    "\n",
    "    date_data = [date for date in data.keys()]\n",
    "    date_data.reverse()\n",
    "    date_data.extend(period)\n",
    "    \n",
    "    close_price_data = [float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]]) for date in data.keys()]\n",
    "    close_price_data.reverse()\n",
    "    close_price_data = np.array(close_price_data)\n",
    "    close_price_data = np.append(close_price_data, np.ones(len(period)))\n",
    "\n",
    "    num_data_points = len(date_data)\n",
    "    display_date_range = \"from \" + date_data[0] + \" to \" + date_data[num_data_points-1]\n",
    "    print(\"Number data points\", num_data_points, display_date_range)\n",
    "\n",
    "    return date_data, close_price_data, num_data_points, display_date_range\n",
    "\n",
    "class Normalization():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "    \n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    # use the next day as label\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "# Class to prepare data for training and LSTM model\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # right now we have only 1 feature, so we need to convert `x` into [batch, sequence, features]\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "# neural network model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        \n",
    "        # layer 2\n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "    \n",
    "# function for training LSTM model\n",
    "def run_epoch(dataloader, is_training=False):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        y = y.to(config[\"training\"][\"device\"])\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += (loss.detach().item() / batchsize)\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    return epoch_loss, lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1029d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data points 5902 from 1999-11-01 to 2023-04-17\n",
      "5902\n",
      "5897\n",
      "[-1.66545996 -1.66545996 -1.66545996 -1.66545996]\n",
      "[25.27001843 25.50658881 25.40443342 ...  1.          1.\n",
      "  1.        ]\n",
      "5902\n",
      "[-1.19607973 -1.19150448 -1.19348015 ... -1.66545996 -1.66545996\n",
      " -1.66545996]\n",
      "5902\n",
      "MMM training\n",
      "Epoch[1/100] | loss train:0.081161| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.015103| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012466| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.011324| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011683| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.010746| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.010864| lr:0.010000\n",
      "Epoch[8/100] | loss train:0.010031| lr:0.010000\n",
      "Epoch[9/100] | loss train:0.010648| lr:0.010000\n",
      "Epoch[10/100] | loss train:0.010045| lr:0.010000\n",
      "Epoch[11/100] | loss train:0.010739| lr:0.010000\n",
      "Epoch[12/100] | loss train:0.011069| lr:0.010000\n",
      "Epoch[13/100] | loss train:0.010877| lr:0.010000\n",
      "Epoch[14/100] | loss train:0.009792| lr:0.010000\n",
      "Epoch[15/100] | loss train:0.010516| lr:0.010000\n",
      "Epoch[16/100] | loss train:0.008977| lr:0.010000\n",
      "Epoch[17/100] | loss train:0.010081| lr:0.010000\n",
      "Epoch[18/100] | loss train:0.012693| lr:0.010000\n",
      "Epoch[19/100] | loss train:0.009917| lr:0.010000\n",
      "Epoch[20/100] | loss train:0.009934| lr:0.010000\n",
      "Epoch[21/100] | loss train:0.009549| lr:0.010000\n",
      "Epoch[22/100] | loss train:0.010800| lr:0.010000\n",
      "Epoch[23/100] | loss train:0.009258| lr:0.010000\n",
      "Epoch[24/100] | loss train:0.010990| lr:0.010000\n",
      "Epoch[25/100] | loss train:0.011537| lr:0.010000\n",
      "Epoch[26/100] | loss train:0.011373| lr:0.010000\n",
      "Early stopping.\n",
      "[2.07336745 2.07336745 2.07336745 2.07336745]\n",
      "[12.02575039 12.02575039 12.02575039 12.02575039]\n",
      "Number data points 5902 from 1999-11-01 to 2023-04-17\n",
      "5902\n",
      "5897\n",
      "[-0.94571331 -0.94571331 -0.94571331 -0.94571331]\n",
      "[2.7720521  2.75218867 2.79301905 ... 1.         1.         1.        ]\n",
      "5902\n",
      "[-0.86537256 -0.86627312 -0.86442197 ... -0.94571331 -0.94571331\n",
      " -0.94571331]\n",
      "5902\n",
      "AOS training\n",
      "Epoch[1/100] | loss train:0.103259| lr:0.010000\n",
      "Epoch[2/100] | loss train:0.018083| lr:0.010000\n",
      "Epoch[3/100] | loss train:0.012340| lr:0.010000\n",
      "Epoch[4/100] | loss train:0.015570| lr:0.010000\n",
      "Epoch[5/100] | loss train:0.011856| lr:0.010000\n",
      "Epoch[6/100] | loss train:0.014462| lr:0.010000\n",
      "Epoch[7/100] | loss train:0.012788| lr:0.010000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m training\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ticker))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m---> 54\u001b[0m     loss_train, lr_train \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     loss_val, lr_val \u001b[38;5;241m=\u001b[39m run_epoch(val_dataloader)\n\u001b[0;32m     56\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[6], line 123\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(dataloader, is_training)\u001b[0m\n\u001b[0;32m    120\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39mcontiguous(), y\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m--> 123\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    126\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m batchsize)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# API call limit per minute\n",
    "limit = 5\n",
    "\n",
    "check = time.time()\n",
    "\n",
    "for i,ticker in enumerate(tickers):\n",
    "    \n",
    "    # account for API calls per minute\n",
    "    if not i + 1 % limit:\n",
    "        sleep = check + 60 - time.time()\n",
    "        if sleep > 0:\n",
    "            time.sleep(sleep)\n",
    "        check = time.time()      \n",
    "    \n",
    "    date_data, close_price_data, num_data_points, display_date_range = get_data(config, ticker, next_week)\n",
    "    \n",
    "    scaler = Normalization()\n",
    "    normalized_close_price_data = scaler.fit_transform(close_price_data)\n",
    "    \n",
    "    data_x, data_x_unseen = prepare_data_x(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    data_y = prepare_data_y(normalized_close_price_data, window_size=config[\"data\"][\"window_size\"])\n",
    "    \n",
    "    split_index = num_data_points - len(next_week)\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "    val_dataset = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "    model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "    model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    n_epochs_stop = config[\"training\"][\"epoch_stop\"]\n",
    "    \n",
    "    print('{} training'.format(ticker))\n",
    "    for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "        loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "        loss_val, lr_val = run_epoch(val_dataloader)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch[{}/{}] | loss train:{:.6f}| lr:{:.6f}'\n",
    "                  .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, lr_train))\n",
    "        if loss_train < best_loss:\n",
    "            best_loss = loss_train\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "            \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predicted_val = np.array([])\n",
    "\n",
    "    for idx, (x, y) in enumerate(val_dataloader):\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        out = model(x)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        predicted_val = np.concatenate((predicted_val, out))\n",
    "\n",
    "    print(forecast)\n",
    "    forecast = scaler.inverse_transform(predicted_val)\n",
    "    print(forecast)\n",
    "    \n",
    "    df = pd.DataFrame([[next_week,forecast]], columns = ['date','close price']) \n",
    "    path = \"csv/TimeSeries/\" + ticker + \"_predict.csv\"\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c24040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
